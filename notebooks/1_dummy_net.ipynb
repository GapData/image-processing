{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from utils.data import init_model_logging\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animals Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [fur, extinction, weight] \n",
    "animals = np.array([[1.0, 0.7, 0.9], # lion\n",
    "                    [0.8, 0.8, 1.0], # lion\n",
    "                    [0.1, 0.3, 0.2], # seal\n",
    "                    [0.0, 0.2, 0.1]  # seal\n",
    "                   ], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_labels = np.array(\n",
    "                [[ 1.0],  # lion\n",
    "                 [ 1.0],  # lion\n",
    "                 [-1.0], # seal\n",
    "                 [-1.0]  # seal\n",
    "                ], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Without Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 3], name='x')\n",
    "    \n",
    "    w1 = tf.constant(np.array([[1, 2], [4, 5], [7, 8]], dtype=np.float32), name='w1_const')\n",
    "    h = tf.matmul(x, w1)\n",
    "\n",
    "    w2 = tf.constant(np.array([[1], [1]], dtype=np.float32), name='w2_const')\n",
    "    y = tf.matmul(h, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session(graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction on animals:\n",
      "[[ 22.79999924]\n",
      " [ 24.60000038]\n",
      " [  6.        ]\n",
      " [  3.29999995]]\n"
     ]
    }
   ],
   "source": [
    "prediction, = session.run([y], feed_dict={x: animals})\n",
    "print('Prediction on animals:')\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net With Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 3], name='x')\n",
    "    label = tf.placeholder(tf.float32, shape=[None, 1], name='label')\n",
    "    \n",
    "    w1 = tf.constant(np.array([[1, 2], [4, 5], [7, 8]], dtype=np.float32), name='w1_const')\n",
    "    h = tf.matmul(x, w1)\n",
    "\n",
    "    w2 = tf.constant(np.array([[1], [1]], dtype=np.float32), name='w2_const')\n",
    "    y = tf.matmul(h, w2)\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(label, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session(graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\n",
      "274.922\n"
     ]
    }
   ],
   "source": [
    "loss_on_animal_data, = session.run([loss], feed_dict={x: animals, label: animal_labels})\n",
    "print('Loss:')\n",
    "print(loss_on_animal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net With Trainable Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 3], name='x')\n",
    "    label = tf.placeholder(tf.float32, shape=[None, 1], name='label')\n",
    "    \n",
    "    w1 = tf.get_variable('w1', initializer=tf.truncated_normal(shape=[3,2], stddev=0.1))\n",
    "    h = tf.matmul(x, w1)\n",
    "\n",
    "    w2 = tf.get_variable('w2', initializer=tf.truncated_normal(shape=[2,1], stddev=0.1))\n",
    "    y = tf.matmul(h, w2)\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(label, y)\n",
    "    initialize_vars = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session(graph=graph)\n",
    "session.run(initialize_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:\n",
      "[[ 0.04552605  0.12432265]\n",
      " [ 0.01720901 -0.06935367]\n",
      " [ 0.02880107  0.00268986]]\n",
      "w2:\n",
      "[[-0.069423  ]\n",
      " [-0.15416943]]\n"
     ]
    }
   ],
   "source": [
    "w1_values, w2_values = session.run([w1, w2], feed_dict={x: animals, label: animal_labels})\n",
    "print('w1:')\n",
    "print(w1_values)\n",
    "print('w2:')\n",
    "print(w2_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net With Train Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 3], name='x')\n",
    "    label = tf.placeholder(tf.float32, shape=[None, 1], name='label')\n",
    "    \n",
    "    w1 = tf.get_variable('w1', initializer=tf.truncated_normal(shape=[3,2], stddev=0.1))\n",
    "    h = tf.matmul(x, w1)\n",
    "\n",
    "    w2 = tf.get_variable('w2', initializer=tf.truncated_normal(shape=[2,1], stddev=0.1))\n",
    "    y = tf.matmul(h, w2)\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(label, y)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "    initialize_vars = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session(graph=graph)\n",
    "session.run(initialize_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss 0.9966091513633728\n",
      "Iteration 100: loss 0.9316807985305786\n",
      "Iteration 200: loss 0.7040440440177917\n",
      "Iteration 300: loss 0.6317330598831177\n",
      "Iteration 400: loss 0.6112731695175171\n",
      "Iteration 500: loss 0.5888817310333252\n",
      "Iteration 600: loss 0.562445878982544\n",
      "Iteration 700: loss 0.5305814743041992\n",
      "Iteration 800: loss 0.4922639727592468\n",
      "Iteration 900: loss 0.44750556349754333\n",
      "Iteration 1000: loss 0.3979797065258026\n",
      "Iteration 1100: loss 0.34704089164733887\n",
      "Iteration 1200: loss 0.29887187480926514\n",
      "Iteration 1300: loss 0.25708669424057007\n",
      "Iteration 1400: loss 0.223580002784729\n",
      "Iteration 1500: loss 0.198277086019516\n",
      "Iteration 1600: loss 0.17974495887756348\n",
      "Iteration 1700: loss 0.16608849167823792\n",
      "Iteration 1800: loss 0.1556028127670288\n",
      "Iteration 1900: loss 0.14703553915023804\n",
      "Iteration 2000: loss 0.13958027958869934\n",
      "Iteration 2100: loss 0.1327628195285797\n",
      "Iteration 2200: loss 0.12632226943969727\n",
      "Iteration 2300: loss 0.12012259662151337\n",
      "Iteration 2400: loss 0.11409652978181839\n",
      "Iteration 2500: loss 0.10821305215358734\n",
      "Iteration 2600: loss 0.10246036946773529\n",
      "Iteration 2700: loss 0.09683611989021301\n",
      "Iteration 2800: loss 0.09134341031312943\n",
      "Iteration 2900: loss 0.08598741888999939\n",
      "Iteration 3000: loss 0.08077514171600342\n",
      "Iteration 3100: loss 0.07571398466825485\n",
      "Iteration 3200: loss 0.07081156969070435\n",
      "Iteration 3300: loss 0.06607566773891449\n",
      "Iteration 3400: loss 0.061513207852840424\n",
      "Iteration 3500: loss 0.057130955159664154\n",
      "Iteration 3600: loss 0.05293489992618561\n",
      "Iteration 3700: loss 0.04892966151237488\n",
      "Iteration 3800: loss 0.045119237154722214\n",
      "Iteration 3900: loss 0.04150646924972534\n",
      "Iteration 4000: loss 0.03809263929724693\n",
      "Iteration 4100: loss 0.03487839549779892\n",
      "Iteration 4200: loss 0.03186242654919624\n",
      "Iteration 4300: loss 0.02904253825545311\n",
      "Iteration 4400: loss 0.02641528844833374\n",
      "Iteration 4500: loss 0.023976126685738564\n",
      "Iteration 4600: loss 0.02171942964196205\n",
      "Iteration 4700: loss 0.019638672471046448\n",
      "Iteration 4800: loss 0.017726752907037735\n",
      "Iteration 4900: loss 0.015975646674633026\n",
      "Iteration 5000: loss 0.014377117156982422\n",
      "Iteration 5100: loss 0.012922432273626328\n",
      "Iteration 5200: loss 0.011602656915783882\n",
      "Iteration 5300: loss 0.010408906266093254\n",
      "Iteration 5400: loss 0.009332221001386642\n",
      "Iteration 5500: loss 0.00836371723562479\n",
      "Iteration 5600: loss 0.007494964171200991\n",
      "Iteration 5700: loss 0.006717609241604805\n",
      "Iteration 5800: loss 0.006023739464581013\n",
      "Iteration 5900: loss 0.005405822303146124\n",
      "Iteration 6000: loss 0.0048567550256848335\n",
      "Iteration 6100: loss 0.004369895905256271\n",
      "Iteration 6200: loss 0.003939040936529636\n",
      "Iteration 6300: loss 0.0035584757570177317\n",
      "Iteration 6400: loss 0.0032229553908109665\n",
      "Iteration 6500: loss 0.002927643246948719\n",
      "Iteration 6600: loss 0.0026681101880967617\n",
      "Iteration 6700: loss 0.002440390642732382\n",
      "Iteration 6800: loss 0.002240848960354924\n",
      "Iteration 6900: loss 0.002066235523670912\n",
      "Iteration 7000: loss 0.001913654152303934\n",
      "Iteration 7100: loss 0.0017804563976824284\n",
      "Iteration 7200: loss 0.0016643112758174539\n",
      "Iteration 7300: loss 0.0015631450805813074\n",
      "Iteration 7400: loss 0.0014750875998288393\n",
      "Iteration 7500: loss 0.0013985598925501108\n",
      "Iteration 7600: loss 0.001332069281488657\n",
      "Iteration 7700: loss 0.0012743644183501601\n",
      "Iteration 7800: loss 0.0012243051314726472\n",
      "Iteration 7900: loss 0.0011809158604592085\n",
      "Iteration 8000: loss 0.0011433386243879795\n",
      "Iteration 8100: loss 0.0011108021717518568\n",
      "Iteration 8200: loss 0.001082644797861576\n",
      "Iteration 8300: loss 0.0010583060793578625\n",
      "Iteration 8400: loss 0.0010372750693932176\n",
      "Iteration 8500: loss 0.001019096001982689\n",
      "Iteration 8600: loss 0.0010034017032012343\n",
      "Iteration 8700: loss 0.0009898548014461994\n",
      "Iteration 8800: loss 0.0009781543631106615\n",
      "Iteration 8900: loss 0.0009680717485025525\n",
      "Iteration 9000: loss 0.0009593713330104947\n",
      "Iteration 9100: loss 0.0009518741862848401\n",
      "Iteration 9200: loss 0.0009454088285565376\n",
      "Iteration 9300: loss 0.0009398454567417502\n",
      "Iteration 9400: loss 0.0009350435575470328\n",
      "Iteration 9500: loss 0.0009309120941907167\n",
      "Iteration 9600: loss 0.0009273553732782602\n",
      "Iteration 9700: loss 0.0009242919040843844\n",
      "Iteration 9800: loss 0.0009216470061801374\n",
      "Iteration 9900: loss 0.0009193828445859253\n",
      "Iteration 10000: loss 0.0009174288134090602\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(10001):\n",
    "    _, loss_on_animal_data = session.run([train_step, loss], feed_dict={x: animals, label: animal_labels})\n",
    "    if iteration % 100 == 0:\n",
    "        print(\"Iteration {}: loss {}\".format(iteration, loss_on_animal_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = session.run(y, feed_dict={x: animals})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction on animals:\n",
      "[[ 1.00865924]\n",
      " [ 0.99412858]\n",
      " [-1.03579915]\n",
      " [-0.95226544]]\n",
      "Labels:\n",
      "[[ 1.]\n",
      " [ 1.]\n",
      " [-1.]\n",
      " [-1.]]\n"
     ]
    }
   ],
   "source": [
    "print('Prediction on animals:')\n",
    "print(prediction)\n",
    "print('Labels:')\n",
    "print(animal_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net with logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function collecting data for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(name, var):\n",
    "    with tf.name_scope(name):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('inputs'):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, 3], name='x')\n",
    "        label = tf.placeholder(tf.float32, shape=[None, 1], name='label')\n",
    "    \n",
    "    with tf.variable_scope('layer_1'):\n",
    "        w1 = tf.get_variable('w1', initializer=tf.truncated_normal(shape=[3,2], stddev=0.1))\n",
    "        h = tf.matmul(x, w1)\n",
    "        variable_summaries('w1', w1)\n",
    "\n",
    "    \n",
    "    with tf.variable_scope('layer_2'):\n",
    "        w2 = tf.get_variable('w2', initializer=tf.truncated_normal(shape=[2,1], stddev=0.1))\n",
    "        y = tf.matmul(h, w2)\n",
    "    \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.losses.mean_squared_error(label, y)\n",
    "        variable_summaries('loss', loss)\n",
    "\n",
    "    with tf.name_scope('training'):\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(loss)\n",
    "\n",
    "    initialize_vars = tf.global_variables_initializer()\n",
    "    merge_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Model Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/tensorboard_summaries/dummy_net/'\n",
    "logging_meta = init_model_logging(base_dir, 'experiment1', graph=graph, remove_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_path': '/tensorboard_summaries/dummy_net/experiment1/valid/model.ckpt',\n",
       " 'saver': <tensorflow.python.training.saver.Saver at 0x7fb181ea39b0>,\n",
       " 'train_writer': <tensorflow.python.summary.writer.writer.FileWriter at 0x7fb181ea4d30>,\n",
       " 'train_writer_dir': '/tensorboard_summaries/dummy_net/experiment1/train',\n",
       " 'valid_writer': <tensorflow.python.summary.writer.writer.FileWriter at 0x7fb1805f14a8>,\n",
       " 'valid_writer_dir': '/tensorboard_summaries/dummy_net/experiment1/valid'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss 1.0013372898101807\n",
      "Iteration 1: loss 1.0007268190383911\n",
      "Iteration 2: loss 1.0001341104507446\n",
      "Iteration 3: loss 0.9995492100715637\n",
      "Iteration 4: loss 0.9989627003669739\n",
      "Iteration 5: loss 0.9983651638031006\n",
      "Iteration 6: loss 0.9977471828460693\n",
      "Iteration 7: loss 0.9970992207527161\n",
      "Iteration 8: loss 0.9964112639427185\n",
      "Iteration 9: loss 0.9956730008125305\n",
      "Iteration 10: loss 0.9948734045028687\n",
      "Iteration 11: loss 0.9940007925033569\n",
      "Iteration 12: loss 0.9930425882339478\n",
      "Iteration 13: loss 0.991985023021698\n",
      "Iteration 14: loss 0.9908133745193481\n",
      "Iteration 15: loss 0.9895115494728088\n",
      "Iteration 16: loss 0.9880616664886475\n",
      "Iteration 17: loss 0.9864447116851807\n",
      "Iteration 18: loss 0.9846397638320923\n",
      "Iteration 19: loss 0.9826237559318542\n",
      "Iteration 20: loss 0.9803719520568848\n",
      "Iteration 21: loss 0.9778574705123901\n",
      "Iteration 22: loss 0.9750513434410095\n",
      "Iteration 23: loss 0.9719225168228149\n",
      "Iteration 24: loss 0.9684380888938904\n",
      "Iteration 25: loss 0.9645634889602661\n",
      "Iteration 26: loss 0.9602624773979187\n",
      "Iteration 27: loss 0.9554979205131531\n",
      "Iteration 28: loss 0.9502324461936951\n",
      "Iteration 29: loss 0.9444288015365601\n",
      "Iteration 30: loss 0.9380515813827515\n",
      "Iteration 31: loss 0.9310678243637085\n",
      "Iteration 32: loss 0.923448920249939\n",
      "Iteration 33: loss 0.9151721596717834\n",
      "Iteration 34: loss 0.9062228202819824\n",
      "Iteration 35: loss 0.8965961933135986\n",
      "Iteration 36: loss 0.886299729347229\n",
      "Iteration 37: loss 0.8753551244735718\n",
      "Iteration 38: loss 0.8638004064559937\n",
      "Iteration 39: loss 0.8516908884048462\n",
      "Iteration 40: loss 0.8391005992889404\n",
      "Iteration 41: loss 0.8261218070983887\n",
      "Iteration 42: loss 0.8128639459609985\n",
      "Iteration 43: loss 0.799451470375061\n",
      "Iteration 44: loss 0.7860202789306641\n",
      "Iteration 45: loss 0.7727127075195312\n",
      "Iteration 46: loss 0.7596730589866638\n",
      "Iteration 47: loss 0.7470400333404541\n",
      "Iteration 48: loss 0.7349418997764587\n",
      "Iteration 49: loss 0.7234901785850525\n",
      "Iteration 50: loss 0.7127753496170044\n",
      "Iteration 51: loss 0.702863335609436\n",
      "Iteration 52: loss 0.6937943696975708\n",
      "Iteration 53: loss 0.6855826377868652\n",
      "Iteration 54: loss 0.6782186627388\n",
      "Iteration 55: loss 0.6716721057891846\n",
      "Iteration 56: loss 0.6658955812454224\n",
      "Iteration 57: loss 0.6608294248580933\n",
      "Iteration 58: loss 0.6564062237739563\n",
      "Iteration 59: loss 0.652554452419281\n",
      "Iteration 60: loss 0.6492023468017578\n",
      "Iteration 61: loss 0.6462807655334473\n",
      "Iteration 62: loss 0.6437249183654785\n",
      "Iteration 63: loss 0.6414759159088135\n",
      "Iteration 64: loss 0.6394811868667603\n",
      "Iteration 65: loss 0.6376949548721313\n",
      "Iteration 66: loss 0.6360780000686646\n",
      "Iteration 67: loss 0.6345971822738647\n",
      "Iteration 68: loss 0.6332248449325562\n",
      "Iteration 69: loss 0.631938099861145\n",
      "Iteration 70: loss 0.6307182908058167\n",
      "Iteration 71: loss 0.6295502185821533\n",
      "Iteration 72: loss 0.6284219622612\n",
      "Iteration 73: loss 0.6273236870765686\n",
      "Iteration 74: loss 0.6262475252151489\n",
      "Iteration 75: loss 0.6251875162124634\n",
      "Iteration 76: loss 0.6241388916969299\n",
      "Iteration 77: loss 0.6230977773666382\n",
      "Iteration 78: loss 0.622061014175415\n",
      "Iteration 79: loss 0.6210265159606934\n",
      "Iteration 80: loss 0.6199923753738403\n",
      "Iteration 81: loss 0.6189570426940918\n",
      "Iteration 82: loss 0.617919385433197\n",
      "Iteration 83: loss 0.6168784499168396\n",
      "Iteration 84: loss 0.6158335208892822\n",
      "Iteration 85: loss 0.6147840023040771\n",
      "Iteration 86: loss 0.6137293577194214\n",
      "Iteration 87: loss 0.6126692295074463\n",
      "Iteration 88: loss 0.6116032004356384\n",
      "Iteration 89: loss 0.6105309724807739\n",
      "Iteration 90: loss 0.6094522476196289\n",
      "Iteration 91: loss 0.6083667278289795\n",
      "Iteration 92: loss 0.6072743535041809\n",
      "Iteration 93: loss 0.6061747074127197\n",
      "Iteration 94: loss 0.605067789554596\n",
      "Iteration 95: loss 0.6039531230926514\n",
      "Iteration 96: loss 0.6028307676315308\n",
      "Iteration 97: loss 0.6017004251480103\n",
      "Iteration 98: loss 0.6005620360374451\n",
      "Iteration 99: loss 0.5994153022766113\n",
      "Iteration 100: loss 0.5982599854469299\n",
      "Iteration 101: loss 0.5970960855484009\n",
      "Iteration 102: loss 0.5959233045578003\n",
      "Iteration 103: loss 0.5947414636611938\n",
      "Iteration 104: loss 0.593550443649292\n",
      "Iteration 105: loss 0.5923500061035156\n",
      "Iteration 106: loss 0.5911401510238647\n",
      "Iteration 107: loss 0.5899204015731812\n",
      "Iteration 108: loss 0.5886908173561096\n",
      "Iteration 109: loss 0.5874511003494263\n",
      "Iteration 110: loss 0.5862011313438416\n",
      "Iteration 111: loss 0.5849407315254211\n",
      "Iteration 112: loss 0.5836696028709412\n",
      "Iteration 113: loss 0.5823876857757568\n",
      "Iteration 114: loss 0.5810948610305786\n",
      "Iteration 115: loss 0.5797908306121826\n",
      "Iteration 116: loss 0.5784754753112793\n",
      "Iteration 117: loss 0.5771485567092896\n",
      "Iteration 118: loss 0.5758099555969238\n",
      "Iteration 119: loss 0.574459433555603\n",
      "Iteration 120: loss 0.5730968713760376\n",
      "Iteration 121: loss 0.571722149848938\n",
      "Iteration 122: loss 0.5703349113464355\n",
      "Iteration 123: loss 0.5689350366592407\n",
      "Iteration 124: loss 0.567522406578064\n",
      "Iteration 125: loss 0.5660970211029053\n",
      "Iteration 126: loss 0.5646583437919617\n",
      "Iteration 127: loss 0.5632064342498779\n",
      "Iteration 128: loss 0.5617411136627197\n",
      "Iteration 129: loss 0.560262143611908\n",
      "Iteration 130: loss 0.5587693452835083\n",
      "Iteration 131: loss 0.557262659072876\n",
      "Iteration 132: loss 0.5557419061660767\n",
      "Iteration 133: loss 0.5542067289352417\n",
      "Iteration 134: loss 0.5526573061943054\n",
      "Iteration 135: loss 0.5510932207107544\n",
      "Iteration 136: loss 0.5495144128799438\n",
      "Iteration 137: loss 0.547920823097229\n",
      "Iteration 138: loss 0.5463120937347412\n",
      "Iteration 139: loss 0.54468834400177\n",
      "Iteration 140: loss 0.5430493354797363\n",
      "Iteration 141: loss 0.541394829750061\n",
      "Iteration 142: loss 0.5397248864173889\n",
      "Iteration 143: loss 0.5380392074584961\n",
      "Iteration 144: loss 0.5363378524780273\n",
      "Iteration 145: loss 0.5346206426620483\n",
      "Iteration 146: loss 0.5328873991966248\n",
      "Iteration 147: loss 0.5311381220817566\n",
      "Iteration 148: loss 0.5293726325035095\n",
      "Iteration 149: loss 0.5275909304618835\n",
      "Iteration 150: loss 0.5257928967475891\n",
      "Iteration 151: loss 0.5239785313606262\n",
      "Iteration 152: loss 0.5221475958824158\n",
      "Iteration 153: loss 0.5203002691268921\n",
      "Iteration 154: loss 0.5184363126754761\n",
      "Iteration 155: loss 0.5165557265281677\n",
      "Iteration 156: loss 0.5146585702896118\n",
      "Iteration 157: loss 0.5127447247505188\n",
      "Iteration 158: loss 0.5108141899108887\n",
      "Iteration 159: loss 0.5088669657707214\n",
      "Iteration 160: loss 0.5069031119346619\n",
      "Iteration 161: loss 0.5049225091934204\n",
      "Iteration 162: loss 0.5029253363609314\n",
      "Iteration 163: loss 0.5009115934371948\n",
      "Iteration 164: loss 0.49888116121292114\n",
      "Iteration 165: loss 0.49683427810668945\n",
      "Iteration 166: loss 0.4947708249092102\n",
      "Iteration 167: loss 0.4926910996437073\n",
      "Iteration 168: loss 0.4905950427055359\n",
      "Iteration 169: loss 0.4884827733039856\n",
      "Iteration 170: loss 0.48635441064834595\n",
      "Iteration 171: loss 0.4842100143432617\n",
      "Iteration 172: loss 0.48204976320266724\n",
      "Iteration 173: loss 0.4798738360404968\n",
      "Iteration 174: loss 0.4776822626590729\n",
      "Iteration 175: loss 0.4754753112792969\n",
      "Iteration 176: loss 0.47325313091278076\n",
      "Iteration 177: loss 0.47101593017578125\n",
      "Iteration 178: loss 0.4687638580799103\n",
      "Iteration 179: loss 0.46649712324142456\n",
      "Iteration 180: loss 0.4642159044742584\n",
      "Iteration 181: loss 0.4619206190109253\n",
      "Iteration 182: loss 0.45961129665374756\n",
      "Iteration 183: loss 0.45728838443756104\n",
      "Iteration 184: loss 0.4549520015716553\n",
      "Iteration 185: loss 0.4526025056838989\n",
      "Iteration 186: loss 0.4502401351928711\n",
      "Iteration 187: loss 0.4478652775287628\n",
      "Iteration 188: loss 0.4454781413078308\n",
      "Iteration 189: loss 0.4430791735649109\n",
      "Iteration 190: loss 0.44066867232322693\n",
      "Iteration 191: loss 0.4382469356060028\n",
      "Iteration 192: loss 0.43581435084342957\n",
      "Iteration 193: loss 0.43337130546569824\n",
      "Iteration 194: loss 0.4309181869029999\n",
      "Iteration 195: loss 0.4284553527832031\n",
      "Iteration 196: loss 0.4259832799434662\n",
      "Iteration 197: loss 0.4235023260116577\n",
      "Iteration 198: loss 0.4210129380226135\n",
      "Iteration 199: loss 0.41851550340652466\n",
      "Iteration 200: loss 0.4160105586051941\n",
      "Iteration 201: loss 0.4134983420372009\n",
      "Iteration 202: loss 0.41097962856292725\n",
      "Iteration 203: loss 0.40845468640327454\n",
      "Iteration 204: loss 0.4059240221977234\n",
      "Iteration 205: loss 0.4033881425857544\n",
      "Iteration 206: loss 0.40084749460220337\n",
      "Iteration 207: loss 0.39830267429351807\n",
      "Iteration 208: loss 0.39575403928756714\n",
      "Iteration 209: loss 0.39320218563079834\n",
      "Iteration 210: loss 0.39064767956733704\n",
      "Iteration 211: loss 0.38809093832969666\n",
      "Iteration 212: loss 0.38553255796432495\n",
      "Iteration 213: loss 0.3829730153083801\n",
      "Iteration 214: loss 0.38041290640830994\n",
      "Iteration 215: loss 0.3778526782989502\n",
      "Iteration 216: loss 0.3752930164337158\n",
      "Iteration 217: loss 0.37273430824279785\n",
      "Iteration 218: loss 0.37017717957496643\n",
      "Iteration 219: loss 0.3676221966743469\n",
      "Iteration 220: loss 0.36506983637809753\n",
      "Iteration 221: loss 0.362520694732666\n",
      "Iteration 222: loss 0.35997527837753296\n",
      "Iteration 223: loss 0.35743415355682373\n",
      "Iteration 224: loss 0.3548978567123413\n",
      "Iteration 225: loss 0.35236698389053345\n",
      "Iteration 226: loss 0.34984201192855835\n",
      "Iteration 227: loss 0.34732353687286377\n",
      "Iteration 228: loss 0.3448120355606079\n",
      "Iteration 229: loss 0.34230804443359375\n",
      "Iteration 230: loss 0.33981215953826904\n",
      "Iteration 231: loss 0.337324857711792\n",
      "Iteration 232: loss 0.3348466157913208\n",
      "Iteration 233: loss 0.3323780298233032\n",
      "Iteration 234: loss 0.3299195170402527\n",
      "Iteration 235: loss 0.32747167348861694\n",
      "Iteration 236: loss 0.32503488659858704\n",
      "Iteration 237: loss 0.3226097524166107\n",
      "Iteration 238: loss 0.32019662857055664\n",
      "Iteration 239: loss 0.31779611110687256\n",
      "Iteration 240: loss 0.3154085874557495\n",
      "Iteration 241: loss 0.3130345046520233\n",
      "Iteration 242: loss 0.3106742799282074\n",
      "Iteration 243: loss 0.30832844972610474\n",
      "Iteration 244: loss 0.3059973418712616\n",
      "Iteration 245: loss 0.3036813735961914\n",
      "Iteration 246: loss 0.30138105154037476\n",
      "Iteration 247: loss 0.29909658432006836\n",
      "Iteration 248: loss 0.2968285083770752\n",
      "Iteration 249: loss 0.29457712173461914\n",
      "Iteration 250: loss 0.29234278202056885\n",
      "Iteration 251: loss 0.29012584686279297\n",
      "Iteration 252: loss 0.2879266142845154\n",
      "Iteration 253: loss 0.28574541211128235\n",
      "Iteration 254: loss 0.2835825979709625\n",
      "Iteration 255: loss 0.28143835067749023\n",
      "Iteration 256: loss 0.27931302785873413\n",
      "Iteration 257: loss 0.2772068977355957\n",
      "Iteration 258: loss 0.27512019872665405\n",
      "Iteration 259: loss 0.2730531394481659\n",
      "Iteration 260: loss 0.2710060477256775\n",
      "Iteration 261: loss 0.26897895336151123\n",
      "Iteration 262: loss 0.266972154378891\n",
      "Iteration 263: loss 0.26498591899871826\n",
      "Iteration 264: loss 0.26302021741867065\n",
      "Iteration 265: loss 0.26107537746429443\n",
      "Iteration 266: loss 0.2591514587402344\n",
      "Iteration 267: loss 0.2572486400604248\n",
      "Iteration 268: loss 0.2553669810295105\n",
      "Iteration 269: loss 0.253506600856781\n",
      "Iteration 270: loss 0.2516675591468811\n",
      "Iteration 271: loss 0.24985000491142273\n",
      "Iteration 272: loss 0.24805396795272827\n",
      "Iteration 273: loss 0.24627943336963654\n",
      "Iteration 274: loss 0.24452655017375946\n",
      "Iteration 275: loss 0.24279527366161346\n",
      "Iteration 276: loss 0.24108566343784332\n",
      "Iteration 277: loss 0.2393975704908371\n",
      "Iteration 278: loss 0.23773112893104553\n",
      "Iteration 279: loss 0.23608626425266266\n",
      "Iteration 280: loss 0.23446296155452728\n",
      "Iteration 281: loss 0.23286117613315582\n",
      "Iteration 282: loss 0.2312808334827423\n",
      "Iteration 283: loss 0.22972184419631958\n",
      "Iteration 284: loss 0.22818416357040405\n",
      "Iteration 285: loss 0.22666774690151215\n",
      "Iteration 286: loss 0.22517238557338715\n",
      "Iteration 287: loss 0.22369801998138428\n",
      "Iteration 288: loss 0.22224456071853638\n",
      "Iteration 289: loss 0.22081179916858673\n",
      "Iteration 290: loss 0.21939972043037415\n",
      "Iteration 291: loss 0.2180081009864807\n",
      "Iteration 292: loss 0.2166367769241333\n",
      "Iteration 293: loss 0.21528562903404236\n",
      "Iteration 294: loss 0.21395447850227356\n",
      "Iteration 295: loss 0.21264314651489258\n",
      "Iteration 296: loss 0.2113514542579651\n",
      "Iteration 297: loss 0.21007929742336273\n",
      "Iteration 298: loss 0.20882633328437805\n",
      "Iteration 299: loss 0.2075924277305603\n",
      "Iteration 300: loss 0.20637744665145874\n",
      "Iteration 301: loss 0.2051810920238495\n",
      "Iteration 302: loss 0.2040031999349594\n",
      "Iteration 303: loss 0.20284348726272583\n",
      "Iteration 304: loss 0.2017017900943756\n",
      "Iteration 305: loss 0.20057789981365204\n",
      "Iteration 306: loss 0.199471578001976\n",
      "Iteration 307: loss 0.19838249683380127\n",
      "Iteration 308: loss 0.19731055200099945\n",
      "Iteration 309: loss 0.19625547528266907\n",
      "Iteration 310: loss 0.19521698355674744\n",
      "Iteration 311: loss 0.19419482350349426\n",
      "Iteration 312: loss 0.19318881630897522\n",
      "Iteration 313: loss 0.19219866394996643\n",
      "Iteration 314: loss 0.19122418761253357\n",
      "Iteration 315: loss 0.19026505947113037\n",
      "Iteration 316: loss 0.18932104110717773\n",
      "Iteration 317: loss 0.18839192390441895\n",
      "Iteration 318: loss 0.1874774694442749\n",
      "Iteration 319: loss 0.18657740950584412\n",
      "Iteration 320: loss 0.18569141626358032\n",
      "Iteration 321: loss 0.18481940031051636\n",
      "Iteration 322: loss 0.18396097421646118\n",
      "Iteration 323: loss 0.18311598896980286\n",
      "Iteration 324: loss 0.18228408694267273\n",
      "Iteration 325: loss 0.18146510422229767\n",
      "Iteration 326: loss 0.18065878748893738\n",
      "Iteration 327: loss 0.17986489832401276\n",
      "Iteration 328: loss 0.1790831834077835\n",
      "Iteration 329: loss 0.17831334471702576\n",
      "Iteration 330: loss 0.17755526304244995\n",
      "Iteration 331: loss 0.17680859565734863\n",
      "Iteration 332: loss 0.1760731339454651\n",
      "Iteration 333: loss 0.175348699092865\n",
      "Iteration 334: loss 0.17463502287864685\n",
      "Iteration 335: loss 0.17393186688423157\n",
      "Iteration 336: loss 0.17323902249336243\n",
      "Iteration 337: loss 0.17255628108978271\n",
      "Iteration 338: loss 0.17188337445259094\n",
      "Iteration 339: loss 0.17122013866901398\n",
      "Iteration 340: loss 0.17056630551815033\n",
      "Iteration 341: loss 0.16992174088954926\n",
      "Iteration 342: loss 0.16928616166114807\n",
      "Iteration 343: loss 0.16865941882133484\n",
      "Iteration 344: loss 0.16804122924804688\n",
      "Iteration 345: loss 0.16743148863315582\n",
      "Iteration 346: loss 0.16682995855808258\n",
      "Iteration 347: loss 0.16623643040657043\n",
      "Iteration 348: loss 0.16565071046352386\n",
      "Iteration 349: loss 0.16507264971733093\n",
      "Iteration 350: loss 0.1645020693540573\n",
      "Iteration 351: loss 0.16393879055976868\n",
      "Iteration 352: loss 0.16338256001472473\n",
      "Iteration 353: loss 0.16283324360847473\n",
      "Iteration 354: loss 0.16229063272476196\n",
      "Iteration 355: loss 0.16175466775894165\n",
      "Iteration 356: loss 0.16122516989707947\n",
      "Iteration 357: loss 0.16070181131362915\n",
      "Iteration 358: loss 0.16018462181091309\n",
      "Iteration 359: loss 0.15967342257499695\n",
      "Iteration 360: loss 0.15916788578033447\n",
      "Iteration 361: loss 0.15866805613040924\n",
      "Iteration 362: loss 0.15817376971244812\n",
      "Iteration 363: loss 0.15768474340438843\n",
      "Iteration 364: loss 0.15720096230506897\n",
      "Iteration 365: loss 0.15672236680984497\n",
      "Iteration 366: loss 0.1562485247850418\n",
      "Iteration 367: loss 0.1557796746492386\n",
      "Iteration 368: loss 0.15531539916992188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 369: loss 0.15485571324825287\n",
      "Iteration 370: loss 0.15440042316913605\n",
      "Iteration 371: loss 0.1539495587348938\n",
      "Iteration 372: loss 0.15350280702114105\n",
      "Iteration 373: loss 0.15306013822555542\n",
      "Iteration 374: loss 0.1526215374469757\n",
      "Iteration 375: loss 0.15218672156333923\n",
      "Iteration 376: loss 0.15175575017929077\n",
      "Iteration 377: loss 0.15132836997509003\n",
      "Iteration 378: loss 0.1509045660495758\n",
      "Iteration 379: loss 0.15048423409461975\n",
      "Iteration 380: loss 0.1500672996044159\n",
      "Iteration 381: loss 0.1496536135673523\n",
      "Iteration 382: loss 0.14924313127994537\n",
      "Iteration 383: loss 0.14883571863174438\n",
      "Iteration 384: loss 0.14843130111694336\n",
      "Iteration 385: loss 0.1480298638343811\n",
      "Iteration 386: loss 0.1476312279701233\n",
      "Iteration 387: loss 0.1472354382276535\n",
      "Iteration 388: loss 0.14684227108955383\n",
      "Iteration 389: loss 0.14645174145698547\n",
      "Iteration 390: loss 0.14606374502182007\n",
      "Iteration 391: loss 0.14567822217941284\n",
      "Iteration 392: loss 0.1452951431274414\n",
      "Iteration 393: loss 0.14491434395313263\n",
      "Iteration 394: loss 0.1445358246564865\n",
      "Iteration 395: loss 0.14415952563285828\n",
      "Iteration 396: loss 0.14378535747528076\n",
      "Iteration 397: loss 0.14341327548027039\n",
      "Iteration 398: loss 0.14304323494434357\n",
      "Iteration 399: loss 0.14267519116401672\n",
      "Iteration 400: loss 0.1423090249300003\n",
      "Iteration 401: loss 0.1419447362422943\n",
      "Iteration 402: loss 0.14158232510089874\n",
      "Iteration 403: loss 0.14122159779071808\n",
      "Iteration 404: loss 0.1408626139163971\n",
      "Iteration 405: loss 0.14050528407096863\n",
      "Iteration 406: loss 0.14014959335327148\n",
      "Iteration 407: loss 0.1397954821586609\n",
      "Iteration 408: loss 0.13944292068481445\n",
      "Iteration 409: loss 0.13909178972244263\n",
      "Iteration 410: loss 0.13874214887619019\n",
      "Iteration 411: loss 0.13839395344257355\n",
      "Iteration 412: loss 0.13804711401462555\n",
      "Iteration 413: loss 0.13770155608654022\n",
      "Iteration 414: loss 0.13735733926296234\n",
      "Iteration 415: loss 0.13701441884040833\n",
      "Iteration 416: loss 0.13667269051074982\n",
      "Iteration 417: loss 0.13633209466934204\n",
      "Iteration 418: loss 0.13599281013011932\n",
      "Iteration 419: loss 0.13565459847450256\n",
      "Iteration 420: loss 0.13531750440597534\n",
      "Iteration 421: loss 0.13498151302337646\n",
      "Iteration 422: loss 0.13464650511741638\n",
      "Iteration 423: loss 0.13431257009506226\n",
      "Iteration 424: loss 0.1339796483516693\n",
      "Iteration 425: loss 0.13364766538143158\n",
      "Iteration 426: loss 0.13331671059131622\n",
      "Iteration 427: loss 0.13298660516738892\n",
      "Iteration 428: loss 0.1326574683189392\n",
      "Iteration 429: loss 0.13232913613319397\n",
      "Iteration 430: loss 0.13200172781944275\n",
      "Iteration 431: loss 0.13167518377304077\n",
      "Iteration 432: loss 0.1313493698835373\n",
      "Iteration 433: loss 0.13102443516254425\n",
      "Iteration 434: loss 0.1307002156972885\n",
      "Iteration 435: loss 0.13037681579589844\n",
      "Iteration 436: loss 0.13005417585372925\n",
      "Iteration 437: loss 0.12973222136497498\n",
      "Iteration 438: loss 0.1294110119342804\n",
      "Iteration 439: loss 0.1290905475616455\n",
      "Iteration 440: loss 0.1287706345319748\n",
      "Iteration 441: loss 0.12845146656036377\n",
      "Iteration 442: loss 0.12813299894332886\n",
      "Iteration 443: loss 0.1278151273727417\n",
      "Iteration 444: loss 0.12749788165092468\n",
      "Iteration 445: loss 0.1271812468767166\n",
      "Iteration 446: loss 0.12686525285243988\n",
      "Iteration 447: loss 0.12654978036880493\n",
      "Iteration 448: loss 0.12623494863510132\n",
      "Iteration 449: loss 0.12592065334320068\n",
      "Iteration 450: loss 0.1256069839000702\n",
      "Iteration 451: loss 0.1252937614917755\n",
      "Iteration 452: loss 0.12498115748167038\n",
      "Iteration 453: loss 0.12466901540756226\n",
      "Iteration 454: loss 0.1243574395775795\n",
      "Iteration 455: loss 0.12404640763998032\n",
      "Iteration 456: loss 0.12373585999011993\n",
      "Iteration 457: loss 0.12342574447393417\n",
      "Iteration 458: loss 0.123116135597229\n",
      "Iteration 459: loss 0.12280705571174622\n",
      "Iteration 460: loss 0.12249837070703506\n",
      "Iteration 461: loss 0.1221902146935463\n",
      "Iteration 462: loss 0.12188245356082916\n",
      "Iteration 463: loss 0.12157522886991501\n",
      "Iteration 464: loss 0.12126842886209488\n",
      "Iteration 465: loss 0.12096208333969116\n",
      "Iteration 466: loss 0.1206560879945755\n",
      "Iteration 467: loss 0.12035059183835983\n",
      "Iteration 468: loss 0.12004547566175461\n",
      "Iteration 469: loss 0.119740791618824\n",
      "Iteration 470: loss 0.11943650245666504\n",
      "Iteration 471: loss 0.1191326230764389\n",
      "Iteration 472: loss 0.11882917582988739\n",
      "Iteration 473: loss 0.11852605640888214\n",
      "Iteration 474: loss 0.11822337657213211\n",
      "Iteration 475: loss 0.1179211214184761\n",
      "Iteration 476: loss 0.11761923134326935\n",
      "Iteration 477: loss 0.11731774359941483\n",
      "Iteration 478: loss 0.11701659858226776\n",
      "Iteration 479: loss 0.11671580374240875\n",
      "Iteration 480: loss 0.11641543358564377\n",
      "Iteration 481: loss 0.11611540615558624\n",
      "Iteration 482: loss 0.11581569910049438\n",
      "Iteration 483: loss 0.11551640182733536\n",
      "Iteration 484: loss 0.11521746218204498\n",
      "Iteration 485: loss 0.11491891741752625\n",
      "Iteration 486: loss 0.11462065577507019\n",
      "Iteration 487: loss 0.11432276666164398\n",
      "Iteration 488: loss 0.11402519792318344\n",
      "Iteration 489: loss 0.11372803896665573\n",
      "Iteration 490: loss 0.11343125998973846\n",
      "Iteration 491: loss 0.11313466727733612\n",
      "Iteration 492: loss 0.11283854395151138\n",
      "Iteration 493: loss 0.1125427633523941\n",
      "Iteration 494: loss 0.11224731057882309\n",
      "Iteration 495: loss 0.11195217072963715\n",
      "Iteration 496: loss 0.11165732145309448\n",
      "Iteration 497: loss 0.11136286705732346\n",
      "Iteration 498: loss 0.1110687181353569\n",
      "Iteration 499: loss 0.11077497899532318\n",
      "Iteration 500: loss 0.11048144102096558\n",
      "Iteration 501: loss 0.110188327729702\n",
      "Iteration 502: loss 0.1098955050110817\n",
      "Iteration 503: loss 0.10960294306278229\n",
      "Iteration 504: loss 0.1093108206987381\n",
      "Iteration 505: loss 0.10901898145675659\n",
      "Iteration 506: loss 0.10872749239206314\n",
      "Iteration 507: loss 0.10843627154827118\n",
      "Iteration 508: loss 0.10814537107944489\n",
      "Iteration 509: loss 0.10785485804080963\n",
      "Iteration 510: loss 0.10756467282772064\n",
      "Iteration 511: loss 0.10727474093437195\n",
      "Iteration 512: loss 0.10698512196540833\n",
      "Iteration 513: loss 0.10669584572315216\n",
      "Iteration 514: loss 0.10640694200992584\n",
      "Iteration 515: loss 0.10611828416585922\n",
      "Iteration 516: loss 0.10582999885082245\n",
      "Iteration 517: loss 0.10554196685552597\n",
      "Iteration 518: loss 0.10525427758693695\n",
      "Iteration 519: loss 0.10496694594621658\n",
      "Iteration 520: loss 0.10467991232872009\n",
      "Iteration 521: loss 0.10439317673444748\n",
      "Iteration 522: loss 0.1041068360209465\n",
      "Iteration 523: loss 0.10382071137428284\n",
      "Iteration 524: loss 0.10353496670722961\n",
      "Iteration 525: loss 0.10324949771165848\n",
      "Iteration 526: loss 0.1029643565416336\n",
      "Iteration 527: loss 0.10267960280179977\n",
      "Iteration 528: loss 0.10239511728286743\n",
      "Iteration 529: loss 0.10211095213890076\n",
      "Iteration 530: loss 0.10182708501815796\n",
      "Iteration 531: loss 0.10154353827238083\n",
      "Iteration 532: loss 0.10126032680273056\n",
      "Iteration 533: loss 0.10097745060920715\n",
      "Iteration 534: loss 0.10069487243890762\n",
      "Iteration 535: loss 0.10041266679763794\n",
      "Iteration 536: loss 0.10013066232204437\n",
      "Iteration 537: loss 0.09984908998012543\n",
      "Iteration 538: loss 0.09956783056259155\n",
      "Iteration 539: loss 0.09928680956363678\n",
      "Iteration 540: loss 0.09900617599487305\n",
      "Iteration 541: loss 0.09872591495513916\n",
      "Iteration 542: loss 0.09844585508108139\n",
      "Iteration 543: loss 0.09816625714302063\n",
      "Iteration 544: loss 0.09788686037063599\n",
      "Iteration 545: loss 0.09760784357786179\n",
      "Iteration 546: loss 0.09732909500598907\n",
      "Iteration 547: loss 0.097050741314888\n",
      "Iteration 548: loss 0.09677273035049438\n",
      "Iteration 549: loss 0.09649497270584106\n",
      "Iteration 550: loss 0.09621758759021759\n",
      "Iteration 551: loss 0.095940500497818\n",
      "Iteration 552: loss 0.09566379338502884\n",
      "Iteration 553: loss 0.09538740664720535\n",
      "Iteration 554: loss 0.09511134028434753\n",
      "Iteration 555: loss 0.09483559429645538\n",
      "Iteration 556: loss 0.09456013143062592\n",
      "Iteration 557: loss 0.09428508579730988\n",
      "Iteration 558: loss 0.09401034563779831\n",
      "Iteration 559: loss 0.09373593330383301\n",
      "Iteration 560: loss 0.09346179664134979\n",
      "Iteration 561: loss 0.0931880921125412\n",
      "Iteration 562: loss 0.09291476011276245\n",
      "Iteration 563: loss 0.09264165163040161\n",
      "Iteration 564: loss 0.09236893057823181\n",
      "Iteration 565: loss 0.09209657460451126\n",
      "Iteration 566: loss 0.09182444214820862\n",
      "Iteration 567: loss 0.09155277907848358\n",
      "Iteration 568: loss 0.09128138422966003\n",
      "Iteration 569: loss 0.09101036190986633\n",
      "Iteration 570: loss 0.09073961526155472\n",
      "Iteration 571: loss 0.09046930074691772\n",
      "Iteration 572: loss 0.09019926935434341\n",
      "Iteration 573: loss 0.08992963284254074\n",
      "Iteration 574: loss 0.08966035395860672\n",
      "Iteration 575: loss 0.08939135819673538\n",
      "Iteration 576: loss 0.0891227200627327\n",
      "Iteration 577: loss 0.08885446190834045\n",
      "Iteration 578: loss 0.08858658373355865\n",
      "Iteration 579: loss 0.08831903338432312\n",
      "Iteration 580: loss 0.08805181086063385\n",
      "Iteration 581: loss 0.08778492361307144\n",
      "Iteration 582: loss 0.08751839399337769\n",
      "Iteration 583: loss 0.08725222945213318\n",
      "Iteration 584: loss 0.08698645234107971\n",
      "Iteration 585: loss 0.08672095835208893\n",
      "Iteration 586: loss 0.08645587414503098\n",
      "Iteration 587: loss 0.08619115501642227\n",
      "Iteration 588: loss 0.08592678606510162\n",
      "Iteration 589: loss 0.08566273748874664\n",
      "Iteration 590: loss 0.08539911359548569\n",
      "Iteration 591: loss 0.08513576537370682\n",
      "Iteration 592: loss 0.08487285673618317\n",
      "Iteration 593: loss 0.08461025357246399\n",
      "Iteration 594: loss 0.08434802293777466\n",
      "Iteration 595: loss 0.08408621698617935\n",
      "Iteration 596: loss 0.0838247537612915\n",
      "Iteration 597: loss 0.08356359601020813\n",
      "Iteration 598: loss 0.08330285549163818\n",
      "Iteration 599: loss 0.0830424353480339\n",
      "Iteration 600: loss 0.08278244733810425\n",
      "Iteration 601: loss 0.08252280950546265\n",
      "Iteration 602: loss 0.08226355165243149\n",
      "Iteration 603: loss 0.08200463652610779\n",
      "Iteration 604: loss 0.08174610137939453\n",
      "Iteration 605: loss 0.08148795366287231\n",
      "Iteration 606: loss 0.08123020082712173\n",
      "Iteration 607: loss 0.0809728354215622\n",
      "Iteration 608: loss 0.08071574568748474\n",
      "Iteration 609: loss 0.08045908808708191\n",
      "Iteration 610: loss 0.0802028477191925\n",
      "Iteration 611: loss 0.07994695752859116\n",
      "Iteration 612: loss 0.07969142496585846\n",
      "Iteration 613: loss 0.07943631708621979\n",
      "Iteration 614: loss 0.07918153703212738\n",
      "Iteration 615: loss 0.07892724871635437\n",
      "Iteration 616: loss 0.07867325097322464\n",
      "Iteration 617: loss 0.07841960340738297\n",
      "Iteration 618: loss 0.0781664252281189\n",
      "Iteration 619: loss 0.07791358232498169\n",
      "Iteration 620: loss 0.0776611715555191\n",
      "Iteration 621: loss 0.07740910351276398\n",
      "Iteration 622: loss 0.07715749740600586\n",
      "Iteration 623: loss 0.07690621167421341\n",
      "Iteration 624: loss 0.07665533572435379\n",
      "Iteration 625: loss 0.07640492916107178\n",
      "Iteration 626: loss 0.07615478336811066\n",
      "Iteration 627: loss 0.07590512931346893\n",
      "Iteration 628: loss 0.07565590739250183\n",
      "Iteration 629: loss 0.07540696114301682\n",
      "Iteration 630: loss 0.07515843212604523\n",
      "Iteration 631: loss 0.07491037994623184\n",
      "Iteration 632: loss 0.07466268539428711\n",
      "Iteration 633: loss 0.0744154155254364\n",
      "Iteration 634: loss 0.07416848838329315\n",
      "Iteration 635: loss 0.07392200827598572\n",
      "Iteration 636: loss 0.0736759603023529\n",
      "Iteration 637: loss 0.07343021035194397\n",
      "Iteration 638: loss 0.07318497449159622\n",
      "Iteration 639: loss 0.07294008135795593\n",
      "Iteration 640: loss 0.07269564270973206\n",
      "Iteration 641: loss 0.07245156168937683\n",
      "Iteration 642: loss 0.07220789045095444\n",
      "Iteration 643: loss 0.07196472585201263\n",
      "Iteration 644: loss 0.07172191888093948\n",
      "Iteration 645: loss 0.07147946208715439\n",
      "Iteration 646: loss 0.0712374597787857\n",
      "Iteration 647: loss 0.07099586725234985\n",
      "Iteration 648: loss 0.07075470685958862\n",
      "Iteration 649: loss 0.07051397114992142\n",
      "Iteration 650: loss 0.07027367502450943\n",
      "Iteration 651: loss 0.07003375887870789\n",
      "Iteration 652: loss 0.06979425251483917\n",
      "Iteration 653: loss 0.06955517828464508\n",
      "Iteration 654: loss 0.06931650638580322\n",
      "Iteration 655: loss 0.06907826662063599\n",
      "Iteration 656: loss 0.06884045898914337\n",
      "Iteration 657: loss 0.06860306113958359\n",
      "Iteration 658: loss 0.0683661550283432\n",
      "Iteration 659: loss 0.0681295394897461\n",
      "Iteration 660: loss 0.06789341568946838\n",
      "Iteration 661: loss 0.06765782833099365\n",
      "Iteration 662: loss 0.06742247939109802\n",
      "Iteration 663: loss 0.06718767434358597\n",
      "Iteration 664: loss 0.06695325672626495\n",
      "Iteration 665: loss 0.06671929359436035\n",
      "Iteration 666: loss 0.06648577749729156\n",
      "Iteration 667: loss 0.0662526786327362\n",
      "Iteration 668: loss 0.0660199522972107\n",
      "Iteration 669: loss 0.06578771024942398\n",
      "Iteration 670: loss 0.06555595248937607\n",
      "Iteration 671: loss 0.06532460451126099\n",
      "Iteration 672: loss 0.06509362906217575\n",
      "Iteration 673: loss 0.06486310064792633\n",
      "Iteration 674: loss 0.0646330714225769\n",
      "Iteration 675: loss 0.06440345197916031\n",
      "Iteration 676: loss 0.06417429447174072\n",
      "Iteration 677: loss 0.06394558399915695\n",
      "Iteration 678: loss 0.06371727585792542\n",
      "Iteration 679: loss 0.06348943710327148\n",
      "Iteration 680: loss 0.06326200067996979\n",
      "Iteration 681: loss 0.06303507834672928\n",
      "Iteration 682: loss 0.0628085508942604\n",
      "Iteration 683: loss 0.06258248537778854\n",
      "Iteration 684: loss 0.0623568519949913\n",
      "Iteration 685: loss 0.062131669372320175\n",
      "Iteration 686: loss 0.06190689653158188\n",
      "Iteration 687: loss 0.061682626605033875\n",
      "Iteration 688: loss 0.0614587739109993\n",
      "Iteration 689: loss 0.061235398054122925\n",
      "Iteration 690: loss 0.06101248413324356\n",
      "Iteration 691: loss 0.06078995391726494\n",
      "Iteration 692: loss 0.06056798994541168\n",
      "Iteration 693: loss 0.06034637987613678\n",
      "Iteration 694: loss 0.06012532860040665\n",
      "Iteration 695: loss 0.05990464612841606\n",
      "Iteration 696: loss 0.05968441069126129\n",
      "Iteration 697: loss 0.059464648365974426\n",
      "Iteration 698: loss 0.059245407581329346\n",
      "Iteration 699: loss 0.05902655050158501\n",
      "Iteration 700: loss 0.05880822241306305\n",
      "Iteration 701: loss 0.05859026685357094\n",
      "Iteration 702: loss 0.058372803032398224\n",
      "Iteration 703: loss 0.058155857026576996\n",
      "Iteration 704: loss 0.05793927237391472\n",
      "Iteration 705: loss 0.057723190635442734\n",
      "Iteration 706: loss 0.05750763788819313\n",
      "Iteration 707: loss 0.057292498648166656\n",
      "Iteration 708: loss 0.057077787816524506\n",
      "Iteration 709: loss 0.05686359852552414\n",
      "Iteration 710: loss 0.05664987862110138\n",
      "Iteration 711: loss 0.05643657222390175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 712: loss 0.05622371286153793\n",
      "Iteration 713: loss 0.05601140856742859\n",
      "Iteration 714: loss 0.05579947307705879\n",
      "Iteration 715: loss 0.05558812618255615\n",
      "Iteration 716: loss 0.05537717416882515\n",
      "Iteration 717: loss 0.05516676604747772\n",
      "Iteration 718: loss 0.05495671182870865\n",
      "Iteration 719: loss 0.05474723502993584\n",
      "Iteration 720: loss 0.054538141936063766\n",
      "Iteration 721: loss 0.05432958900928497\n",
      "Iteration 722: loss 0.05412144213914871\n",
      "Iteration 723: loss 0.053913794457912445\n",
      "Iteration 724: loss 0.05370667949318886\n",
      "Iteration 725: loss 0.05349995940923691\n",
      "Iteration 726: loss 0.05329370126128197\n",
      "Iteration 727: loss 0.053088024258613586\n",
      "Iteration 728: loss 0.05288275331258774\n",
      "Iteration 729: loss 0.05267803370952606\n",
      "Iteration 730: loss 0.05247368663549423\n",
      "Iteration 731: loss 0.05226994305849075\n",
      "Iteration 732: loss 0.052066557109355927\n",
      "Iteration 733: loss 0.051863688975572586\n",
      "Iteration 734: loss 0.05166133493185043\n",
      "Iteration 735: loss 0.05145946145057678\n",
      "Iteration 736: loss 0.051258061081171036\n",
      "Iteration 737: loss 0.051057107746601105\n",
      "Iteration 738: loss 0.05085667222738266\n",
      "Iteration 739: loss 0.050656646490097046\n",
      "Iteration 740: loss 0.050457146018743515\n",
      "Iteration 741: loss 0.05025814473628998\n",
      "Iteration 742: loss 0.05005965381860733\n",
      "Iteration 743: loss 0.04986162111163139\n",
      "Iteration 744: loss 0.04966405779123306\n",
      "Iteration 745: loss 0.04946703463792801\n",
      "Iteration 746: loss 0.04927044361829758\n",
      "Iteration 747: loss 0.049074411392211914\n",
      "Iteration 748: loss 0.04887878894805908\n",
      "Iteration 749: loss 0.04868364706635475\n",
      "Iteration 750: loss 0.04848907142877579\n",
      "Iteration 751: loss 0.04829489439725876\n",
      "Iteration 752: loss 0.0481012687087059\n",
      "Iteration 753: loss 0.04790815711021423\n",
      "Iteration 754: loss 0.04771547019481659\n",
      "Iteration 755: loss 0.047523267567157745\n",
      "Iteration 756: loss 0.04733153432607651\n",
      "Iteration 757: loss 0.04714035615324974\n",
      "Iteration 758: loss 0.04694966971874237\n",
      "Iteration 759: loss 0.046759411692619324\n",
      "Iteration 760: loss 0.04656974971294403\n",
      "Iteration 761: loss 0.046380434185266495\n",
      "Iteration 762: loss 0.04619170352816582\n",
      "Iteration 763: loss 0.046003468334674835\n",
      "Iteration 764: loss 0.04581576958298683\n",
      "Iteration 765: loss 0.045628469437360764\n",
      "Iteration 766: loss 0.045441702008247375\n",
      "Iteration 767: loss 0.04525543004274368\n",
      "Iteration 768: loss 0.04506964609026909\n",
      "Iteration 769: loss 0.0448843352496624\n",
      "Iteration 770: loss 0.044699545949697495\n",
      "Iteration 771: loss 0.0445152223110199\n",
      "Iteration 772: loss 0.04433146119117737\n",
      "Iteration 773: loss 0.044148094952106476\n",
      "Iteration 774: loss 0.04396536201238632\n",
      "Iteration 775: loss 0.04378301650285721\n",
      "Iteration 776: loss 0.043601177632808685\n",
      "Iteration 777: loss 0.043419867753982544\n",
      "Iteration 778: loss 0.04323907941579819\n",
      "Iteration 779: loss 0.043058641254901886\n",
      "Iteration 780: loss 0.04287886992096901\n",
      "Iteration 781: loss 0.042699553072452545\n",
      "Iteration 782: loss 0.04252069443464279\n",
      "Iteration 783: loss 0.042342409491539\n",
      "Iteration 784: loss 0.042164482176303864\n",
      "Iteration 785: loss 0.041987139731645584\n",
      "Iteration 786: loss 0.04181034862995148\n",
      "Iteration 787: loss 0.041634004563093185\n",
      "Iteration 788: loss 0.041458141058683395\n",
      "Iteration 789: loss 0.04128274321556091\n",
      "Iteration 790: loss 0.041107889264822006\n",
      "Iteration 791: loss 0.04093353822827339\n",
      "Iteration 792: loss 0.040759697556495667\n",
      "Iteration 793: loss 0.040586378425359726\n",
      "Iteration 794: loss 0.04041346162557602\n",
      "Iteration 795: loss 0.040241073817014694\n",
      "Iteration 796: loss 0.04006922245025635\n",
      "Iteration 797: loss 0.03989780694246292\n",
      "Iteration 798: loss 0.03972702473402023\n",
      "Iteration 799: loss 0.039556629955768585\n",
      "Iteration 800: loss 0.03938676044344902\n",
      "Iteration 801: loss 0.03921741247177124\n",
      "Iteration 802: loss 0.039048511534929276\n",
      "Iteration 803: loss 0.03888014331459999\n",
      "Iteration 804: loss 0.038712210953235626\n",
      "Iteration 805: loss 0.03854489326477051\n",
      "Iteration 806: loss 0.0383780412375927\n",
      "Iteration 807: loss 0.038211651146411896\n",
      "Iteration 808: loss 0.038045771420001984\n",
      "Iteration 809: loss 0.03788040578365326\n",
      "Iteration 810: loss 0.037715502083301544\n",
      "Iteration 811: loss 0.03755106404423714\n",
      "Iteration 812: loss 0.03738725930452347\n",
      "Iteration 813: loss 0.03722384572029114\n",
      "Iteration 814: loss 0.037060968577861786\n",
      "Iteration 815: loss 0.03689854219555855\n",
      "Iteration 816: loss 0.03673666715621948\n",
      "Iteration 817: loss 0.03657526522874832\n",
      "Iteration 818: loss 0.03641435503959656\n",
      "Iteration 819: loss 0.036253947764635086\n",
      "Iteration 820: loss 0.0360940545797348\n",
      "Iteration 821: loss 0.03593463450670242\n",
      "Iteration 822: loss 0.03577577322721481\n",
      "Iteration 823: loss 0.03561733290553093\n",
      "Iteration 824: loss 0.035459499806165695\n",
      "Iteration 825: loss 0.03530201315879822\n",
      "Iteration 826: loss 0.035145103931427\n",
      "Iteration 827: loss 0.03498874604701996\n",
      "Iteration 828: loss 0.03483274579048157\n",
      "Iteration 829: loss 0.03467734530568123\n",
      "Iteration 830: loss 0.03452247753739357\n",
      "Iteration 831: loss 0.03436799719929695\n",
      "Iteration 832: loss 0.03421405702829361\n",
      "Iteration 833: loss 0.034060604870319366\n",
      "Iteration 834: loss 0.03390762582421303\n",
      "Iteration 835: loss 0.03375519439578056\n",
      "Iteration 836: loss 0.033603236079216\n",
      "Iteration 837: loss 0.033451758325099945\n",
      "Iteration 838: loss 0.03330080211162567\n",
      "Iteration 839: loss 0.03315028175711632\n",
      "Iteration 840: loss 0.03300034999847412\n",
      "Iteration 841: loss 0.03285085782408714\n",
      "Iteration 842: loss 0.032701849937438965\n",
      "Iteration 843: loss 0.032553356140851974\n",
      "Iteration 844: loss 0.032405395060777664\n",
      "Iteration 845: loss 0.032257772982120514\n",
      "Iteration 846: loss 0.03211081027984619\n",
      "Iteration 847: loss 0.03196423500776291\n",
      "Iteration 848: loss 0.03181818872690201\n",
      "Iteration 849: loss 0.031672656536102295\n",
      "Iteration 850: loss 0.031527601182460785\n",
      "Iteration 851: loss 0.03138302266597748\n",
      "Iteration 852: loss 0.03123890608549118\n",
      "Iteration 853: loss 0.03109530732035637\n",
      "Iteration 854: loss 0.030952222645282745\n",
      "Iteration 855: loss 0.03080962598323822\n",
      "Iteration 856: loss 0.03066748008131981\n",
      "Iteration 857: loss 0.030525870621204376\n",
      "Iteration 858: loss 0.030384689569473267\n",
      "Iteration 859: loss 0.030243970453739166\n",
      "Iteration 860: loss 0.030103811994194984\n",
      "Iteration 861: loss 0.02996418997645378\n",
      "Iteration 862: loss 0.02982492372393608\n",
      "Iteration 863: loss 0.02968621253967285\n",
      "Iteration 864: loss 0.029547926038503647\n",
      "Iteration 865: loss 0.029410192742943764\n",
      "Iteration 866: loss 0.029272910207509995\n",
      "Iteration 867: loss 0.029136136174201965\n",
      "Iteration 868: loss 0.0289998147636652\n",
      "Iteration 869: loss 0.028863921761512756\n",
      "Iteration 870: loss 0.028728611767292023\n",
      "Iteration 871: loss 0.028593720868229866\n",
      "Iteration 872: loss 0.028459317982196808\n",
      "Iteration 873: loss 0.028325462713837624\n",
      "Iteration 874: loss 0.02819203957915306\n",
      "Iteration 875: loss 0.02805905044078827\n",
      "Iteration 876: loss 0.02792659029364586\n",
      "Iteration 877: loss 0.02779458649456501\n",
      "Iteration 878: loss 0.02766304463148117\n",
      "Iteration 879: loss 0.027532033622264862\n",
      "Iteration 880: loss 0.02740149199962616\n",
      "Iteration 881: loss 0.027271348983049393\n",
      "Iteration 882: loss 0.02714177966117859\n",
      "Iteration 883: loss 0.027012642472982407\n",
      "Iteration 884: loss 0.026883944869041443\n",
      "Iteration 885: loss 0.02675577811896801\n",
      "Iteration 886: loss 0.026628002524375916\n",
      "Iteration 887: loss 0.026500806212425232\n",
      "Iteration 888: loss 0.02637401595711708\n",
      "Iteration 889: loss 0.02624773606657982\n",
      "Iteration 890: loss 0.026121878996491432\n",
      "Iteration 891: loss 0.025996487587690353\n",
      "Iteration 892: loss 0.02587161771953106\n",
      "Iteration 893: loss 0.02574719861149788\n",
      "Iteration 894: loss 0.02562321349978447\n",
      "Iteration 895: loss 0.02549966610968113\n",
      "Iteration 896: loss 0.025376681238412857\n",
      "Iteration 897: loss 0.025254134088754654\n",
      "Iteration 898: loss 0.025131992995738983\n",
      "Iteration 899: loss 0.02501041069626808\n",
      "Iteration 900: loss 0.024889204651117325\n",
      "Iteration 901: loss 0.024768508970737457\n",
      "Iteration 902: loss 0.024648208171129227\n",
      "Iteration 903: loss 0.02452843077480793\n",
      "Iteration 904: loss 0.02440914511680603\n",
      "Iteration 905: loss 0.024290207773447037\n",
      "Iteration 906: loss 0.02417183294892311\n",
      "Iteration 907: loss 0.024053920060396194\n",
      "Iteration 908: loss 0.023936381563544273\n",
      "Iteration 909: loss 0.02381936088204384\n",
      "Iteration 910: loss 0.023702768608927727\n",
      "Iteration 911: loss 0.023586655035614967\n",
      "Iteration 912: loss 0.02347094565629959\n",
      "Iteration 913: loss 0.02335570566356182\n",
      "Iteration 914: loss 0.02324092574417591\n",
      "Iteration 915: loss 0.023126639425754547\n",
      "Iteration 916: loss 0.023012790828943253\n",
      "Iteration 917: loss 0.02289935015141964\n",
      "Iteration 918: loss 0.02278638444840908\n",
      "Iteration 919: loss 0.02267390675842762\n",
      "Iteration 920: loss 0.02256184071302414\n",
      "Iteration 921: loss 0.02245020493865013\n",
      "Iteration 922: loss 0.02233900874853134\n",
      "Iteration 923: loss 0.022228306159377098\n",
      "Iteration 924: loss 0.022117979824543\n",
      "Iteration 925: loss 0.022008121013641357\n",
      "Iteration 926: loss 0.02189873531460762\n",
      "Iteration 927: loss 0.021789787337183952\n",
      "Iteration 928: loss 0.021681293845176697\n",
      "Iteration 929: loss 0.021573130041360855\n",
      "Iteration 930: loss 0.021465562283992767\n",
      "Iteration 931: loss 0.021358314901590347\n",
      "Iteration 932: loss 0.0212516151368618\n",
      "Iteration 933: loss 0.021145276725292206\n",
      "Iteration 934: loss 0.021039381623268127\n",
      "Iteration 935: loss 0.020933911204338074\n",
      "Iteration 936: loss 0.020828863605856895\n",
      "Iteration 937: loss 0.020724322646856308\n",
      "Iteration 938: loss 0.02062016725540161\n",
      "Iteration 939: loss 0.02051641047000885\n",
      "Iteration 940: loss 0.020413096994161606\n",
      "Iteration 941: loss 0.020310252904891968\n",
      "Iteration 942: loss 0.020207831636071205\n",
      "Iteration 943: loss 0.02010582759976387\n",
      "Iteration 944: loss 0.020004240795969963\n",
      "Iteration 945: loss 0.019903074949979782\n",
      "Iteration 946: loss 0.019802382215857506\n",
      "Iteration 947: loss 0.019702058285474777\n",
      "Iteration 948: loss 0.01960217021405697\n",
      "Iteration 949: loss 0.019502736628055573\n",
      "Iteration 950: loss 0.019403677433729172\n",
      "Iteration 951: loss 0.01930510438978672\n",
      "Iteration 952: loss 0.01920684427022934\n",
      "Iteration 953: loss 0.019109096378087997\n",
      "Iteration 954: loss 0.019011683762073517\n",
      "Iteration 955: loss 0.018914775922894478\n",
      "Iteration 956: loss 0.01881822943687439\n",
      "Iteration 957: loss 0.01872209645807743\n",
      "Iteration 958: loss 0.01862638257443905\n",
      "Iteration 959: loss 0.01853109709918499\n",
      "Iteration 960: loss 0.01843620091676712\n",
      "Iteration 961: loss 0.018341735005378723\n",
      "Iteration 962: loss 0.01824766770005226\n",
      "Iteration 963: loss 0.01815403625369072\n",
      "Iteration 964: loss 0.01806078851222992\n",
      "Iteration 965: loss 0.017967961728572845\n",
      "Iteration 966: loss 0.017875544726848602\n",
      "Iteration 967: loss 0.017783520743250847\n",
      "Iteration 968: loss 0.017691876739263535\n",
      "Iteration 969: loss 0.01760067418217659\n",
      "Iteration 970: loss 0.01750990003347397\n",
      "Iteration 971: loss 0.017419395968317986\n",
      "Iteration 972: loss 0.017329376190900803\n",
      "Iteration 973: loss 0.01723979040980339\n",
      "Iteration 974: loss 0.017150521278381348\n",
      "Iteration 975: loss 0.017061688005924225\n",
      "Iteration 976: loss 0.016973217949271202\n",
      "Iteration 977: loss 0.016885191202163696\n",
      "Iteration 978: loss 0.01679760217666626\n",
      "Iteration 979: loss 0.016710326075553894\n",
      "Iteration 980: loss 0.016623469069600105\n",
      "Iteration 981: loss 0.016536980867385864\n",
      "Iteration 982: loss 0.0164509080350399\n",
      "Iteration 983: loss 0.016365166753530502\n",
      "Iteration 984: loss 0.01627989299595356\n",
      "Iteration 985: loss 0.01619497314095497\n",
      "Iteration 986: loss 0.016110409051179886\n",
      "Iteration 987: loss 0.016026228666305542\n",
      "Iteration 988: loss 0.015942463651299477\n",
      "Iteration 989: loss 0.01585911214351654\n",
      "Iteration 990: loss 0.015776077285408974\n",
      "Iteration 991: loss 0.01569344848394394\n",
      "Iteration 992: loss 0.015611208975315094\n",
      "Iteration 993: loss 0.015529302880167961\n",
      "Iteration 994: loss 0.015447821468114853\n",
      "Iteration 995: loss 0.015366640873253345\n",
      "Iteration 996: loss 0.015285923145711422\n",
      "Iteration 997: loss 0.015205524861812592\n",
      "Iteration 998: loss 0.01512552984058857\n",
      "Iteration 999: loss 0.01504587009549141\n",
      "Iteration 1000: loss 0.014966600574553013\n",
      "Iteration 1001: loss 0.014887725934386253\n",
      "Iteration 1002: loss 0.014809202402830124\n",
      "Iteration 1003: loss 0.014731032773852348\n",
      "Iteration 1004: loss 0.014653218910098076\n",
      "Iteration 1005: loss 0.014575819484889507\n",
      "Iteration 1006: loss 0.014498686417937279\n",
      "Iteration 1007: loss 0.01442202739417553\n",
      "Iteration 1008: loss 0.014345656149089336\n",
      "Iteration 1009: loss 0.014269668608903885\n",
      "Iteration 1010: loss 0.014193986542522907\n",
      "Iteration 1011: loss 0.014118727296590805\n",
      "Iteration 1012: loss 0.014043811708688736\n",
      "Iteration 1013: loss 0.013969260267913342\n",
      "Iteration 1014: loss 0.013895070180296898\n",
      "Iteration 1015: loss 0.013821200467646122\n",
      "Iteration 1016: loss 0.013747688382863998\n",
      "Iteration 1017: loss 0.013674531131982803\n",
      "Iteration 1018: loss 0.01360175758600235\n",
      "Iteration 1019: loss 0.013529262505471706\n",
      "Iteration 1020: loss 0.013457178138196468\n",
      "Iteration 1021: loss 0.01338542252779007\n",
      "Iteration 1022: loss 0.01331394538283348\n",
      "Iteration 1023: loss 0.013242914341390133\n",
      "Iteration 1024: loss 0.013172167353332043\n",
      "Iteration 1025: loss 0.013101790100336075\n",
      "Iteration 1026: loss 0.013031728565692902\n",
      "Iteration 1027: loss 0.012962039560079575\n",
      "Iteration 1028: loss 0.012892640195786953\n",
      "Iteration 1029: loss 0.012823629193007946\n",
      "Iteration 1030: loss 0.012754946015775204\n",
      "Iteration 1031: loss 0.012686556205153465\n",
      "Iteration 1032: loss 0.012618502601981163\n",
      "Iteration 1033: loss 0.012550847604870796\n",
      "Iteration 1034: loss 0.012483470141887665\n",
      "Iteration 1035: loss 0.012416450306773186\n",
      "Iteration 1036: loss 0.012349735014140606\n",
      "Iteration 1037: loss 0.01228337548673153\n",
      "Iteration 1038: loss 0.01221734844148159\n",
      "Iteration 1039: loss 0.012151614762842655\n",
      "Iteration 1040: loss 0.012086230330169201\n",
      "Iteration 1041: loss 0.012021144852042198\n",
      "Iteration 1042: loss 0.011956430971622467\n",
      "Iteration 1043: loss 0.011891988106071949\n",
      "Iteration 1044: loss 0.011827871203422546\n",
      "Iteration 1045: loss 0.01176409050822258\n",
      "Iteration 1046: loss 0.011700637638568878\n",
      "Iteration 1047: loss 0.011637469753623009\n",
      "Iteration 1048: loss 0.011574650183320045\n",
      "Iteration 1049: loss 0.01151212677359581\n",
      "Iteration 1050: loss 0.011449890211224556\n",
      "Iteration 1051: loss 0.011388007551431656\n",
      "Iteration 1052: loss 0.011326435953378677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1053: loss 0.011265155859291553\n",
      "Iteration 1054: loss 0.011204239912331104\n",
      "Iteration 1055: loss 0.011143533512949944\n",
      "Iteration 1056: loss 0.011083172634243965\n",
      "Iteration 1057: loss 0.011023161932826042\n",
      "Iteration 1058: loss 0.010963420383632183\n",
      "Iteration 1059: loss 0.010904001072049141\n",
      "Iteration 1060: loss 0.010844847187399864\n",
      "Iteration 1061: loss 0.010786037892103195\n",
      "Iteration 1062: loss 0.010727516375482082\n",
      "Iteration 1063: loss 0.01066927146166563\n",
      "Iteration 1064: loss 0.010611332952976227\n",
      "Iteration 1065: loss 0.010553683154284954\n",
      "Iteration 1066: loss 0.010496355593204498\n",
      "Iteration 1067: loss 0.01043932605534792\n",
      "Iteration 1068: loss 0.01038256473839283\n",
      "Iteration 1069: loss 0.010326116345822811\n",
      "Iteration 1070: loss 0.010269967839121819\n",
      "Iteration 1071: loss 0.010214097797870636\n",
      "Iteration 1072: loss 0.01015849132090807\n",
      "Iteration 1073: loss 0.010103225708007812\n",
      "Iteration 1074: loss 0.010048199445009232\n",
      "Iteration 1075: loss 0.009993446990847588\n",
      "Iteration 1076: loss 0.009939013049006462\n",
      "Iteration 1077: loss 0.009884843602776527\n",
      "Iteration 1078: loss 0.009830983355641365\n",
      "Iteration 1079: loss 0.009777389466762543\n",
      "Iteration 1080: loss 0.009724077768623829\n",
      "Iteration 1081: loss 0.009671054780483246\n",
      "Iteration 1082: loss 0.009618300013244152\n",
      "Iteration 1083: loss 0.009565833956003189\n",
      "Iteration 1084: loss 0.009513630531728268\n",
      "Iteration 1085: loss 0.009461703710258007\n",
      "Iteration 1086: loss 0.009410031139850616\n",
      "Iteration 1087: loss 0.009358678013086319\n",
      "Iteration 1088: loss 0.009307557716965675\n",
      "Iteration 1089: loss 0.0092566953971982\n",
      "Iteration 1090: loss 0.009206140413880348\n",
      "Iteration 1091: loss 0.009155833162367344\n",
      "Iteration 1092: loss 0.009105836972594261\n",
      "Iteration 1093: loss 0.009056062437593937\n",
      "Iteration 1094: loss 0.009006567299365997\n",
      "Iteration 1095: loss 0.00895729847252369\n",
      "Iteration 1096: loss 0.008908344432711601\n",
      "Iteration 1097: loss 0.008859636262059212\n",
      "Iteration 1098: loss 0.008811168372631073\n",
      "Iteration 1099: loss 0.008762975223362446\n",
      "Iteration 1100: loss 0.008715024217963219\n",
      "Iteration 1101: loss 0.008667360059916973\n",
      "Iteration 1102: loss 0.008619912900030613\n",
      "Iteration 1103: loss 0.008572785183787346\n",
      "Iteration 1104: loss 0.008525852113962173\n",
      "Iteration 1105: loss 0.00847921147942543\n",
      "Iteration 1106: loss 0.008432813920080662\n",
      "Iteration 1107: loss 0.00838666781783104\n",
      "Iteration 1108: loss 0.008340748958289623\n",
      "Iteration 1109: loss 0.00829508900642395\n",
      "Iteration 1110: loss 0.00824968796223402\n",
      "Iteration 1111: loss 0.008204512298107147\n",
      "Iteration 1112: loss 0.008159622550010681\n",
      "Iteration 1113: loss 0.008114941418170929\n",
      "Iteration 1114: loss 0.00807051733136177\n",
      "Iteration 1115: loss 0.00802634097635746\n",
      "Iteration 1116: loss 0.007982401177287102\n",
      "Iteration 1117: loss 0.007938661612570286\n",
      "Iteration 1118: loss 0.007895238697528839\n",
      "Iteration 1119: loss 0.007852001115679741\n",
      "Iteration 1120: loss 0.007809020578861237\n",
      "Iteration 1121: loss 0.007766241207718849\n",
      "Iteration 1122: loss 0.0077237579971551895\n",
      "Iteration 1123: loss 0.007681476883590221\n",
      "Iteration 1124: loss 0.00763943325728178\n",
      "Iteration 1125: loss 0.007597628980875015\n",
      "Iteration 1126: loss 0.007556046359241009\n",
      "Iteration 1127: loss 0.007514692842960358\n",
      "Iteration 1128: loss 0.007473559584468603\n",
      "Iteration 1129: loss 0.007432668469846249\n",
      "Iteration 1130: loss 0.007391994819045067\n",
      "Iteration 1131: loss 0.007351557724177837\n",
      "Iteration 1132: loss 0.007311360910534859\n",
      "Iteration 1133: loss 0.007271372713148594\n",
      "Iteration 1134: loss 0.007231610827147961\n",
      "Iteration 1135: loss 0.007192056626081467\n",
      "Iteration 1136: loss 0.007152735721319914\n",
      "Iteration 1137: loss 0.007113673724234104\n",
      "Iteration 1138: loss 0.0070747570134699345\n",
      "Iteration 1139: loss 0.007036086171865463\n",
      "Iteration 1140: loss 0.006997660733759403\n",
      "Iteration 1141: loss 0.006959398277103901\n",
      "Iteration 1142: loss 0.0069213686510920525\n",
      "Iteration 1143: loss 0.006883583497256041\n",
      "Iteration 1144: loss 0.006846009753644466\n",
      "Iteration 1145: loss 0.006808635778725147\n",
      "Iteration 1146: loss 0.006771470420062542\n",
      "Iteration 1147: loss 0.006734510418027639\n",
      "Iteration 1148: loss 0.006697766482830048\n",
      "Iteration 1149: loss 0.006661220453679562\n",
      "Iteration 1150: loss 0.00662488117814064\n",
      "Iteration 1151: loss 0.006588773801922798\n",
      "Iteration 1152: loss 0.006552864797413349\n",
      "Iteration 1153: loss 0.006517147645354271\n",
      "Iteration 1154: loss 0.00648163165897131\n",
      "Iteration 1155: loss 0.006446308922022581\n",
      "Iteration 1156: loss 0.006411201320588589\n",
      "Iteration 1157: loss 0.006376327481120825\n",
      "Iteration 1158: loss 0.00634163711220026\n",
      "Iteration 1159: loss 0.006307130679488182\n",
      "Iteration 1160: loss 0.006272795610129833\n",
      "Iteration 1161: loss 0.006238715257495642\n",
      "Iteration 1162: loss 0.006204760633409023\n",
      "Iteration 1163: loss 0.006171051412820816\n",
      "Iteration 1164: loss 0.006137543823570013\n",
      "Iteration 1165: loss 0.006104182451963425\n",
      "Iteration 1166: loss 0.00607105391100049\n",
      "Iteration 1167: loss 0.00603813910856843\n",
      "Iteration 1168: loss 0.006005333736538887\n",
      "Iteration 1169: loss 0.0059727816842496395\n",
      "Iteration 1170: loss 0.0059404089115560055\n",
      "Iteration 1171: loss 0.005908228922635317\n",
      "Iteration 1172: loss 0.005876203998923302\n",
      "Iteration 1173: loss 0.005844396539032459\n",
      "Iteration 1174: loss 0.005812773481011391\n",
      "Iteration 1175: loss 0.005781332030892372\n",
      "Iteration 1176: loss 0.005750052165240049\n",
      "Iteration 1177: loss 0.00571898277848959\n",
      "Iteration 1178: loss 0.005688052158802748\n",
      "Iteration 1179: loss 0.005657350178807974\n",
      "Iteration 1180: loss 0.005626817233860493\n",
      "Iteration 1181: loss 0.005596436560153961\n",
      "Iteration 1182: loss 0.005566263571381569\n",
      "Iteration 1183: loss 0.00553625263273716\n",
      "Iteration 1184: loss 0.005506439134478569\n",
      "Iteration 1185: loss 0.005476757418364286\n",
      "Iteration 1186: loss 0.005447289440780878\n",
      "Iteration 1187: loss 0.005417958367615938\n",
      "Iteration 1188: loss 0.005388826597481966\n",
      "Iteration 1189: loss 0.005359875038266182\n",
      "Iteration 1190: loss 0.00533106317743659\n",
      "Iteration 1191: loss 0.005302460398525\n",
      "Iteration 1192: loss 0.005274033173918724\n",
      "Iteration 1193: loss 0.005245730746537447\n",
      "Iteration 1194: loss 0.005217617377638817\n",
      "Iteration 1195: loss 0.005189636722207069\n",
      "Iteration 1196: loss 0.00516186747699976\n",
      "Iteration 1197: loss 0.005134254228323698\n",
      "Iteration 1198: loss 0.005106794647872448\n",
      "Iteration 1199: loss 0.005079499911516905\n",
      "Iteration 1200: loss 0.005052383989095688\n",
      "Iteration 1201: loss 0.005025443620979786\n",
      "Iteration 1202: loss 0.004998614080250263\n",
      "Iteration 1203: loss 0.004971949849277735\n",
      "Iteration 1204: loss 0.0049454583786427975\n",
      "Iteration 1205: loss 0.004919133614748716\n",
      "Iteration 1206: loss 0.004892981145530939\n",
      "Iteration 1207: loss 0.004866943694651127\n",
      "Iteration 1208: loss 0.0048410831950604916\n",
      "Iteration 1209: loss 0.004815385676920414\n",
      "Iteration 1210: loss 0.004789857193827629\n",
      "Iteration 1211: loss 0.004764433950185776\n",
      "Iteration 1212: loss 0.004739220719784498\n",
      "Iteration 1213: loss 0.004714115522801876\n",
      "Iteration 1214: loss 0.004689214751124382\n",
      "Iteration 1215: loss 0.0046643721871078014\n",
      "Iteration 1216: loss 0.0046397787518799305\n",
      "Iteration 1217: loss 0.004615267738699913\n",
      "Iteration 1218: loss 0.0045909336768090725\n",
      "Iteration 1219: loss 0.004566733725368977\n",
      "Iteration 1220: loss 0.004542691633105278\n",
      "Iteration 1221: loss 0.004518784116953611\n",
      "Iteration 1222: loss 0.004495047964155674\n",
      "Iteration 1223: loss 0.004471414722502232\n",
      "Iteration 1224: loss 0.004447947256267071\n",
      "Iteration 1225: loss 0.004424612037837505\n",
      "Iteration 1226: loss 0.004401435609906912\n",
      "Iteration 1227: loss 0.00437838165089488\n",
      "Iteration 1228: loss 0.004355502314865589\n",
      "Iteration 1229: loss 0.004332743585109711\n",
      "Iteration 1230: loss 0.004310127347707748\n",
      "Iteration 1231: loss 0.0042876023799180984\n",
      "Iteration 1232: loss 0.004265273455530405\n",
      "Iteration 1233: loss 0.00424306932836771\n",
      "Iteration 1234: loss 0.004220995586365461\n",
      "Iteration 1235: loss 0.004199059680104256\n",
      "Iteration 1236: loss 0.004177240189164877\n",
      "Iteration 1237: loss 0.004155593924224377\n",
      "Iteration 1238: loss 0.004134037997573614\n",
      "Iteration 1239: loss 0.004112624563276768\n",
      "Iteration 1240: loss 0.004091374110430479\n",
      "Iteration 1241: loss 0.004070246126502752\n",
      "Iteration 1242: loss 0.00404922291636467\n",
      "Iteration 1243: loss 0.004028319846838713\n",
      "Iteration 1244: loss 0.004007576499134302\n",
      "Iteration 1245: loss 0.003986949101090431\n",
      "Iteration 1246: loss 0.003966457676142454\n",
      "Iteration 1247: loss 0.003946091514080763\n",
      "Iteration 1248: loss 0.003925845958292484\n",
      "Iteration 1249: loss 0.003905731253325939\n",
      "Iteration 1250: loss 0.0038857259787619114\n",
      "Iteration 1251: loss 0.00386588042601943\n",
      "Iteration 1252: loss 0.003846124280244112\n",
      "Iteration 1253: loss 0.0038265076000243425\n",
      "Iteration 1254: loss 0.0038069949951022863\n",
      "Iteration 1255: loss 0.003787611611187458\n",
      "Iteration 1256: loss 0.0037683723494410515\n",
      "Iteration 1257: loss 0.003749219235032797\n",
      "Iteration 1258: loss 0.0037301951088011265\n",
      "Iteration 1259: loss 0.003711313009262085\n",
      "Iteration 1260: loss 0.0036925068125128746\n",
      "Iteration 1261: loss 0.0036738638300448656\n",
      "Iteration 1262: loss 0.003655320033431053\n",
      "Iteration 1263: loss 0.0036368914879858494\n",
      "Iteration 1264: loss 0.003618563525378704\n",
      "Iteration 1265: loss 0.003600359195843339\n",
      "Iteration 1266: loss 0.0035822547506541014\n",
      "Iteration 1267: loss 0.003564284648746252\n",
      "Iteration 1268: loss 0.003546421881765127\n",
      "Iteration 1269: loss 0.0035286869388073683\n",
      "Iteration 1270: loss 0.0035110381431877613\n",
      "Iteration 1271: loss 0.0034935197327286005\n",
      "Iteration 1272: loss 0.0034761051647365093\n",
      "Iteration 1273: loss 0.0034587911795824766\n",
      "Iteration 1274: loss 0.0034416038542985916\n",
      "Iteration 1275: loss 0.003424503840506077\n",
      "Iteration 1276: loss 0.003407541196793318\n",
      "Iteration 1277: loss 0.003390661207959056\n",
      "Iteration 1278: loss 0.0033738920465111732\n",
      "Iteration 1279: loss 0.0033572367392480373\n",
      "Iteration 1280: loss 0.003340667113661766\n",
      "Iteration 1281: loss 0.0033242153003811836\n",
      "Iteration 1282: loss 0.00330787873826921\n",
      "Iteration 1283: loss 0.0032916232012212276\n",
      "Iteration 1284: loss 0.0032755094580352306\n",
      "Iteration 1285: loss 0.0032594562508165836\n",
      "Iteration 1286: loss 0.003243548097088933\n",
      "Iteration 1287: loss 0.0032276855781674385\n",
      "Iteration 1288: loss 0.003211966250091791\n",
      "Iteration 1289: loss 0.0031963279470801353\n",
      "Iteration 1290: loss 0.0031807890627533197\n",
      "Iteration 1291: loss 0.003165367990732193\n",
      "Iteration 1292: loss 0.003150041215121746\n",
      "Iteration 1293: loss 0.0031347903423011303\n",
      "Iteration 1294: loss 0.003119663568213582\n",
      "Iteration 1295: loss 0.0031046171206980944\n",
      "Iteration 1296: loss 0.0030896631069481373\n",
      "Iteration 1297: loss 0.003074826207011938\n",
      "Iteration 1298: loss 0.003060048446059227\n",
      "Iteration 1299: loss 0.0030454127117991447\n",
      "Iteration 1300: loss 0.0030308347195386887\n",
      "Iteration 1301: loss 0.003016365459188819\n",
      "Iteration 1302: loss 0.0030019867699593306\n",
      "Iteration 1303: loss 0.0029876944608986378\n",
      "Iteration 1304: loss 0.002973493654280901\n",
      "Iteration 1305: loss 0.002959383884444833\n",
      "Iteration 1306: loss 0.0029453784227371216\n",
      "Iteration 1307: loss 0.00293146469630301\n",
      "Iteration 1308: loss 0.002917639445513487\n",
      "Iteration 1309: loss 0.002903875196352601\n",
      "Iteration 1310: loss 0.0028902157209813595\n",
      "Iteration 1311: loss 0.0028766547329723835\n",
      "Iteration 1312: loss 0.002863184316083789\n",
      "Iteration 1313: loss 0.0028497849125415087\n",
      "Iteration 1314: loss 0.002836484694853425\n",
      "Iteration 1315: loss 0.002823249204084277\n",
      "Iteration 1316: loss 0.0028101287316530943\n",
      "Iteration 1317: loss 0.0027970662340521812\n",
      "Iteration 1318: loss 0.0027841110713779926\n",
      "Iteration 1319: loss 0.0027712369337677956\n",
      "Iteration 1320: loss 0.0027584456838667393\n",
      "Iteration 1321: loss 0.0027457275427877903\n",
      "Iteration 1322: loss 0.002733092289417982\n",
      "Iteration 1323: loss 0.0027205385267734528\n",
      "Iteration 1324: loss 0.002708075800910592\n",
      "Iteration 1325: loss 0.00269569200463593\n",
      "Iteration 1326: loss 0.002683381550014019\n",
      "Iteration 1327: loss 0.002671157708391547\n",
      "Iteration 1328: loss 0.0026590153574943542\n",
      "Iteration 1329: loss 0.0026469596195966005\n",
      "Iteration 1330: loss 0.002634964417666197\n",
      "Iteration 1331: loss 0.002623051404953003\n",
      "Iteration 1332: loss 0.002611227799206972\n",
      "Iteration 1333: loss 0.0025994766037911177\n",
      "Iteration 1334: loss 0.002587797585874796\n",
      "Iteration 1335: loss 0.0025762091390788555\n",
      "Iteration 1336: loss 0.0025646646972745657\n",
      "Iteration 1337: loss 0.002553208963945508\n",
      "Iteration 1338: loss 0.0025418391451239586\n",
      "Iteration 1339: loss 0.0025305496528744698\n",
      "Iteration 1340: loss 0.0025193318724632263\n",
      "Iteration 1341: loss 0.00250816997140646\n",
      "Iteration 1342: loss 0.0024971025995910168\n",
      "Iteration 1343: loss 0.0024860911071300507\n",
      "Iteration 1344: loss 0.002475180197507143\n",
      "Iteration 1345: loss 0.002464295132085681\n",
      "Iteration 1346: loss 0.002453508786857128\n",
      "Iteration 1347: loss 0.0024427955504506826\n",
      "Iteration 1348: loss 0.0024321340024471283\n",
      "Iteration 1349: loss 0.0024215702433139086\n",
      "Iteration 1350: loss 0.0024110658559948206\n",
      "Iteration 1351: loss 0.0024006192106753588\n",
      "Iteration 1352: loss 0.0023902517277747393\n",
      "Iteration 1353: loss 0.002379960147663951\n",
      "Iteration 1354: loss 0.00236970791593194\n",
      "Iteration 1355: loss 0.0023595558013767004\n",
      "Iteration 1356: loss 0.0023494642227888107\n",
      "Iteration 1357: loss 0.002339421771466732\n",
      "Iteration 1358: loss 0.0023294631391763687\n",
      "Iteration 1359: loss 0.0023195678368210793\n",
      "Iteration 1360: loss 0.002309724222868681\n",
      "Iteration 1361: loss 0.00229996582493186\n",
      "Iteration 1362: loss 0.0022902542259544134\n",
      "Iteration 1363: loss 0.0022806264460086823\n",
      "Iteration 1364: loss 0.0022710440680384636\n",
      "Iteration 1365: loss 0.0022615366615355015\n",
      "Iteration 1366: loss 0.0022520979400724173\n",
      "Iteration 1367: loss 0.002242711139842868\n",
      "Iteration 1368: loss 0.0022333823144435883\n",
      "Iteration 1369: loss 0.0022241335827857256\n",
      "Iteration 1370: loss 0.0022149360738694668\n",
      "Iteration 1371: loss 0.002205796539783478\n",
      "Iteration 1372: loss 0.0021967354696244\n",
      "Iteration 1373: loss 0.0021877202671021223\n",
      "Iteration 1374: loss 0.0021787513978779316\n",
      "Iteration 1375: loss 0.0021698633208870888\n",
      "Iteration 1376: loss 0.0021610248368233442\n",
      "Iteration 1377: loss 0.002152255503460765\n",
      "Iteration 1378: loss 0.0021435366943478584\n",
      "Iteration 1379: loss 0.0021348800510168076\n",
      "Iteration 1380: loss 0.002126275096088648\n",
      "Iteration 1381: loss 0.0021177304442971945\n",
      "Iteration 1382: loss 0.002109246328473091\n",
      "Iteration 1383: loss 0.002100813202559948\n",
      "Iteration 1384: loss 0.00209243968129158\n",
      "Iteration 1385: loss 0.0020841415971517563\n",
      "Iteration 1386: loss 0.002075871219858527\n",
      "Iteration 1387: loss 0.0020676518324762583\n",
      "Iteration 1388: loss 0.0020595083478838205\n",
      "Iteration 1389: loss 0.0020514209754765034\n",
      "Iteration 1390: loss 0.0020433783065527678\n",
      "Iteration 1391: loss 0.002035395009443164\n",
      "Iteration 1392: loss 0.0020274571143090725\n",
      "Iteration 1393: loss 0.00201957649551332\n",
      "Iteration 1394: loss 0.002011745236814022\n",
      "Iteration 1395: loss 0.002003980800509453\n",
      "Iteration 1396: loss 0.0019962675869464874\n",
      "Iteration 1397: loss 0.001988594653084874\n",
      "Iteration 1398: loss 0.0019809792283922434\n",
      "Iteration 1399: loss 0.001973415957763791\n",
      "Iteration 1400: loss 0.0019658838864415884\n",
      "Iteration 1401: loss 0.0019584358669817448\n",
      "Iteration 1402: loss 0.0019510315032675862\n",
      "Iteration 1403: loss 0.0019436611328274012\n",
      "Iteration 1404: loss 0.001936348620802164\n",
      "Iteration 1405: loss 0.001929075107909739\n",
      "Iteration 1406: loss 0.0019218744710087776\n",
      "Iteration 1407: loss 0.0019146957201883197\n",
      "Iteration 1408: loss 0.001907584024593234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1409: loss 0.00190051831305027\n",
      "Iteration 1410: loss 0.0018934942781925201\n",
      "Iteration 1411: loss 0.001886528916656971\n",
      "Iteration 1412: loss 0.0018796025542542338\n",
      "Iteration 1413: loss 0.0018727200804278255\n",
      "Iteration 1414: loss 0.0018658919725567102\n",
      "Iteration 1415: loss 0.001859109615907073\n",
      "Iteration 1416: loss 0.001852356712333858\n",
      "Iteration 1417: loss 0.001845671678893268\n",
      "Iteration 1418: loss 0.0018390296027064323\n",
      "Iteration 1419: loss 0.0018324258271604776\n",
      "Iteration 1420: loss 0.001825862331315875\n",
      "Iteration 1421: loss 0.0018193586729466915\n",
      "Iteration 1422: loss 0.0018128798110410571\n",
      "Iteration 1423: loss 0.00180646195076406\n",
      "Iteration 1424: loss 0.0018000822747126222\n",
      "Iteration 1425: loss 0.0017937524244189262\n",
      "Iteration 1426: loss 0.0017874494660645723\n",
      "Iteration 1427: loss 0.0017811993602663279\n",
      "Iteration 1428: loss 0.0017749908147379756\n",
      "Iteration 1429: loss 0.0017688348889350891\n",
      "Iteration 1430: loss 0.0017627074848860502\n",
      "Iteration 1431: loss 0.0017566271126270294\n",
      "Iteration 1432: loss 0.001750580850057304\n",
      "Iteration 1433: loss 0.0017445856938138604\n",
      "Iteration 1434: loss 0.0017386290710419416\n",
      "Iteration 1435: loss 0.0017327172681689262\n",
      "Iteration 1436: loss 0.0017268352676182985\n",
      "Iteration 1437: loss 0.0017210084479302168\n",
      "Iteration 1438: loss 0.0017152104992419481\n",
      "Iteration 1439: loss 0.0017094602808356285\n",
      "Iteration 1440: loss 0.001703740912489593\n",
      "Iteration 1441: loss 0.0016980620566755533\n",
      "Iteration 1442: loss 0.0016924234805628657\n",
      "Iteration 1443: loss 0.0016868236707523465\n",
      "Iteration 1444: loss 0.00168125843629241\n",
      "Iteration 1445: loss 0.0016757515259087086\n",
      "Iteration 1446: loss 0.0016702638240531087\n",
      "Iteration 1447: loss 0.0016648105811327696\n",
      "Iteration 1448: loss 0.0016594012267887592\n",
      "Iteration 1449: loss 0.0016540372744202614\n",
      "Iteration 1450: loss 0.0016486968379467726\n",
      "Iteration 1451: loss 0.0016433985438197851\n",
      "Iteration 1452: loss 0.0016381414607167244\n",
      "Iteration 1453: loss 0.0016329172067344189\n",
      "Iteration 1454: loss 0.0016277243848890066\n",
      "Iteration 1455: loss 0.0016225655563175678\n",
      "Iteration 1456: loss 0.0016174587653949857\n",
      "Iteration 1457: loss 0.0016123659443110228\n",
      "Iteration 1458: loss 0.0016073292354121804\n",
      "Iteration 1459: loss 0.001602309406735003\n",
      "Iteration 1460: loss 0.0015973385889083147\n",
      "Iteration 1461: loss 0.0015924018807709217\n",
      "Iteration 1462: loss 0.0015874929958954453\n",
      "Iteration 1463: loss 0.0015826091403141618\n",
      "Iteration 1464: loss 0.0015777663793414831\n",
      "Iteration 1465: loss 0.0015729658771306276\n",
      "Iteration 1466: loss 0.0015681934310123324\n",
      "Iteration 1467: loss 0.0015634461306035519\n",
      "Iteration 1468: loss 0.0015587438829243183\n",
      "Iteration 1469: loss 0.001554066315293312\n",
      "Iteration 1470: loss 0.0015494219260290265\n",
      "Iteration 1471: loss 0.0015448150224983692\n",
      "Iteration 1472: loss 0.0015402325661852956\n",
      "Iteration 1473: loss 0.0015356920193880796\n",
      "Iteration 1474: loss 0.0015311706811189651\n",
      "Iteration 1475: loss 0.0015266810078173876\n",
      "Iteration 1476: loss 0.0015222353395074606\n",
      "Iteration 1477: loss 0.0015178173780441284\n",
      "Iteration 1478: loss 0.0015134175773710012\n",
      "Iteration 1479: loss 0.0015090620145201683\n",
      "Iteration 1480: loss 0.0015047363704070449\n",
      "Iteration 1481: loss 0.0015004295855760574\n",
      "Iteration 1482: loss 0.0014961594715714455\n",
      "Iteration 1483: loss 0.0014919298700988293\n",
      "Iteration 1484: loss 0.0014877161011099815\n",
      "Iteration 1485: loss 0.0014835321344435215\n",
      "Iteration 1486: loss 0.001479378785006702\n",
      "Iteration 1487: loss 0.0014752481365576386\n",
      "Iteration 1488: loss 0.0014711651019752026\n",
      "Iteration 1489: loss 0.0014670935925096273\n",
      "Iteration 1490: loss 0.0014630511868745089\n",
      "Iteration 1491: loss 0.0014590496430173516\n",
      "Iteration 1492: loss 0.001455069868825376\n",
      "Iteration 1493: loss 0.0014511062763631344\n",
      "Iteration 1494: loss 0.0014471912290900946\n",
      "Iteration 1495: loss 0.0014432943426072598\n",
      "Iteration 1496: loss 0.0014394266763702035\n",
      "Iteration 1497: loss 0.0014355838065966964\n",
      "Iteration 1498: loss 0.0014317615423351526\n",
      "Iteration 1499: loss 0.0014279662864282727\n",
      "Iteration 1500: loss 0.0014242003671824932\n",
      "Iteration 1501: loss 0.0014204751932993531\n",
      "Iteration 1502: loss 0.0014167542103677988\n",
      "Iteration 1503: loss 0.001413076650351286\n",
      "Iteration 1504: loss 0.0014094219077378511\n",
      "Iteration 1505: loss 0.0014057797379791737\n",
      "Iteration 1506: loss 0.0014021840179339051\n",
      "Iteration 1507: loss 0.0013985889963805676\n",
      "Iteration 1508: loss 0.0013950418215245008\n",
      "Iteration 1509: loss 0.0013915039598941803\n",
      "Iteration 1510: loss 0.0013880047481507063\n",
      "Iteration 1511: loss 0.0013844993663951755\n",
      "Iteration 1512: loss 0.0013810513773933053\n",
      "Iteration 1513: loss 0.001377625740133226\n",
      "Iteration 1514: loss 0.0013742122100666165\n",
      "Iteration 1515: loss 0.0013708225451409817\n",
      "Iteration 1516: loss 0.0013674598885700107\n",
      "Iteration 1517: loss 0.0013641153927892447\n",
      "Iteration 1518: loss 0.0013608084991574287\n",
      "Iteration 1519: loss 0.0013575181365013123\n",
      "Iteration 1520: loss 0.0013542539672926068\n",
      "Iteration 1521: loss 0.0013510077260434628\n",
      "Iteration 1522: loss 0.0013477851171046495\n",
      "Iteration 1523: loss 0.0013445831136777997\n",
      "Iteration 1524: loss 0.0013414034619927406\n",
      "Iteration 1525: loss 0.0013382472097873688\n",
      "Iteration 1526: loss 0.0013351200614124537\n",
      "Iteration 1527: loss 0.0013320138677954674\n",
      "Iteration 1528: loss 0.0013289222260937095\n",
      "Iteration 1529: loss 0.0013258582912385464\n",
      "Iteration 1530: loss 0.0013228097232058644\n",
      "Iteration 1531: loss 0.0013197922380641103\n",
      "Iteration 1532: loss 0.0013167785946279764\n",
      "Iteration 1533: loss 0.0013138029025867581\n",
      "Iteration 1534: loss 0.0013108476996421814\n",
      "Iteration 1535: loss 0.0013079042546451092\n",
      "Iteration 1536: loss 0.0013049938715994358\n",
      "Iteration 1537: loss 0.0013020939659327269\n",
      "Iteration 1538: loss 0.001299216179177165\n",
      "Iteration 1539: loss 0.0012963535264134407\n",
      "Iteration 1540: loss 0.0012935177655890584\n",
      "Iteration 1541: loss 0.0012907031923532486\n",
      "Iteration 1542: loss 0.0012879096902906895\n",
      "Iteration 1543: loss 0.0012851314386352897\n",
      "Iteration 1544: loss 0.0012823785655200481\n",
      "Iteration 1545: loss 0.0012796454830095172\n",
      "Iteration 1546: loss 0.0012769270688295364\n",
      "Iteration 1547: loss 0.001274222624488175\n",
      "Iteration 1548: loss 0.0012715468183159828\n",
      "Iteration 1549: loss 0.0012688874267041683\n",
      "Iteration 1550: loss 0.0012662466615438461\n",
      "Iteration 1551: loss 0.00126362475566566\n",
      "Iteration 1552: loss 0.0012610219419002533\n",
      "Iteration 1553: loss 0.0012584340292960405\n",
      "Iteration 1554: loss 0.001255867420695722\n",
      "Iteration 1555: loss 0.001253320835530758\n",
      "Iteration 1556: loss 0.0012507911305874586\n",
      "Iteration 1557: loss 0.001248275744728744\n",
      "Iteration 1558: loss 0.0012457810807973146\n",
      "Iteration 1559: loss 0.001243306789547205\n",
      "Iteration 1560: loss 0.0012408437905833125\n",
      "Iteration 1561: loss 0.0012384101282805204\n",
      "Iteration 1562: loss 0.0012359822867438197\n",
      "Iteration 1563: loss 0.0012335715582594275\n",
      "Iteration 1564: loss 0.0012311858590692282\n",
      "Iteration 1565: loss 0.0012288109865039587\n",
      "Iteration 1566: loss 0.0012264485703781247\n",
      "Iteration 1567: loss 0.0012241130461916327\n",
      "Iteration 1568: loss 0.0012217944022268057\n",
      "Iteration 1569: loss 0.0012194886803627014\n",
      "Iteration 1570: loss 0.0012171920388936996\n",
      "Iteration 1571: loss 0.0012149212416261435\n",
      "Iteration 1572: loss 0.0012126729125156999\n",
      "Iteration 1573: loss 0.0012104311026632786\n",
      "Iteration 1574: loss 0.0012082040775567293\n",
      "Iteration 1575: loss 0.0012059953296557069\n",
      "Iteration 1576: loss 0.0012038025306537747\n",
      "Iteration 1577: loss 0.001201622886583209\n",
      "Iteration 1578: loss 0.0011994640808552504\n",
      "Iteration 1579: loss 0.001197316567413509\n",
      "Iteration 1580: loss 0.0011951896594837308\n",
      "Iteration 1581: loss 0.0011930725304409862\n",
      "Iteration 1582: loss 0.0011909687891602516\n",
      "Iteration 1583: loss 0.0011888863518834114\n",
      "Iteration 1584: loss 0.0011868140427395701\n",
      "Iteration 1585: loss 0.0011847538407891989\n",
      "Iteration 1586: loss 0.0011827130801975727\n",
      "Iteration 1587: loss 0.00118069292511791\n",
      "Iteration 1588: loss 0.001178679638542235\n",
      "Iteration 1589: loss 0.0011766769457608461\n",
      "Iteration 1590: loss 0.0011746957898139954\n",
      "Iteration 1591: loss 0.001172727788798511\n",
      "Iteration 1592: loss 0.0011707724770531058\n",
      "Iteration 1593: loss 0.0011688248487189412\n",
      "Iteration 1594: loss 0.00116690993309021\n",
      "Iteration 1595: loss 0.0011649946682155132\n",
      "Iteration 1596: loss 0.0011630940716713667\n",
      "Iteration 1597: loss 0.0011612134985625744\n",
      "Iteration 1598: loss 0.0011593367671594024\n",
      "Iteration 1599: loss 0.001157473772764206\n",
      "Iteration 1600: loss 0.0011556359240785241\n",
      "Iteration 1601: loss 0.0011538033140823245\n",
      "Iteration 1602: loss 0.0011519796680659056\n",
      "Iteration 1603: loss 0.0011501794215291739\n",
      "Iteration 1604: loss 0.0011483889538794756\n",
      "Iteration 1605: loss 0.001146603492088616\n",
      "Iteration 1606: loss 0.0011448387522250414\n",
      "Iteration 1607: loss 0.0011430816957727075\n",
      "Iteration 1608: loss 0.0011413441970944405\n",
      "Iteration 1609: loss 0.0011396158952265978\n",
      "Iteration 1610: loss 0.0011378938797861338\n",
      "Iteration 1611: loss 0.0011361909564584494\n",
      "Iteration 1612: loss 0.0011345038656145334\n",
      "Iteration 1613: loss 0.001132819103077054\n",
      "Iteration 1614: loss 0.0011311563430354\n",
      "Iteration 1615: loss 0.0011294992873445153\n",
      "Iteration 1616: loss 0.0011278517777100205\n",
      "Iteration 1617: loss 0.0011262232437729836\n",
      "Iteration 1618: loss 0.001124597038142383\n",
      "Iteration 1619: loss 0.0011229898082092404\n",
      "Iteration 1620: loss 0.0011213975958526134\n",
      "Iteration 1621: loss 0.0011198066640645266\n",
      "Iteration 1622: loss 0.0011182387825101614\n",
      "Iteration 1623: loss 0.0011166699696332216\n",
      "Iteration 1624: loss 0.0011151222279295325\n",
      "Iteration 1625: loss 0.0011135819368064404\n",
      "Iteration 1626: loss 0.0011120544513687491\n",
      "Iteration 1627: loss 0.0011105366284027696\n",
      "Iteration 1628: loss 0.0011090299813076854\n",
      "Iteration 1629: loss 0.0011075311340391636\n",
      "Iteration 1630: loss 0.0011060512624680996\n",
      "Iteration 1631: loss 0.0011045725550502539\n",
      "Iteration 1632: loss 0.0011031138710677624\n",
      "Iteration 1633: loss 0.001101657166145742\n",
      "Iteration 1634: loss 0.001100213616155088\n",
      "Iteration 1635: loss 0.0010987833375111222\n",
      "Iteration 1636: loss 0.0010973577154800296\n",
      "Iteration 1637: loss 0.0010959467617794871\n",
      "Iteration 1638: loss 0.0010945489630103111\n",
      "Iteration 1639: loss 0.0010931572178378701\n",
      "Iteration 1640: loss 0.0010917731560766697\n",
      "Iteration 1641: loss 0.0010904038790613413\n",
      "Iteration 1642: loss 0.0010890411213040352\n",
      "Iteration 1643: loss 0.001087687909603119\n",
      "Iteration 1644: loss 0.001086346572265029\n",
      "Iteration 1645: loss 0.0010850178077816963\n",
      "Iteration 1646: loss 0.0010836981236934662\n",
      "Iteration 1647: loss 0.0010823770426213741\n",
      "Iteration 1648: loss 0.0010810793610289693\n",
      "Iteration 1649: loss 0.0010797821450978518\n",
      "Iteration 1650: loss 0.0010785020422190428\n",
      "Iteration 1651: loss 0.0010772284585982561\n",
      "Iteration 1652: loss 0.00107596127782017\n",
      "Iteration 1653: loss 0.0010747011983767152\n",
      "Iteration 1654: loss 0.0010734571842476726\n",
      "Iteration 1655: loss 0.0010722146835178137\n",
      "Iteration 1656: loss 0.0010709883645176888\n",
      "Iteration 1657: loss 0.0010697700781747699\n",
      "Iteration 1658: loss 0.0010685601737350225\n",
      "Iteration 1659: loss 0.0010673541110008955\n",
      "Iteration 1660: loss 0.0010661596897989511\n",
      "Iteration 1661: loss 0.0010649727191776037\n",
      "Iteration 1662: loss 0.0010637944797053933\n",
      "Iteration 1663: loss 0.0010626318398863077\n",
      "Iteration 1664: loss 0.0010614737402647734\n",
      "Iteration 1665: loss 0.0010603161063045263\n",
      "Iteration 1666: loss 0.001059176167473197\n",
      "Iteration 1667: loss 0.0010580422822386026\n",
      "Iteration 1668: loss 0.001056912587955594\n",
      "Iteration 1669: loss 0.001055796048603952\n",
      "Iteration 1670: loss 0.001054684049449861\n",
      "Iteration 1671: loss 0.0010535817127674818\n",
      "Iteration 1672: loss 0.0010524929966777563\n",
      "Iteration 1673: loss 0.001051402185112238\n",
      "Iteration 1674: loss 0.0010503304656594992\n",
      "Iteration 1675: loss 0.0010492552537471056\n",
      "Iteration 1676: loss 0.0010481965728104115\n",
      "Iteration 1677: loss 0.0010471373097971082\n",
      "Iteration 1678: loss 0.0010460924822837114\n",
      "Iteration 1679: loss 0.0010450506815686822\n",
      "Iteration 1680: loss 0.0010440181940793991\n",
      "Iteration 1681: loss 0.0010429982794448733\n",
      "Iteration 1682: loss 0.0010419853497296572\n",
      "Iteration 1683: loss 0.0010409692768007517\n",
      "Iteration 1684: loss 0.0010399684542790055\n",
      "Iteration 1685: loss 0.0010389733361080289\n",
      "Iteration 1686: loss 0.0010379867162555456\n",
      "Iteration 1687: loss 0.0010370066156610847\n",
      "Iteration 1688: loss 0.0010360307060182095\n",
      "Iteration 1689: loss 0.0010350713273510337\n",
      "Iteration 1690: loss 0.0010341082233935595\n",
      "Iteration 1691: loss 0.0010331578087061644\n",
      "Iteration 1692: loss 0.0010322073940187693\n",
      "Iteration 1693: loss 0.001031276537105441\n",
      "Iteration 1694: loss 0.0010303431190550327\n",
      "Iteration 1695: loss 0.001029421342536807\n",
      "Iteration 1696: loss 0.001028501894325018\n",
      "Iteration 1697: loss 0.0010275878012180328\n",
      "Iteration 1698: loss 0.001026685698889196\n",
      "Iteration 1699: loss 0.0010257905814796686\n",
      "Iteration 1700: loss 0.001024895696900785\n",
      "Iteration 1701: loss 0.0010240142000839114\n",
      "Iteration 1702: loss 0.0010231351479887962\n",
      "Iteration 1703: loss 0.001022264827042818\n",
      "Iteration 1704: loss 0.0010213988134637475\n",
      "Iteration 1705: loss 0.0010205345461145043\n",
      "Iteration 1706: loss 0.0010196855291724205\n",
      "Iteration 1707: loss 0.0010188384912908077\n",
      "Iteration 1708: loss 0.001018001465126872\n",
      "Iteration 1709: loss 0.0010171672329306602\n",
      "Iteration 1710: loss 0.001016339403577149\n",
      "Iteration 1711: loss 0.0010155176278203726\n",
      "Iteration 1712: loss 0.001014693989418447\n",
      "Iteration 1713: loss 0.0010138879297301173\n",
      "Iteration 1714: loss 0.001013085595332086\n",
      "Iteration 1715: loss 0.0010122848907485604\n",
      "Iteration 1716: loss 0.0010114958276972175\n",
      "Iteration 1717: loss 0.0010107074631378055\n",
      "Iteration 1718: loss 0.001009926199913025\n",
      "Iteration 1719: loss 0.0010091554140672088\n",
      "Iteration 1720: loss 0.0010083802044391632\n",
      "Iteration 1721: loss 0.0010076214093714952\n",
      "Iteration 1722: loss 0.0010068555129691958\n",
      "Iteration 1723: loss 0.0010061098728328943\n",
      "Iteration 1724: loss 0.0010053631849586964\n",
      "Iteration 1725: loss 0.0010046185925602913\n",
      "Iteration 1726: loss 0.0010038812179118395\n",
      "Iteration 1727: loss 0.001003154437057674\n",
      "Iteration 1728: loss 0.0010024296352639794\n",
      "Iteration 1729: loss 0.0010017110034823418\n",
      "Iteration 1730: loss 0.0010009966790676117\n",
      "Iteration 1731: loss 0.0010002822382375598\n",
      "Iteration 1732: loss 0.0009995757136493921\n",
      "Iteration 1733: loss 0.0009988786187022924\n",
      "Iteration 1734: loss 0.0009981832699850202\n",
      "Iteration 1735: loss 0.000997494556941092\n",
      "Iteration 1736: loss 0.0009968122467398643\n",
      "Iteration 1737: loss 0.0009961359901353717\n",
      "Iteration 1738: loss 0.0009954635752364993\n",
      "Iteration 1739: loss 0.000994790461845696\n",
      "Iteration 1740: loss 0.0009941230528056622\n",
      "Iteration 1741: loss 0.000993472756817937\n",
      "Iteration 1742: loss 0.0009928145445883274\n",
      "Iteration 1743: loss 0.0009921651799231768\n",
      "Iteration 1744: loss 0.000991524662822485\n",
      "Iteration 1745: loss 0.0009908813517540693\n",
      "Iteration 1746: loss 0.000990243279375136\n",
      "Iteration 1747: loss 0.0009896160336211324\n",
      "Iteration 1748: loss 0.0009889898356050253\n",
      "Iteration 1749: loss 0.0009883670136332512\n",
      "Iteration 1750: loss 0.0009877535048872232\n",
      "Iteration 1751: loss 0.0009871397633105516\n",
      "Iteration 1752: loss 0.000986532773822546\n",
      "Iteration 1753: loss 0.000985928694717586\n",
      "Iteration 1754: loss 0.0009853339288383722\n",
      "Iteration 1755: loss 0.0009847329929471016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1756: loss 0.0009841462597250938\n",
      "Iteration 1757: loss 0.00098356232047081\n",
      "Iteration 1758: loss 0.0009829762857407331\n",
      "Iteration 1759: loss 0.0009824029402807355\n",
      "Iteration 1760: loss 0.0009818320395424962\n",
      "Iteration 1761: loss 0.0009812616044655442\n",
      "Iteration 1762: loss 0.000980697339400649\n",
      "Iteration 1763: loss 0.0009801359847187996\n",
      "Iteration 1764: loss 0.0009795796358957887\n",
      "Iteration 1765: loss 0.000979026546701789\n",
      "Iteration 1766: loss 0.0009784784633666277\n",
      "Iteration 1767: loss 0.0009779357351362705\n",
      "Iteration 1768: loss 0.0009773984784260392\n",
      "Iteration 1769: loss 0.0009768573800101876\n",
      "Iteration 1770: loss 0.0009763253037817776\n",
      "Iteration 1771: loss 0.0009758012020029128\n",
      "Iteration 1772: loss 0.0009752711048349738\n",
      "Iteration 1773: loss 0.0009747588774189353\n",
      "Iteration 1774: loss 0.0009742436814121902\n",
      "Iteration 1775: loss 0.0009737276704981923\n",
      "Iteration 1776: loss 0.0009732182952575386\n",
      "Iteration 1777: loss 0.0009727127617225051\n",
      "Iteration 1778: loss 0.0009722139220684767\n",
      "Iteration 1779: loss 0.0009717122302390635\n",
      "Iteration 1780: loss 0.0009712222381494939\n",
      "Iteration 1781: loss 0.0009707318968139589\n",
      "Iteration 1782: loss 0.0009702442330308259\n",
      "Iteration 1783: loss 0.0009697595378383994\n",
      "Iteration 1784: loss 0.0009692811290733516\n",
      "Iteration 1785: loss 0.0009688090649433434\n",
      "Iteration 1786: loss 0.0009683343232609332\n",
      "Iteration 1787: loss 0.0009678610367700458\n",
      "Iteration 1788: loss 0.0009674021857790649\n",
      "Iteration 1789: loss 0.0009669371647760272\n",
      "Iteration 1790: loss 0.0009664770914241672\n",
      "Iteration 1791: loss 0.0009660236537456512\n",
      "Iteration 1792: loss 0.0009655700996518135\n",
      "Iteration 1793: loss 0.0009651247528381646\n",
      "Iteration 1794: loss 0.0009646756807342172\n",
      "Iteration 1795: loss 0.0009642355144023895\n",
      "Iteration 1796: loss 0.0009637954644858837\n",
      "Iteration 1797: loss 0.0009633601875975728\n",
      "Iteration 1798: loss 0.0009629247360862792\n",
      "Iteration 1799: loss 0.0009624997619539499\n",
      "Iteration 1800: loss 0.0009620700730010867\n",
      "Iteration 1801: loss 0.0009616519091650844\n",
      "Iteration 1802: loss 0.0009612302528694272\n",
      "Iteration 1803: loss 0.0009608132531866431\n",
      "Iteration 1804: loss 0.0009604039369150996\n",
      "Iteration 1805: loss 0.0009599904296919703\n",
      "Iteration 1806: loss 0.0009595813462510705\n",
      "Iteration 1807: loss 0.0009591776179149747\n",
      "Iteration 1808: loss 0.0009587735403329134\n",
      "Iteration 1809: loss 0.0009583784267306328\n",
      "Iteration 1810: loss 0.0009579814504832029\n",
      "Iteration 1811: loss 0.0009575920412316918\n",
      "Iteration 1812: loss 0.0009571995469741523\n",
      "Iteration 1813: loss 0.0009568142122589052\n",
      "Iteration 1814: loss 0.0009564290521666408\n",
      "Iteration 1815: loss 0.0009560489561408758\n",
      "Iteration 1816: loss 0.0009556663571856916\n",
      "Iteration 1817: loss 0.0009552970295771956\n",
      "Iteration 1818: loss 0.000954924151301384\n",
      "Iteration 1819: loss 0.0009545569773763418\n",
      "Iteration 1820: loss 0.0009541858453303576\n",
      "Iteration 1821: loss 0.0009538226295262575\n",
      "Iteration 1822: loss 0.0009534599957987666\n",
      "Iteration 1823: loss 0.0009531016694381833\n",
      "Iteration 1824: loss 0.0009527454967610538\n",
      "Iteration 1825: loss 0.0009523933986201882\n",
      "Iteration 1826: loss 0.000952039030380547\n",
      "Iteration 1827: loss 0.0009516911813989282\n",
      "Iteration 1828: loss 0.0009513470577076077\n",
      "Iteration 1829: loss 0.0009510027593933046\n",
      "Iteration 1830: loss 0.0009506619535386562\n",
      "Iteration 1831: loss 0.0009503229521214962\n",
      "Iteration 1832: loss 0.0009499877924099565\n",
      "Iteration 1833: loss 0.000949655135627836\n",
      "Iteration 1834: loss 0.0009493253892287612\n",
      "Iteration 1835: loss 0.0009489997173659503\n",
      "Iteration 1836: loss 0.000948672357480973\n",
      "Iteration 1837: loss 0.000948349479585886\n",
      "Iteration 1838: loss 0.0009480278240516782\n",
      "Iteration 1839: loss 0.000947705702856183\n",
      "Iteration 1840: loss 0.0009473945247009397\n",
      "Iteration 1841: loss 0.0009470799705013633\n",
      "Iteration 1842: loss 0.0009467666386626661\n",
      "Iteration 1843: loss 0.0009464621543884277\n",
      "Iteration 1844: loss 0.0009461504523642361\n",
      "Iteration 1845: loss 0.0009458487038500607\n",
      "Iteration 1846: loss 0.0009455450926907361\n",
      "Iteration 1847: loss 0.0009452460799366236\n",
      "Iteration 1848: loss 0.0009449489880353212\n",
      "Iteration 1849: loss 0.0009446492185816169\n",
      "Iteration 1850: loss 0.0009443568997085094\n",
      "Iteration 1851: loss 0.0009440641151741147\n",
      "Iteration 1852: loss 0.0009437755215913057\n",
      "Iteration 1853: loss 0.0009434891981072724\n",
      "Iteration 1854: loss 0.0009432009537704289\n",
      "Iteration 1855: loss 0.0009429214405827224\n",
      "Iteration 1856: loss 0.000942645943723619\n",
      "Iteration 1857: loss 0.0009423646843060851\n",
      "Iteration 1858: loss 0.0009420864516869187\n",
      "Iteration 1859: loss 0.0009418136905878782\n",
      "Iteration 1860: loss 0.0009415384847670794\n",
      "Iteration 1861: loss 0.0009412694489583373\n",
      "Iteration 1862: loss 0.0009409982012584805\n",
      "Iteration 1863: loss 0.0009407351026311517\n",
      "Iteration 1864: loss 0.0009404719457961619\n",
      "Iteration 1865: loss 0.0009402043651789427\n",
      "Iteration 1866: loss 0.0009399471455253661\n",
      "Iteration 1867: loss 0.0009396877139806747\n",
      "Iteration 1868: loss 0.0009394313674420118\n",
      "Iteration 1869: loss 0.0009391784551553428\n",
      "Iteration 1870: loss 0.00093892402946949\n",
      "Iteration 1871: loss 0.000938675832003355\n",
      "Iteration 1872: loss 0.0009384258883073926\n",
      "Iteration 1873: loss 0.0009381769341416657\n",
      "Iteration 1874: loss 0.0009379297262057662\n",
      "Iteration 1875: loss 0.0009376880479976535\n",
      "Iteration 1876: loss 0.0009374452056363225\n",
      "Iteration 1877: loss 0.0009372036438435316\n",
      "Iteration 1878: loss 0.0009369681356474757\n",
      "Iteration 1879: loss 0.0009367303573526442\n",
      "Iteration 1880: loss 0.0009364965953864157\n",
      "Iteration 1881: loss 0.0009362660348415375\n",
      "Iteration 1882: loss 0.00093603297136724\n",
      "Iteration 1883: loss 0.0009358070092275739\n",
      "Iteration 1884: loss 0.0009355787769891322\n",
      "Iteration 1885: loss 0.0009353472851216793\n",
      "Iteration 1886: loss 0.00093512749299407\n",
      "Iteration 1887: loss 0.0009349004249088466\n",
      "Iteration 1888: loss 0.0009346824372187257\n",
      "Iteration 1889: loss 0.0009344634599983692\n",
      "Iteration 1890: loss 0.0009342433186247945\n",
      "Iteration 1891: loss 0.0009340279502794147\n",
      "Iteration 1892: loss 0.0009338121162727475\n",
      "Iteration 1893: loss 0.0009335980284959078\n",
      "Iteration 1894: loss 0.0009333917987532914\n",
      "Iteration 1895: loss 0.0009331819601356983\n",
      "Iteration 1896: loss 0.0009329717722721398\n",
      "Iteration 1897: loss 0.0009327667066827416\n",
      "Iteration 1898: loss 0.0009325616993010044\n",
      "Iteration 1899: loss 0.0009323606500402093\n",
      "Iteration 1900: loss 0.000932157039642334\n",
      "Iteration 1901: loss 0.0009319568634964526\n",
      "Iteration 1902: loss 0.0009317553485743701\n",
      "Iteration 1903: loss 0.0009315586648881435\n",
      "Iteration 1904: loss 0.0009313584887422621\n",
      "Iteration 1905: loss 0.0009311679168604314\n",
      "Iteration 1906: loss 0.0009309742017649114\n",
      "Iteration 1907: loss 0.0009307823493145406\n",
      "Iteration 1908: loss 0.000930594396777451\n",
      "Iteration 1909: loss 0.0009304053382948041\n",
      "Iteration 1910: loss 0.0009302146499976516\n",
      "Iteration 1911: loss 0.0009300318779423833\n",
      "Iteration 1912: loss 0.0009298447403125465\n",
      "Iteration 1913: loss 0.0009296622592955828\n",
      "Iteration 1914: loss 0.0009294824558310211\n",
      "Iteration 1915: loss 0.0009293039329349995\n",
      "Iteration 1916: loss 0.000929120637010783\n",
      "Iteration 1917: loss 0.0009289427543990314\n",
      "Iteration 1918: loss 0.0009287649299949408\n",
      "Iteration 1919: loss 0.0009285882697440684\n",
      "Iteration 1920: loss 0.0009284176048822701\n",
      "Iteration 1921: loss 0.0009282450191676617\n",
      "Iteration 1922: loss 0.0009280748199671507\n",
      "Iteration 1923: loss 0.0009279025252908468\n",
      "Iteration 1924: loss 0.0009277352364733815\n",
      "Iteration 1925: loss 0.0009275704505853355\n",
      "Iteration 1926: loss 0.0009274051408283412\n",
      "Iteration 1927: loss 0.000927235814742744\n",
      "Iteration 1928: loss 0.0009270759765058756\n",
      "Iteration 1929: loss 0.0009269086294807494\n",
      "Iteration 1930: loss 0.0009267529239878058\n",
      "Iteration 1931: loss 0.0009265873814001679\n",
      "Iteration 1932: loss 0.0009264287073165178\n",
      "Iteration 1933: loss 0.000926270498894155\n",
      "Iteration 1934: loss 0.0009261155501008034\n",
      "Iteration 1935: loss 0.0009259559446945786\n",
      "Iteration 1936: loss 0.0009258047793991864\n",
      "Iteration 1937: loss 0.0009256561170332134\n",
      "Iteration 1938: loss 0.0009255007025785744\n",
      "Iteration 1939: loss 0.0009253495372831821\n",
      "Iteration 1940: loss 0.0009251971496269107\n",
      "Iteration 1941: loss 0.0009250518633052707\n",
      "Iteration 1942: loss 0.0009249022696167231\n",
      "Iteration 1943: loss 0.0009247580310329795\n",
      "Iteration 1944: loss 0.0009246143163181841\n",
      "Iteration 1945: loss 0.0009244665270671248\n",
      "Iteration 1946: loss 0.0009243251988664269\n",
      "Iteration 1947: loss 0.0009241819498129189\n",
      "Iteration 1948: loss 0.0009240444633178413\n",
      "Iteration 1949: loss 0.0009239045903086662\n",
      "Iteration 1950: loss 0.0009237611666321754\n",
      "Iteration 1951: loss 0.0009236237965524197\n",
      "Iteration 1952: loss 0.0009234873577952385\n",
      "Iteration 1953: loss 0.0009233512682840228\n",
      "Iteration 1954: loss 0.0009232184384018183\n",
      "Iteration 1955: loss 0.0009230864816345274\n",
      "Iteration 1956: loss 0.0009229533607140183\n",
      "Iteration 1957: loss 0.000922823790460825\n",
      "Iteration 1958: loss 0.0009226923575624824\n",
      "Iteration 1959: loss 0.0009225577814504504\n",
      "Iteration 1960: loss 0.0009224288514815271\n",
      "Iteration 1961: loss 0.000922302482649684\n",
      "Iteration 1962: loss 0.0009221795480698347\n",
      "Iteration 1963: loss 0.0009220510255545378\n",
      "Iteration 1964: loss 0.0009219211642630398\n",
      "Iteration 1965: loss 0.0009218007908202708\n",
      "Iteration 1966: loss 0.0009216757607646286\n",
      "Iteration 1967: loss 0.0009215584141202271\n",
      "Iteration 1968: loss 0.000921435363125056\n",
      "Iteration 1969: loss 0.0009213113225996494\n",
      "Iteration 1970: loss 0.000921197235584259\n",
      "Iteration 1971: loss 0.0009210766293108463\n",
      "Iteration 1972: loss 0.0009209592244587839\n",
      "Iteration 1973: loss 0.000920845486689359\n",
      "Iteration 1974: loss 0.0009207255789078772\n",
      "Iteration 1975: loss 0.0009206107351928949\n",
      "Iteration 1976: loss 0.0009204947855323553\n",
      "Iteration 1977: loss 0.000920384656637907\n",
      "Iteration 1978: loss 0.0009202732471749187\n",
      "Iteration 1979: loss 0.0009201611974276602\n",
      "Iteration 1980: loss 0.00092004967154935\n",
      "Iteration 1981: loss 0.0009199428022839129\n",
      "Iteration 1982: loss 0.0009198249317705631\n",
      "Iteration 1983: loss 0.0009197237086482346\n",
      "Iteration 1984: loss 0.0009196134051308036\n",
      "Iteration 1985: loss 0.0009195040911436081\n",
      "Iteration 1986: loss 0.0009193993173539639\n",
      "Iteration 1987: loss 0.0009192965226247907\n",
      "Iteration 1988: loss 0.0009191912831738591\n",
      "Iteration 1989: loss 0.000919086451176554\n",
      "Iteration 1990: loss 0.0009189838310703635\n",
      "Iteration 1991: loss 0.0009188848198391497\n",
      "Iteration 1992: loss 0.0009187798714265227\n",
      "Iteration 1993: loss 0.0009186830138787627\n",
      "Iteration 1994: loss 0.0009185796370729804\n",
      "Iteration 1995: loss 0.0009184782975353301\n",
      "Iteration 1996: loss 0.0009183851070702076\n",
      "Iteration 1997: loss 0.0009182847570627928\n",
      "Iteration 1998: loss 0.0009181909263134003\n",
      "Iteration 1999: loss 0.0009180964552797377\n",
      "Iteration 2000: loss 0.0009179983753710985\n",
      "Iteration 2001: loss 0.0009179009357467294\n",
      "Iteration 2002: loss 0.0009178131585940719\n",
      "Iteration 2003: loss 0.0009177133906632662\n",
      "Iteration 2004: loss 0.000917622703127563\n",
      "Iteration 2005: loss 0.0009175315499305725\n",
      "Iteration 2006: loss 0.0009174427250400186\n",
      "Iteration 2007: loss 0.0009173530852422118\n",
      "Iteration 2008: loss 0.0009172623394988477\n",
      "Iteration 2009: loss 0.0009171697311103344\n",
      "Iteration 2010: loss 0.0009170855628326535\n",
      "Iteration 2011: loss 0.0009169938275590539\n",
      "Iteration 2012: loss 0.0009169126860797405\n",
      "Iteration 2013: loss 0.0009168211836367846\n",
      "Iteration 2014: loss 0.0009167373646050692\n",
      "Iteration 2015: loss 0.0009166548261418939\n",
      "Iteration 2016: loss 0.0009165688534267247\n",
      "Iteration 2017: loss 0.0009164841612800956\n",
      "Iteration 2018: loss 0.0009163990616798401\n",
      "Iteration 2019: loss 0.0009163161739706993\n",
      "Iteration 2020: loss 0.0009162371279671788\n",
      "Iteration 2021: loss 0.0009161516209132969\n",
      "Iteration 2022: loss 0.0009160777553915977\n",
      "Iteration 2023: loss 0.0009159928886219859\n",
      "Iteration 2024: loss 0.0009159116307273507\n",
      "Iteration 2025: loss 0.0009158338070847094\n",
      "Iteration 2026: loss 0.0009157567983493209\n",
      "Iteration 2027: loss 0.000915677985176444\n",
      "Iteration 2028: loss 0.0009155991720035672\n",
      "Iteration 2029: loss 0.0009155237348750234\n",
      "Iteration 2030: loss 0.000915445969440043\n",
      "Iteration 2031: loss 0.0009153722785413265\n",
      "Iteration 2032: loss 0.0009152975981123745\n",
      "Iteration 2033: loss 0.0009152263519354165\n",
      "Iteration 2034: loss 0.0009151515550911427\n",
      "Iteration 2035: loss 0.0009150755358859897\n",
      "Iteration 2036: loss 0.0009150052210316062\n",
      "Iteration 2037: loss 0.0009149341494776309\n",
      "Iteration 2038: loss 0.0009148622630164027\n",
      "Iteration 2039: loss 0.0009147909004241228\n",
      "Iteration 2040: loss 0.0009147217497229576\n",
      "Iteration 2041: loss 0.000914650154300034\n",
      "Iteration 2042: loss 0.000914578209631145\n",
      "Iteration 2043: loss 0.0009145099902525544\n",
      "Iteration 2044: loss 0.0009144446812570095\n",
      "Iteration 2045: loss 0.0009143744828179479\n",
      "Iteration 2046: loss 0.0009143041679635644\n",
      "Iteration 2047: loss 0.0009142404305748641\n",
      "Iteration 2048: loss 0.0009141741902567446\n",
      "Iteration 2049: loss 0.0009141035843640566\n",
      "Iteration 2050: loss 0.0009140415349975228\n",
      "Iteration 2051: loss 0.0009139716858044267\n",
      "Iteration 2052: loss 0.0009139108005911112\n",
      "Iteration 2053: loss 0.0009138506138697267\n",
      "Iteration 2054: loss 0.0009137822198681533\n",
      "Iteration 2055: loss 0.0009137207525782287\n",
      "Iteration 2056: loss 0.0009136572480201721\n",
      "Iteration 2057: loss 0.0009135976433753967\n",
      "Iteration 2058: loss 0.0009135388536378741\n",
      "Iteration 2059: loss 0.0009134756401181221\n",
      "Iteration 2060: loss 0.0009134178981184959\n",
      "Iteration 2061: loss 0.0009133529383689165\n",
      "Iteration 2062: loss 0.0009132935665547848\n",
      "Iteration 2063: loss 0.0009132356499321759\n",
      "Iteration 2064: loss 0.0009131733677349985\n",
      "Iteration 2065: loss 0.0009131184779107571\n",
      "Iteration 2066: loss 0.0009130602702498436\n",
      "Iteration 2067: loss 0.0009130022954195738\n",
      "Iteration 2068: loss 0.0009129435638897121\n",
      "Iteration 2069: loss 0.0009128875099122524\n",
      "Iteration 2070: loss 0.0009128354140557349\n",
      "Iteration 2071: loss 0.0009127757512032986\n",
      "Iteration 2072: loss 0.0009127181838266551\n",
      "Iteration 2073: loss 0.0009126641089096665\n",
      "Iteration 2074: loss 0.0009126096265390515\n",
      "Iteration 2075: loss 0.0009125554352067411\n",
      "Iteration 2076: loss 0.000912503048311919\n",
      "Iteration 2077: loss 0.0009124498465098441\n",
      "Iteration 2078: loss 0.0009123958880081773\n",
      "Iteration 2079: loss 0.0009123433264903724\n",
      "Iteration 2080: loss 0.0009122942574322224\n",
      "Iteration 2081: loss 0.0009122403571382165\n",
      "Iteration 2082: loss 0.0009121905895881355\n",
      "Iteration 2083: loss 0.0009121379116550088\n",
      "Iteration 2084: loss 0.0009120866889134049\n",
      "Iteration 2085: loss 0.0009120380273088813\n",
      "Iteration 2086: loss 0.000911987735889852\n",
      "Iteration 2087: loss 0.0009119372116401792\n",
      "Iteration 2088: loss 0.0009118869202211499\n",
      "Iteration 2089: loss 0.0009118381422013044\n",
      "Iteration 2090: loss 0.0009117929148487747\n",
      "Iteration 2091: loss 0.0009117460576817393\n",
      "Iteration 2092: loss 0.0009116936125792563\n",
      "Iteration 2093: loss 0.0009116476867347956\n",
      "Iteration 2094: loss 0.0009116064757108688\n",
      "Iteration 2095: loss 0.0009115567081607878\n",
      "Iteration 2096: loss 0.0009115105494856834\n",
      "Iteration 2097: loss 0.0009114663698710501\n",
      "Iteration 2098: loss 0.0009114192798733711\n",
      "Iteration 2099: loss 0.0009113750420510769\n",
      "Iteration 2100: loss 0.0009113277774304152\n",
      "Iteration 2101: loss 0.0009112836560234427\n",
      "Iteration 2102: loss 0.0009112393599934876\n",
      "Iteration 2103: loss 0.0009111918043345213\n",
      "Iteration 2104: loss 0.0009111547260545194\n",
      "Iteration 2105: loss 0.0009111078106798232\n",
      "Iteration 2106: loss 0.0009110679384320974\n",
      "Iteration 2107: loss 0.0009110199753195047\n",
      "Iteration 2108: loss 0.0009109822567552328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2109: loss 0.000910941744223237\n",
      "Iteration 2110: loss 0.0009108965750783682\n",
      "Iteration 2111: loss 0.0009108608355745673\n",
      "Iteration 2112: loss 0.0009108162485063076\n",
      "Iteration 2113: loss 0.0009107720106840134\n",
      "Iteration 2114: loss 0.0009107346413657069\n",
      "Iteration 2115: loss 0.0009106938377954066\n",
      "Iteration 2116: loss 0.0009106551297008991\n",
      "Iteration 2117: loss 0.000910612172447145\n",
      "Iteration 2118: loss 0.0009105770732276142\n",
      "Iteration 2119: loss 0.0009105376666411757\n",
      "Iteration 2120: loss 0.0009105015778914094\n",
      "Iteration 2121: loss 0.0009104607161134481\n",
      "Iteration 2122: loss 0.0009104248601943254\n",
      "Iteration 2123: loss 0.0009103837073780596\n",
      "Iteration 2124: loss 0.0009103500051423907\n",
      "Iteration 2125: loss 0.0009103101911023259\n",
      "Iteration 2126: loss 0.000910274509806186\n",
      "Iteration 2127: loss 0.0009102385956794024\n",
      "Iteration 2128: loss 0.0009102048934437335\n",
      "Iteration 2129: loss 0.0009101638570427895\n",
      "Iteration 2130: loss 0.0009101277682930231\n",
      "Iteration 2131: loss 0.0009100937750190496\n",
      "Iteration 2132: loss 0.0009100566385313869\n",
      "Iteration 2133: loss 0.0009100214811041951\n",
      "Iteration 2134: loss 0.0009099884191527963\n",
      "Iteration 2135: loss 0.0009099557064473629\n",
      "Iteration 2136: loss 0.0009099191520363092\n",
      "Iteration 2137: loss 0.0009098820155486465\n",
      "Iteration 2138: loss 0.0009098554728552699\n",
      "Iteration 2139: loss 0.0009098176378756762\n",
      "Iteration 2140: loss 0.0009097870206460357\n",
      "Iteration 2141: loss 0.0009097523870877922\n",
      "Iteration 2142: loss 0.0009097207803279161\n",
      "Iteration 2143: loss 0.0009096883004531264\n",
      "Iteration 2144: loss 0.0009096558205783367\n",
      "Iteration 2145: loss 0.0009096229914575815\n",
      "Iteration 2146: loss 0.0009095938876271248\n",
      "Iteration 2147: loss 0.0009095616405829787\n",
      "Iteration 2148: loss 0.0009095288114622235\n",
      "Iteration 2149: loss 0.0009095032000914216\n",
      "Iteration 2150: loss 0.0009094688575714827\n",
      "Iteration 2151: loss 0.0009094371343962848\n",
      "Iteration 2152: loss 0.0009094077395275235\n",
      "Iteration 2153: loss 0.0009093768894672394\n",
      "Iteration 2154: loss 0.0009093447588384151\n",
      "Iteration 2155: loss 0.0009093161206692457\n",
      "Iteration 2156: loss 0.0009092901018448174\n",
      "Iteration 2157: loss 0.000909259426407516\n",
      "Iteration 2158: loss 0.0009092316031455994\n",
      "Iteration 2159: loss 0.0009092054096981883\n",
      "Iteration 2160: loss 0.0009091704851016402\n",
      "Iteration 2161: loss 0.000909147784113884\n",
      "Iteration 2162: loss 0.0009091171668842435\n",
      "Iteration 2163: loss 0.0009090898092836142\n",
      "Iteration 2164: loss 0.0009090626263059676\n",
      "Iteration 2165: loss 0.0009090334642678499\n",
      "Iteration 2166: loss 0.0009090050589293242\n",
      "Iteration 2167: loss 0.0009089818922802806\n",
      "Iteration 2168: loss 0.0009089520899578929\n",
      "Iteration 2169: loss 0.0009089271770790219\n",
      "Iteration 2170: loss 0.0009088997612707317\n",
      "Iteration 2171: loss 0.0009088774677366018\n",
      "Iteration 2172: loss 0.0009088485967367887\n",
      "Iteration 2173: loss 0.0009088224614970386\n",
      "Iteration 2174: loss 0.0009087942307814956\n",
      "Iteration 2175: loss 0.0009087689104489982\n",
      "Iteration 2176: loss 0.0009087471989914775\n",
      "Iteration 2177: loss 0.000908721995074302\n",
      "Iteration 2178: loss 0.000908699119463563\n",
      "Iteration 2179: loss 0.0009086726931855083\n",
      "Iteration 2180: loss 0.0009086475474759936\n",
      "Iteration 2181: loss 0.0009086229256354272\n",
      "Iteration 2182: loss 0.0009086024365387857\n",
      "Iteration 2183: loss 0.0009085735655389726\n",
      "Iteration 2184: loss 0.0009085527854040265\n",
      "Iteration 2185: loss 0.0009085285710170865\n",
      "Iteration 2186: loss 0.0009085022611543536\n",
      "Iteration 2187: loss 0.0009084799676202238\n",
      "Iteration 2188: loss 0.0009084561606869102\n",
      "Iteration 2189: loss 0.0009084352059289813\n",
      "Iteration 2190: loss 0.0009084106422960758\n",
      "Iteration 2191: loss 0.0009083891054615378\n",
      "Iteration 2192: loss 0.0009083648910745978\n",
      "Iteration 2193: loss 0.000908349291421473\n",
      "Iteration 2194: loss 0.0009083269396796823\n",
      "Iteration 2195: loss 0.0009083025506697595\n",
      "Iteration 2196: loss 0.000908277987036854\n",
      "Iteration 2197: loss 0.000908258487470448\n",
      "Iteration 2198: loss 0.0009082377655431628\n",
      "Iteration 2199: loss 0.000908214133232832\n",
      "Iteration 2200: loss 0.0009081934113055468\n",
      "Iteration 2201: loss 0.0009081765310838819\n",
      "Iteration 2202: loss 0.000908153539057821\n",
      "Iteration 2203: loss 0.0009081314201466739\n",
      "Iteration 2204: loss 0.0009081133175641298\n",
      "Iteration 2205: loss 0.0009080936433747411\n",
      "Iteration 2206: loss 0.0009080687304958701\n",
      "Iteration 2207: loss 0.0009080496383830905\n",
      "Iteration 2208: loss 0.0009080311865545809\n",
      "Iteration 2209: loss 0.0009080150630325079\n",
      "Iteration 2210: loss 0.0009079938754439354\n",
      "Iteration 2211: loss 0.0009079767623916268\n",
      "Iteration 2212: loss 0.0009079544106498361\n",
      "Iteration 2213: loss 0.0009079328738152981\n",
      "Iteration 2214: loss 0.000907916110008955\n",
      "Iteration 2215: loss 0.0009079006849788129\n",
      "Iteration 2216: loss 0.0009078811854124069\n",
      "Iteration 2217: loss 0.0009078620350919664\n",
      "Iteration 2218: loss 0.0009078374714590609\n",
      "Iteration 2219: loss 0.0009078263537958264\n",
      "Iteration 2220: loss 0.0009078047005459666\n",
      "Iteration 2221: loss 0.0009077871800400317\n",
      "Iteration 2222: loss 0.0009077676222659647\n",
      "Iteration 2223: loss 0.0009077505674213171\n",
      "Iteration 2224: loss 0.0009077341528609395\n",
      "Iteration 2225: loss 0.0009077170398086309\n",
      "Iteration 2226: loss 0.0009076989954337478\n",
      "Iteration 2227: loss 0.0009076825226657093\n",
      "Iteration 2228: loss 0.0009076614514924586\n",
      "Iteration 2229: loss 0.0009076469577848911\n",
      "Iteration 2230: loss 0.0009076326387003064\n",
      "Iteration 2231: loss 0.0009076160495169461\n",
      "Iteration 2232: loss 0.0009076001588255167\n",
      "Iteration 2233: loss 0.0009075853158719838\n",
      "Iteration 2234: loss 0.0009075675625354052\n",
      "Iteration 2235: loss 0.000907549518160522\n",
      "Iteration 2236: loss 0.0009075363632291555\n",
      "Iteration 2237: loss 0.0009075186098925769\n",
      "Iteration 2238: loss 0.0009075019042938948\n",
      "Iteration 2239: loss 0.0009074881672859192\n",
      "Iteration 2240: loss 0.0009074707049876451\n",
      "Iteration 2241: loss 0.0009074554545804858\n",
      "Iteration 2242: loss 0.0009074390400201082\n",
      "Iteration 2243: loss 0.0009074247209355235\n",
      "Iteration 2244: loss 0.0009074097033590078\n",
      "Iteration 2245: loss 0.0009073952678591013\n",
      "Iteration 2246: loss 0.0009073785040527582\n",
      "Iteration 2247: loss 0.0009073673281818628\n",
      "Iteration 2248: loss 0.0009073499822989106\n",
      "Iteration 2249: loss 0.0009073324035853148\n",
      "Iteration 2250: loss 0.0009073199471458793\n",
      "Iteration 2251: loss 0.0009073098190128803\n",
      "Iteration 2252: loss 0.0009072924731299281\n",
      "Iteration 2253: loss 0.000907279085367918\n",
      "Iteration 2254: loss 0.0009072654065676033\n",
      "Iteration 2255: loss 0.000907251494936645\n",
      "Iteration 2256: loss 0.0009072382235899568\n",
      "Iteration 2257: loss 0.0009072227403521538\n",
      "Iteration 2258: loss 0.000907210516743362\n",
      "Iteration 2259: loss 0.0009072015527635813\n",
      "Iteration 2260: loss 0.0009071825770661235\n",
      "Iteration 2261: loss 0.0009071708773262799\n",
      "Iteration 2262: loss 0.0009071564418263733\n",
      "Iteration 2263: loss 0.0009071461972780526\n",
      "Iteration 2264: loss 0.0009071329841390252\n",
      "Iteration 2265: loss 0.0009071170352399349\n",
      "Iteration 2266: loss 0.0009071046952158213\n",
      "Iteration 2267: loss 0.0009070964297279716\n",
      "Iteration 2268: loss 0.0009070787345990539\n",
      "Iteration 2269: loss 0.0009070670348592103\n",
      "Iteration 2270: loss 0.000907057081349194\n",
      "Iteration 2271: loss 0.0009070467203855515\n",
      "Iteration 2272: loss 0.000907030189409852\n",
      "Iteration 2273: loss 0.0009070205851458013\n",
      "Iteration 2274: loss 0.0009070092346519232\n",
      "Iteration 2275: loss 0.0009069931693375111\n",
      "Iteration 2276: loss 0.000906985136680305\n",
      "Iteration 2277: loss 0.0009069745428860188\n",
      "Iteration 2278: loss 0.0009069627849385142\n",
      "Iteration 2279: loss 0.0009069504449144006\n",
      "Iteration 2280: loss 0.000906936707906425\n",
      "Iteration 2281: loss 0.0009069242514669895\n",
      "Iteration 2282: loss 0.0009069141233339906\n",
      "Iteration 2283: loss 0.0009069063235074282\n",
      "Iteration 2284: loss 0.0009068899089470506\n",
      "Iteration 2285: loss 0.0009068812942132354\n",
      "Iteration 2286: loss 0.0009068663930520415\n",
      "Iteration 2287: loss 0.0009068595245480537\n",
      "Iteration 2288: loss 0.0009068495710380375\n",
      "Iteration 2289: loss 0.0009068365907296538\n",
      "Iteration 2290: loss 0.0009068275103345513\n",
      "Iteration 2291: loss 0.0009068208746612072\n",
      "Iteration 2292: loss 0.0009068085346370935\n",
      "Iteration 2293: loss 0.0009067943901754916\n",
      "Iteration 2294: loss 0.000906784029211849\n",
      "Iteration 2295: loss 0.0009067783830687404\n",
      "Iteration 2296: loss 0.000906764471437782\n",
      "Iteration 2297: loss 0.0009067550417967141\n",
      "Iteration 2298: loss 0.0009067408973351121\n",
      "Iteration 2299: loss 0.0009067319333553314\n",
      "Iteration 2300: loss 0.0009067247156053782\n",
      "Iteration 2301: loss 0.0009067172650247812\n",
      "Iteration 2302: loss 0.0009067034116014838\n",
      "Iteration 2303: loss 0.0009066924685612321\n",
      "Iteration 2304: loss 0.0009066857164725661\n",
      "Iteration 2305: loss 0.0009066771017387509\n",
      "Iteration 2306: loss 0.0009066701168194413\n",
      "Iteration 2307: loss 0.0009066577185876667\n",
      "Iteration 2308: loss 0.0009066498605534434\n",
      "Iteration 2309: loss 0.0009066440397873521\n",
      "Iteration 2310: loss 0.0009066285565495491\n",
      "Iteration 2311: loss 0.0009066199418157339\n",
      "Iteration 2312: loss 0.0009066091733984649\n",
      "Iteration 2313: loss 0.000906605739146471\n",
      "Iteration 2314: loss 0.0009065956110134721\n",
      "Iteration 2315: loss 0.000906584202311933\n",
      "Iteration 2316: loss 0.0009065818740054965\n",
      "Iteration 2317: loss 0.0009065708145499229\n",
      "Iteration 2318: loss 0.0009065598715096712\n",
      "Iteration 2319: loss 0.0009065497433766723\n",
      "Iteration 2320: loss 0.0009065442136488855\n",
      "Iteration 2321: loss 0.0009065326303243637\n",
      "Iteration 2322: loss 0.000906523666344583\n",
      "Iteration 2323: loss 0.0009065191261470318\n",
      "Iteration 2324: loss 0.0009065093472599983\n",
      "Iteration 2325: loss 0.0009065013146027923\n",
      "Iteration 2326: loss 0.0009064909536391497\n",
      "Iteration 2327: loss 0.0009064846090041101\n",
      "Iteration 2328: loss 0.0009064801270142198\n",
      "Iteration 2329: loss 0.0009064727928489447\n",
      "Iteration 2330: loss 0.0009064634796231985\n",
      "Iteration 2331: loss 0.0009064533514901996\n",
      "Iteration 2332: loss 0.0009064454352483153\n",
      "Iteration 2333: loss 0.0009064407786354423\n",
      "Iteration 2334: loss 0.0009064285550266504\n",
      "Iteration 2335: loss 0.0009064292535185814\n",
      "Iteration 2336: loss 0.0009064154583029449\n",
      "Iteration 2337: loss 0.0009064131882041693\n",
      "Iteration 2338: loss 0.0009064024197869003\n",
      "Iteration 2339: loss 0.0009063956094905734\n",
      "Iteration 2340: loss 0.0009063886245712638\n",
      "Iteration 2341: loss 0.0009063757606782019\n",
      "Iteration 2342: loss 0.0009063751203939319\n",
      "Iteration 2343: loss 0.0009063641191460192\n",
      "Iteration 2344: loss 0.0009063563193194568\n",
      "Iteration 2345: loss 0.0009063535835593939\n",
      "Iteration 2346: loss 0.0009063418838195503\n",
      "Iteration 2347: loss 0.0009063385077752173\n",
      "Iteration 2348: loss 0.0009063309989869595\n",
      "Iteration 2349: loss 0.0009063227334991097\n",
      "Iteration 2350: loss 0.0009063223842531443\n",
      "Iteration 2351: loss 0.000906312488950789\n",
      "Iteration 2352: loss 0.0009063063771463931\n",
      "Iteration 2353: loss 0.000906299683265388\n",
      "Iteration 2354: loss 0.0009062878089025617\n",
      "Iteration 2355: loss 0.0009062846656888723\n",
      "Iteration 2356: loss 0.0009062770986929536\n",
      "Iteration 2357: loss 0.0009062725002877414\n",
      "Iteration 2358: loss 0.0009062652243301272\n",
      "Iteration 2359: loss 0.0009062550961971283\n",
      "Iteration 2360: loss 0.0009062532335519791\n",
      "Iteration 2361: loss 0.0009062476456165314\n",
      "Iteration 2362: loss 0.0009062427561730146\n",
      "Iteration 2363: loss 0.0009062340250238776\n",
      "Iteration 2364: loss 0.0009062291937880218\n",
      "Iteration 2365: loss 0.000906223664060235\n",
      "Iteration 2366: loss 0.0009062182507477701\n",
      "Iteration 2367: loss 0.0009062127210199833\n",
      "Iteration 2368: loss 0.0009061999153345823\n",
      "Iteration 2369: loss 0.0009061965392902493\n",
      "Iteration 2370: loss 0.0009061942109838128\n",
      "Iteration 2371: loss 0.0009061882738023996\n",
      "Iteration 2372: loss 0.0009061829186975956\n",
      "Iteration 2373: loss 0.0009061762248165905\n",
      "Iteration 2374: loss 0.0009061676682904363\n",
      "Iteration 2375: loss 0.0009061654563993216\n",
      "Iteration 2376: loss 0.0009061568416655064\n",
      "Iteration 2377: loss 0.000906154396943748\n",
      "Iteration 2378: loss 0.00090614496730268\n",
      "Iteration 2379: loss 0.0009061413584277034\n",
      "Iteration 2380: loss 0.0009061401942744851\n",
      "Iteration 2381: loss 0.0009061305318027735\n",
      "Iteration 2382: loss 0.0009061269229277968\n",
      "Iteration 2383: loss 0.0009061241871677339\n",
      "Iteration 2384: loss 0.0009061125456355512\n",
      "Iteration 2385: loss 0.0009061141754500568\n",
      "Iteration 2386: loss 0.0009061065502464771\n",
      "Iteration 2387: loss 0.0009060985175892711\n",
      "Iteration 2388: loss 0.0009060916490852833\n",
      "Iteration 2389: loss 0.0009060907177627087\n",
      "Iteration 2390: loss 0.000906085770111531\n",
      "Iteration 2391: loss 0.0009060794254764915\n",
      "Iteration 2392: loss 0.0009060755837708712\n",
      "Iteration 2393: loss 0.0009060711599886417\n",
      "Iteration 2394: loss 0.0009060691809281707\n",
      "Iteration 2395: loss 0.0009060596348717809\n",
      "Iteration 2396: loss 0.0009060592274181545\n",
      "Iteration 2397: loss 0.0009060513111762702\n",
      "Iteration 2398: loss 0.0009060471784323454\n",
      "Iteration 2399: loss 0.0009060411830432713\n",
      "Iteration 2400: loss 0.0009060356533154845\n",
      "Iteration 2401: loss 0.0009060332085937262\n",
      "Iteration 2402: loss 0.0009060319280251861\n",
      "Iteration 2403: loss 0.0009060217998921871\n",
      "Iteration 2404: loss 0.0009060220327228308\n",
      "Iteration 2405: loss 0.0009060163283720613\n",
      "Iteration 2406: loss 0.0009060108568519354\n",
      "Iteration 2407: loss 0.000906004395801574\n",
      "Iteration 2408: loss 0.0009060022421181202\n",
      "Iteration 2409: loss 0.0009059965377673507\n",
      "Iteration 2410: loss 0.000905991590116173\n",
      "Iteration 2411: loss 0.0009059866424649954\n",
      "Iteration 2412: loss 0.0009059828007593751\n",
      "Iteration 2413: loss 0.0009059826261363924\n",
      "Iteration 2414: loss 0.0009059783769771457\n",
      "Iteration 2415: loss 0.0009059755830094218\n",
      "Iteration 2416: loss 0.0009059685980901122\n",
      "Iteration 2417: loss 0.0009059646399691701\n",
      "Iteration 2418: loss 0.0009059600415639579\n",
      "Iteration 2419: loss 0.0009059561416506767\n",
      "Iteration 2420: loss 0.0009059560252353549\n",
      "Iteration 2421: loss 0.0009059463045559824\n",
      "Iteration 2422: loss 0.0009059474105015397\n",
      "Iteration 2423: loss 0.0009059394942596555\n",
      "Iteration 2424: loss 0.0009059349540621042\n",
      "Iteration 2425: loss 0.0009059341391548514\n",
      "Iteration 2426: loss 0.0009059266885742545\n",
      "Iteration 2427: loss 0.000905922963283956\n",
      "Iteration 2428: loss 0.0009059240692295134\n",
      "Iteration 2429: loss 0.0009059201693162322\n",
      "Iteration 2430: loss 0.0009059167932718992\n",
      "Iteration 2431: loss 0.0009059089934453368\n",
      "Iteration 2432: loss 0.0009059083531610668\n",
      "Iteration 2433: loss 0.0009059049189090729\n",
      "Iteration 2434: loss 0.0009058957220986485\n",
      "Iteration 2435: loss 0.0009058932191692293\n",
      "Iteration 2436: loss 0.0009058908326551318\n",
      "Iteration 2437: loss 0.0009058904252015054\n",
      "Iteration 2438: loss 0.000905887340195477\n",
      "Iteration 2439: loss 0.0009058816358447075\n",
      "Iteration 2440: loss 0.0009058756986632943\n",
      "Iteration 2441: loss 0.0009058756404556334\n",
      "Iteration 2442: loss 0.0009058719733729959\n",
      "Iteration 2443: loss 0.0009058688301593065\n",
      "Iteration 2444: loss 0.0009058623109012842\n",
      "Iteration 2445: loss 0.0009058603318408132\n",
      "Iteration 2446: loss 0.0009058603318408132\n",
      "Iteration 2447: loss 0.0009058539872057736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2448: loss 0.0009058521245606244\n",
      "Iteration 2449: loss 0.000905845663510263\n",
      "Iteration 2450: loss 0.0009058505529537797\n",
      "Iteration 2451: loss 0.0009058446157723665\n",
      "Iteration 2452: loss 0.0009058390278369188\n",
      "Iteration 2453: loss 0.0009058376890607178\n",
      "Iteration 2454: loss 0.0009058311115950346\n",
      "Iteration 2455: loss 0.000905833556316793\n",
      "Iteration 2456: loss 0.0009058289579115808\n",
      "Iteration 2457: loss 0.0009058251744136214\n",
      "Iteration 2458: loss 0.0009058251744136214\n",
      "Iteration 2459: loss 0.0009058202849701047\n",
      "Iteration 2460: loss 0.0009058157447725534\n",
      "Iteration 2461: loss 0.0009058141149580479\n",
      "Iteration 2462: loss 0.0009058092255145311\n",
      "Iteration 2463: loss 0.000905808643437922\n",
      "Iteration 2464: loss 0.0009058050927706063\n",
      "Iteration 2465: loss 0.000905799213796854\n",
      "Iteration 2466: loss 0.0009058004943653941\n",
      "Iteration 2467: loss 0.0009057959541678429\n",
      "Iteration 2468: loss 0.0009057950228452682\n",
      "Iteration 2469: loss 0.0009057859424501657\n",
      "Iteration 2470: loss 0.0009057879215106368\n",
      "Iteration 2471: loss 0.0009057866991497576\n",
      "Iteration 2472: loss 0.0009057808201760054\n",
      "Iteration 2473: loss 0.0009057786664925516\n",
      "Iteration 2474: loss 0.0009057756396941841\n",
      "Iteration 2475: loss 0.0009057729039341211\n",
      "Iteration 2476: loss 0.000905774999409914\n",
      "Iteration 2477: loss 0.0009057737188413739\n",
      "Iteration 2478: loss 0.0009057646384462714\n",
      "Iteration 2479: loss 0.0009057645802386105\n",
      "Iteration 2480: loss 0.000905762892216444\n",
      "Iteration 2481: loss 0.0009057583520188928\n",
      "Iteration 2482: loss 0.0009057576535269618\n",
      "Iteration 2483: loss 0.0009057557908818126\n",
      "Iteration 2484: loss 0.0009057532297447324\n",
      "Iteration 2485: loss 0.0009057507268153131\n",
      "Iteration 2486: loss 0.0009057483985088766\n",
      "Iteration 2487: loss 0.000905742053873837\n",
      "Iteration 2488: loss 0.0009057477582246065\n",
      "Iteration 2489: loss 0.0009057394927367568\n",
      "Iteration 2490: loss 0.0009057421120814979\n",
      "Iteration 2491: loss 0.0009057363495230675\n",
      "Iteration 2492: loss 0.0009057312272489071\n",
      "Iteration 2493: loss 0.0009057305287569761\n",
      "Iteration 2494: loss 0.0009057258139364421\n",
      "Iteration 2495: loss 0.0009057261049747467\n",
      "Iteration 2496: loss 0.0009057249408215284\n",
      "Iteration 2497: loss 0.0009057227871380746\n",
      "Iteration 2498: loss 0.0009057181887328625\n",
      "Iteration 2499: loss 0.0009057186543941498\n",
      "Iteration 2500: loss 0.0009057139977812767\n",
      "Iteration 2501: loss 0.0009057113202288747\n",
      "Iteration 2502: loss 0.0009057103889063001\n",
      "Iteration 2503: loss 0.0009057086426764727\n",
      "Iteration 2504: loss 0.0009057054994627833\n",
      "Iteration 2505: loss 0.0009057078277692199\n",
      "Iteration 2506: loss 0.0009057053248398006\n",
      "Iteration 2507: loss 0.000905701657757163\n",
      "Iteration 2508: loss 0.0009056979324668646\n",
      "Iteration 2509: loss 0.0009056987473741174\n",
      "Iteration 2510: loss 0.0009056954877451062\n",
      "Iteration 2511: loss 0.0009056928101927042\n",
      "Iteration 2512: loss 0.0009056919952854514\n",
      "Iteration 2513: loss 0.0009056916460394859\n",
      "Iteration 2514: loss 0.0009056897833943367\n",
      "Iteration 2515: loss 0.0009056831477209926\n",
      "Iteration 2516: loss 0.0009056864073500037\n",
      "Iteration 2517: loss 0.0009056842536665499\n",
      "Iteration 2518: loss 0.0009056825656443834\n",
      "Iteration 2519: loss 0.0009056776762008667\n",
      "Iteration 2520: loss 0.0009056798880919814\n",
      "Iteration 2521: loss 0.0009056724375113845\n",
      "Iteration 2522: loss 0.0009056685958057642\n",
      "Iteration 2523: loss 0.000905668712221086\n",
      "Iteration 2524: loss 0.0009056702256202698\n",
      "Iteration 2525: loss 0.0009056699927896261\n",
      "Iteration 2526: loss 0.0009056648705154657\n",
      "Iteration 2527: loss 0.0009056611452251673\n",
      "Iteration 2528: loss 0.0009056623093783855\n",
      "Iteration 2529: loss 0.0009056590497493744\n",
      "Iteration 2530: loss 0.0009056583512574434\n",
      "Iteration 2531: loss 0.0009056559065356851\n",
      "Iteration 2532: loss 0.0009056511335074902\n",
      "Iteration 2533: loss 0.0009056492708623409\n",
      "Iteration 2534: loss 0.0009056499693542719\n",
      "Iteration 2535: loss 0.0009056497947312891\n",
      "Iteration 2536: loss 0.0009056479320861399\n",
      "Iteration 2537: loss 0.0009056489216163754\n",
      "Iteration 2538: loss 0.0009056446142494678\n",
      "Iteration 2539: loss 0.0009056426351889968\n",
      "Iteration 2540: loss 0.0009056417038664222\n",
      "Iteration 2541: loss 0.0009056396665982902\n",
      "Iteration 2542: loss 0.0009056346607394516\n",
      "Iteration 2543: loss 0.0009056378621608019\n",
      "Iteration 2544: loss 0.0009056372800841928\n",
      "Iteration 2545: loss 0.0009056349517777562\n",
      "Iteration 2546: loss 0.0009056355338543653\n",
      "Iteration 2547: loss 0.0009056295384652913\n",
      "Iteration 2548: loss 0.0009056266862899065\n",
      "Iteration 2549: loss 0.0009056263952516019\n",
      "Iteration 2550: loss 0.0009056229027919471\n",
      "Iteration 2551: loss 0.0009056265116669238\n",
      "Iteration 2552: loss 0.0009056208073161542\n",
      "Iteration 2553: loss 0.000905620981939137\n",
      "Iteration 2554: loss 0.000905622960999608\n",
      "Iteration 2555: loss 0.0009056186536327004\n",
      "Iteration 2556: loss 0.0009056155104190111\n",
      "Iteration 2557: loss 0.0009056143462657928\n",
      "Iteration 2558: loss 0.0009056146373040974\n",
      "Iteration 2559: loss 0.0009056121343746781\n",
      "Iteration 2560: loss 0.0009056119015440345\n",
      "Iteration 2561: loss 0.000905610853806138\n",
      "Iteration 2562: loss 0.0009056134731508791\n",
      "Iteration 2563: loss 0.0009056093986146152\n",
      "Iteration 2564: loss 0.0009056064300239086\n",
      "Iteration 2565: loss 0.0009056084672920406\n",
      "Iteration 2566: loss 0.0009056056733243167\n",
      "Iteration 2567: loss 0.0009056028793565929\n",
      "Iteration 2568: loss 0.0009056020062416792\n",
      "Iteration 2569: loss 0.0009055963018909097\n",
      "Iteration 2570: loss 0.0009055996779352427\n",
      "Iteration 2571: loss 0.0009055953705683351\n",
      "Iteration 2572: loss 0.0009055964183062315\n",
      "Iteration 2573: loss 0.0009055929258465767\n",
      "Iteration 2574: loss 0.0009055905975401402\n",
      "Iteration 2575: loss 0.0009055930422618985\n",
      "Iteration 2576: loss 0.0009055942064151168\n",
      "Iteration 2577: loss 0.0009055910632014275\n",
      "Iteration 2578: loss 0.0009055894333869219\n",
      "Iteration 2579: loss 0.0009055856498889625\n",
      "Iteration 2580: loss 0.000905584660358727\n",
      "Iteration 2581: loss 0.0009055887348949909\n",
      "Iteration 2582: loss 0.0009055848931893706\n",
      "Iteration 2583: loss 0.0009055845439434052\n",
      "Iteration 2584: loss 0.0009055820992216468\n",
      "Iteration 2585: loss 0.0009055822738446295\n",
      "Iteration 2586: loss 0.0009055814007297158\n",
      "Iteration 2587: loss 0.0009055822156369686\n",
      "Iteration 2588: loss 0.0009055783157236874\n",
      "Iteration 2589: loss 0.0009055803529918194\n",
      "Iteration 2590: loss 0.0009055742993950844\n",
      "Iteration 2591: loss 0.0009055764530785382\n",
      "Iteration 2592: loss 0.0009055763948708773\n",
      "Iteration 2593: loss 0.0009055763366632164\n",
      "Iteration 2594: loss 0.0009055717382580042\n",
      "Iteration 2595: loss 0.0009055699920281768\n",
      "Iteration 2596: loss 0.0009055688278749585\n",
      "Iteration 2597: loss 0.0009055706905201077\n",
      "Iteration 2598: loss 0.0009055668488144875\n",
      "Iteration 2599: loss 0.0009055680129677057\n",
      "Iteration 2600: loss 0.0009055652772076428\n",
      "Iteration 2601: loss 0.0009055667906068265\n",
      "Iteration 2602: loss 0.0009055613772943616\n",
      "Iteration 2603: loss 0.000905563763808459\n",
      "Iteration 2604: loss 0.0009055625414475799\n",
      "Iteration 2605: loss 0.0009055621922016144\n",
      "Iteration 2606: loss 0.000905559048987925\n",
      "Iteration 2607: loss 0.0009055562550202012\n",
      "Iteration 2608: loss 0.0009055561386048794\n",
      "Iteration 2609: loss 0.0009055551490746439\n",
      "Iteration 2610: loss 0.0009055566042661667\n",
      "Iteration 2611: loss 0.0009055564878508449\n",
      "Iteration 2612: loss 0.0009055561386048794\n",
      "Iteration 2613: loss 0.0009055515401996672\n",
      "Iteration 2614: loss 0.0009055549744516611\n",
      "Iteration 2615: loss 0.0009055465925484896\n",
      "Iteration 2616: loss 0.0009055522386915982\n",
      "Iteration 2617: loss 0.0009055495029315352\n",
      "Iteration 2618: loss 0.000905550317838788\n",
      "Iteration 2619: loss 0.0009055491536855698\n",
      "Iteration 2620: loss 0.000905544264242053\n",
      "Iteration 2621: loss 0.0009055465925484896\n",
      "Iteration 2622: loss 0.0009055460104718804\n",
      "Iteration 2623: loss 0.0009055475820787251\n",
      "Iteration 2624: loss 0.0009055451955646276\n",
      "Iteration 2625: loss 0.000905546941794455\n",
      "Iteration 2626: loss 0.0009055453119799495\n",
      "Iteration 2627: loss 0.0009055385598912835\n",
      "Iteration 2628: loss 0.0009055378031916916\n",
      "Iteration 2629: loss 0.00090553960762918\n",
      "Iteration 2630: loss 0.0009055407135747373\n",
      "Iteration 2631: loss 0.0009055371046997607\n",
      "Iteration 2632: loss 0.0009055353002622724\n",
      "Iteration 2633: loss 0.0009055337868630886\n",
      "Iteration 2634: loss 0.0009055344853550196\n",
      "Iteration 2635: loss 0.000905535533092916\n",
      "Iteration 2636: loss 0.0009055376285687089\n",
      "Iteration 2637: loss 0.0009055302944034338\n",
      "Iteration 2638: loss 0.0009055318660102785\n",
      "Iteration 2639: loss 0.0009055319824256003\n",
      "Iteration 2640: loss 0.0009055301197804511\n",
      "Iteration 2641: loss 0.0009055302944034338\n",
      "Iteration 2642: loss 0.0009055316913872957\n",
      "Iteration 2643: loss 0.0009055292466655374\n",
      "Iteration 2644: loss 0.0009055323316715658\n",
      "Iteration 2645: loss 0.0009055272676050663\n",
      "Iteration 2646: loss 0.0009055280243046582\n",
      "Iteration 2647: loss 0.0009055249392986298\n",
      "Iteration 2648: loss 0.0009055210975930095\n",
      "Iteration 2649: loss 0.0009055263362824917\n",
      "Iteration 2650: loss 0.0009055244736373425\n",
      "Iteration 2651: loss 0.0009055223781615496\n",
      "Iteration 2652: loss 0.0009055250557139516\n",
      "Iteration 2653: loss 0.0009055198170244694\n",
      "Iteration 2654: loss 0.000905522727407515\n",
      "Iteration 2655: loss 0.0009055235423147678\n",
      "Iteration 2656: loss 0.0009055208647623658\n",
      "Iteration 2657: loss 0.0009055181872099638\n",
      "Iteration 2658: loss 0.00090551603352651\n",
      "Iteration 2659: loss 0.0009055167902261019\n",
      "Iteration 2660: loss 0.0009055162663571537\n",
      "Iteration 2661: loss 0.0009055183618329465\n",
      "Iteration 2662: loss 0.0009055146365426481\n",
      "Iteration 2663: loss 0.0009055143455043435\n",
      "Iteration 2664: loss 0.0009055173140950501\n",
      "Iteration 2665: loss 0.000905513996258378\n",
      "Iteration 2666: loss 0.0009055114351212978\n",
      "Iteration 2667: loss 0.0009055156260728836\n",
      "Iteration 2668: loss 0.000905512657482177\n",
      "Iteration 2669: loss 0.0009055091068148613\n",
      "Iteration 2670: loss 0.0009055138798430562\n",
      "Iteration 2671: loss 0.000905513996258378\n",
      "Iteration 2672: loss 0.0009055102709680796\n",
      "Iteration 2673: loss 0.0009055070695467293\n",
      "Iteration 2674: loss 0.000905507942661643\n",
      "Iteration 2675: loss 0.0009055074770003557\n",
      "Iteration 2676: loss 0.0009055088157765567\n",
      "Iteration 2677: loss 0.0009055099217221141\n",
      "Iteration 2678: loss 0.0009055039845407009\n",
      "Iteration 2679: loss 0.0009055017144419253\n",
      "Iteration 2680: loss 0.0009055068949237466\n",
      "Iteration 2681: loss 0.0009055081754922867\n",
      "Iteration 2682: loss 0.0009055047994479537\n",
      "Iteration 2683: loss 0.0009055020054802299\n",
      "Iteration 2684: loss 0.0009055041009560227\n",
      "Iteration 2685: loss 0.0009055039845407009\n",
      "Iteration 2686: loss 0.0009055022965185344\n",
      "Iteration 2687: loss 0.0009055015398189425\n",
      "Iteration 2688: loss 0.0009055013069882989\n",
      "Iteration 2689: loss 0.0009054947877302766\n",
      "Iteration 2690: loss 0.0009054968832060695\n",
      "Iteration 2691: loss 0.0009054997353814542\n",
      "Iteration 2692: loss 0.0009055016562342644\n",
      "Iteration 2693: loss 0.0009054984548129141\n",
      "Iteration 2694: loss 0.0009055042173713446\n",
      "Iteration 2695: loss 0.0009055039845407009\n",
      "Iteration 2696: loss 0.0009054951369762421\n",
      "Iteration 2697: loss 0.0009054951369762421\n",
      "Iteration 2698: loss 0.0009054963011294603\n",
      "Iteration 2699: loss 0.0009054947877302766\n",
      "Iteration 2700: loss 0.0009054968832060695\n",
      "Iteration 2701: loss 0.0009054921683855355\n",
      "Iteration 2702: loss 0.0009054948459379375\n",
      "Iteration 2703: loss 0.0009054945548996329\n",
      "Iteration 2704: loss 0.0009054942056536674\n",
      "Iteration 2705: loss 0.0009054907131940126\n",
      "Iteration 2706: loss 0.0009054951369762421\n",
      "Iteration 2707: loss 0.0009054926922544837\n",
      "Iteration 2708: loss 0.0009054883848875761\n",
      "Iteration 2709: loss 0.0009054905967786908\n",
      "Iteration 2710: loss 0.0009054916445165873\n",
      "Iteration 2711: loss 0.0009054888505488634\n",
      "Iteration 2712: loss 0.0009054883848875761\n",
      "Iteration 2713: loss 0.0009054894326254725\n",
      "Iteration 2714: loss 0.000905487802810967\n",
      "Iteration 2715: loss 0.0009054890833795071\n",
      "Iteration 2716: loss 0.0009054865222424269\n",
      "Iteration 2717: loss 0.0009054840193130076\n",
      "Iteration 2718: loss 0.000905487802810967\n",
      "Iteration 2719: loss 0.0009054887341335416\n",
      "Iteration 2720: loss 0.0009054857073351741\n",
      "Iteration 2721: loss 0.0009054888505488634\n",
      "Iteration 2722: loss 0.0009054833790287375\n",
      "Iteration 2723: loss 0.0009054848924279213\n",
      "Iteration 2724: loss 0.0009054843685589731\n",
      "Iteration 2725: loss 0.0009054857073351741\n",
      "Iteration 2726: loss 0.0009054865804500878\n",
      "Iteration 2727: loss 0.0009054830297827721\n",
      "Iteration 2728: loss 0.0009054865222424269\n",
      "Iteration 2729: loss 0.000905486405827105\n",
      "Iteration 2730: loss 0.0009054820984601974\n",
      "Iteration 2731: loss 0.0009054805850610137\n",
      "Iteration 2732: loss 0.0009054822148755193\n",
      "Iteration 2733: loss 0.0009054815163835883\n",
      "Iteration 2734: loss 0.0009054815163835883\n",
      "Iteration 2735: loss 0.0009054807596839964\n",
      "Iteration 2736: loss 0.0009054818656295538\n",
      "Iteration 2737: loss 0.0009054771508090198\n",
      "Iteration 2738: loss 0.0009054788388311863\n",
      "Iteration 2739: loss 0.0009054811671376228\n",
      "Iteration 2740: loss 0.0009054795373231173\n",
      "Iteration 2741: loss 0.000905477674677968\n",
      "Iteration 2742: loss 0.0009054793044924736\n",
      "Iteration 2743: loss 0.0009054758120328188\n",
      "Iteration 2744: loss 0.0009054836118593812\n",
      "Iteration 2745: loss 0.0009054808760993183\n",
      "Iteration 2746: loss 0.0009054736001417041\n",
      "Iteration 2747: loss 0.0009054754627868533\n",
      "Iteration 2748: loss 0.0009054731344804168\n",
      "Iteration 2749: loss 0.0009054769761860371\n",
      "Iteration 2750: loss 0.0009054752881638706\n",
      "Iteration 2751: loss 0.0009054755792021751\n",
      "Iteration 2752: loss 0.0009054720285348594\n",
      "Iteration 2753: loss 0.0009054722031578422\n",
      "Iteration 2754: loss 0.0009054731344804168\n",
      "Iteration 2755: loss 0.0009054725524038076\n",
      "Iteration 2756: loss 0.0009054727270267904\n",
      "Iteration 2757: loss 0.0009054700494743884\n",
      "Iteration 2758: loss 0.0009054708061739802\n",
      "Iteration 2759: loss 0.0009054725524038076\n",
      "Iteration 2760: loss 0.0009054706315509975\n",
      "Iteration 2761: loss 0.0009054704569280148\n",
      "Iteration 2762: loss 0.0009054703405126929\n",
      "Iteration 2763: loss 0.0009054754627868533\n",
      "Iteration 2764: loss 0.0009054699912667274\n",
      "Iteration 2765: loss 0.0009054697002284229\n",
      "Iteration 2766: loss 0.0009054704569280148\n",
      "Iteration 2767: loss 0.0009054701076820493\n",
      "Iteration 2768: loss 0.0009054676629602909\n",
      "Iteration 2769: loss 0.0009054692927747965\n",
      "Iteration 2770: loss 0.0009054687106981874\n",
      "Iteration 2771: loss 0.0009054732508957386\n",
      "Iteration 2772: loss 0.0009054694091901183\n",
      "Iteration 2773: loss 0.0009054699912667274\n",
      "Iteration 2774: loss 0.0009054690017364919\n",
      "Iteration 2775: loss 0.0009054681286215782\n",
      "Iteration 2776: loss 0.0009054667316377163\n",
      "Iteration 2777: loss 0.0009054656838998199\n",
      "Iteration 2778: loss 0.0009054642869159579\n",
      "Iteration 2779: loss 0.0009054647525772452\n",
      "Iteration 2780: loss 0.0009054702240973711\n",
      "Iteration 2781: loss 0.0009054680704139173\n",
      "Iteration 2782: loss 0.0009054658585228026\n",
      "Iteration 2783: loss 0.0009054677211679518\n",
      "Iteration 2784: loss 0.00090546696446836\n",
      "Iteration 2785: loss 0.0009054641705006361\n",
      "Iteration 2786: loss 0.000905466265976429\n",
      "Iteration 2787: loss 0.0009054676629602909\n",
      "Iteration 2788: loss 0.0009054688271135092\n",
      "Iteration 2789: loss 0.0009054698748514056\n",
      "Iteration 2790: loss 0.0009054645779542625\n",
      "Iteration 2791: loss 0.0009054632391780615\n",
      "Iteration 2792: loss 0.0009054639376699924\n",
      "Iteration 2793: loss 0.0009054637048393488\n",
      "Iteration 2794: loss 0.0009054598631337285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2795: loss 0.0009054607944563031\n",
      "Iteration 2796: loss 0.0009054597467184067\n",
      "Iteration 2797: loss 0.0009054651018232107\n",
      "Iteration 2798: loss 0.0009054648689925671\n",
      "Iteration 2799: loss 0.0009054587571881711\n",
      "Iteration 2800: loss 0.0009054646361619234\n",
      "Iteration 2801: loss 0.000905462889932096\n",
      "Iteration 2802: loss 0.0009054627153091133\n",
      "Iteration 2803: loss 0.0009054638212546706\n",
      "Iteration 2804: loss 0.0009054586989805102\n",
      "Iteration 2805: loss 0.0009054651018232107\n",
      "Iteration 2806: loss 0.0009054627735167742\n",
      "Iteration 2807: loss 0.0009054582333192229\n",
      "Iteration 2808: loss 0.0009054607944563031\n",
      "Iteration 2809: loss 0.0009054610272869468\n",
      "Iteration 2810: loss 0.0009054593974724412\n",
      "Iteration 2811: loss 0.0009054594556801021\n",
      "Iteration 2812: loss 0.0009054607944563031\n",
      "Iteration 2813: loss 0.0009054606780409813\n",
      "Iteration 2814: loss 0.0009054580004885793\n",
      "Iteration 2815: loss 0.0009054589318111539\n",
      "Iteration 2816: loss 0.0009054567199200392\n",
      "Iteration 2817: loss 0.0009054554393514991\n",
      "Iteration 2818: loss 0.0009054618421941996\n",
      "Iteration 2819: loss 0.0009054591646417975\n",
      "Iteration 2820: loss 0.0009054600959643722\n",
      "Iteration 2821: loss 0.0009054583497345448\n",
      "Iteration 2822: loss 0.0009054549736902118\n",
      "Iteration 2823: loss 0.0009054596885107458\n",
      "Iteration 2824: loss 0.0009054551483131945\n",
      "Iteration 2825: loss 0.0009054574184119701\n",
      "Iteration 2826: loss 0.0009054585825651884\n",
      "Iteration 2827: loss 0.00090545485727489\n",
      "Iteration 2828: loss 0.0009054560214281082\n",
      "Iteration 2829: loss 0.0009054585825651884\n",
      "Iteration 2830: loss 0.0009054567199200392\n",
      "Iteration 2831: loss 0.0009054550901055336\n",
      "Iteration 2832: loss 0.0009054552647285163\n",
      "Iteration 2833: loss 0.0009054540423676372\n",
      "Iteration 2834: loss 0.0009054559050127864\n",
      "Iteration 2835: loss 0.000905454158782959\n",
      "Iteration 2836: loss 0.0009054523543454707\n",
      "Iteration 2837: loss 0.0009054549736902118\n",
      "Iteration 2838: loss 0.0009054598631337285\n",
      "Iteration 2839: loss 0.0009054511901922524\n",
      "Iteration 2840: loss 0.0009054549736902118\n",
      "Iteration 2841: loss 0.0009054567199200392\n",
      "Iteration 2842: loss 0.000905450142454356\n",
      "Iteration 2843: loss 0.000905454158782959\n",
      "Iteration 2844: loss 0.0009054557885974646\n",
      "Iteration 2845: loss 0.0009054557885974646\n",
      "Iteration 2846: loss 0.0009054570691660047\n",
      "Iteration 2847: loss 0.0009054526453837752\n",
      "Iteration 2848: loss 0.0009054543334059417\n",
      "Iteration 2849: loss 0.0009054535767063498\n",
      "Iteration 2850: loss 0.0009054531110450625\n",
      "Iteration 2851: loss 0.0009054517722688615\n",
      "Iteration 2852: loss 0.0009054528782144189\n",
      "Iteration 2853: loss 0.0009054547408595681\n",
      "Iteration 2854: loss 0.0009054525289684534\n",
      "Iteration 2855: loss 0.0009054562542587519\n",
      "Iteration 2856: loss 0.0009054506081156433\n",
      "Iteration 2857: loss 0.0009054557885974646\n",
      "Iteration 2858: loss 0.0009054502006620169\n",
      "Iteration 2859: loss 0.0009054520633071661\n",
      "Iteration 2860: loss 0.0009054511319845915\n",
      "Iteration 2861: loss 0.0009054524125531316\n",
      "Iteration 2862: loss 0.0009054521797224879\n",
      "Iteration 2863: loss 0.0009054517140612006\n",
      "Iteration 2864: loss 0.0009054479887709022\n",
      "Iteration 2865: loss 0.0009054522379301488\n",
      "Iteration 2866: loss 0.0009054495603777468\n",
      "Iteration 2867: loss 0.000905453460291028\n",
      "Iteration 2868: loss 0.0009054539841599762\n",
      "Iteration 2869: loss 0.0009054515976458788\n",
      "Iteration 2870: loss 0.000905447406694293\n",
      "Iteration 2871: loss 0.0009054511319845915\n",
      "Iteration 2872: loss 0.0009054485708475113\n",
      "Iteration 2873: loss 0.0009054469410330057\n",
      "Iteration 2874: loss 0.0009054524707607925\n",
      "Iteration 2875: loss 0.0009054476395249367\n",
      "Iteration 2876: loss 0.0009054521797224879\n",
      "Iteration 2877: loss 0.0009054497350007296\n",
      "Iteration 2878: loss 0.0009054465917870402\n",
      "Iteration 2879: loss 0.0009054496767930686\n",
      "Iteration 2880: loss 0.000905451423022896\n",
      "Iteration 2881: loss 0.0009054469992406666\n",
      "Iteration 2882: loss 0.0009054468246176839\n",
      "Iteration 2883: loss 0.0009054485708475113\n",
      "Iteration 2884: loss 0.0009054489200934768\n",
      "Iteration 2885: loss 0.0009054468246176839\n",
      "Iteration 2886: loss 0.0009054495603777468\n",
      "Iteration 2887: loss 0.0009054462425410748\n",
      "Iteration 2888: loss 0.000905445427633822\n",
      "Iteration 2889: loss 0.0009054497350007296\n",
      "Iteration 2890: loss 0.0009054465917870402\n",
      "Iteration 2891: loss 0.0009054468246176839\n",
      "Iteration 2892: loss 0.0009054482216015458\n",
      "Iteration 2893: loss 0.0009054506663233042\n",
      "Iteration 2894: loss 0.0009054451948031783\n",
      "Iteration 2895: loss 0.0009054446709342301\n",
      "Iteration 2896: loss 0.0009054478723555803\n",
      "Iteration 2897: loss 0.0009054448455572128\n",
      "Iteration 2898: loss 0.0009054482216015458\n",
      "Iteration 2899: loss 0.0009054470574483275\n",
      "Iteration 2900: loss 0.0009054464753717184\n",
      "Iteration 2901: loss 0.0009054478723555803\n",
      "Iteration 2902: loss 0.0009054471738636494\n",
      "Iteration 2903: loss 0.0009054468246176839\n",
      "Iteration 2904: loss 0.0009054463007487357\n",
      "Iteration 2905: loss 0.0009054449619725347\n",
      "Iteration 2906: loss 0.0009054442634806037\n",
      "Iteration 2907: loss 0.0009054475231096148\n",
      "Iteration 2908: loss 0.0009054482216015458\n",
      "Iteration 2909: loss 0.0009054464753717184\n",
      "Iteration 2910: loss 0.0009054468828253448\n",
      "Iteration 2911: loss 0.0009054432157427073\n",
      "Iteration 2912: loss 0.000905445369426161\n",
      "Iteration 2913: loss 0.0009054475231096148\n",
      "Iteration 2914: loss 0.0009054485708475113\n",
      "Iteration 2915: loss 0.0009054424008354545\n",
      "Iteration 2916: loss 0.0009054456022568047\n",
      "Iteration 2917: loss 0.0009054462425410748\n",
      "Iteration 2918: loss 0.0009054482216015458\n",
      "Iteration 2919: loss 0.0009054443798959255\n",
      "Iteration 2920: loss 0.0009054469410330057\n",
      "Iteration 2921: loss 0.0009054456604644656\n",
      "Iteration 2922: loss 0.0009054482216015458\n",
      "Iteration 2923: loss 0.0009054444963112473\n",
      "Iteration 2924: loss 0.0009054497932083905\n",
      "Iteration 2925: loss 0.0009054472320713103\n",
      "Iteration 2926: loss 0.0009054461843334138\n",
      "Iteration 2927: loss 0.0009054457186721265\n",
      "Iteration 2928: loss 0.0009054435649886727\n",
      "Iteration 2929: loss 0.000905444088857621\n",
      "Iteration 2930: loss 0.0009054422844201326\n",
      "Iteration 2931: loss 0.0009054419351741672\n",
      "Iteration 2932: loss 0.000905445427633822\n",
      "Iteration 2933: loss 0.0009054398396983743\n",
      "Iteration 2934: loss 0.0009054463589563966\n",
      "Iteration 2935: loss 0.000905445427633822\n",
      "Iteration 2936: loss 0.000905442691873759\n",
      "Iteration 2937: loss 0.0009054421680048108\n",
      "Iteration 2938: loss 0.0009054465335793793\n",
      "Iteration 2939: loss 0.0009054407710209489\n",
      "Iteration 2940: loss 0.0009054432739503682\n",
      "Iteration 2941: loss 0.0009054397232830524\n",
      "Iteration 2942: loss 0.0009054418769665062\n",
      "Iteration 2943: loss 0.0009054398396983743\n",
      "Iteration 2944: loss 0.0009054441470652819\n",
      "Iteration 2945: loss 0.000905442051589489\n",
      "Iteration 2946: loss 0.0009054451948031783\n",
      "Iteration 2947: loss 0.0009054421097971499\n",
      "Iteration 2948: loss 0.0009054431575350463\n",
      "Iteration 2949: loss 0.0009054403053596616\n",
      "Iteration 2950: loss 0.0009054403053596616\n",
      "Iteration 2951: loss 0.0009054412366822362\n",
      "Iteration 2952: loss 0.0009054460097104311\n",
      "Iteration 2953: loss 0.0009054430993273854\n",
      "Iteration 2954: loss 0.0009054443798959255\n",
      "Iteration 2955: loss 0.0009054444963112473\n",
      "Iteration 2956: loss 0.0009054418187588453\n",
      "Iteration 2957: loss 0.0009054426336660981\n",
      "Iteration 2958: loss 0.0009054450783878565\n",
      "Iteration 2959: loss 0.0009054429829120636\n",
      "Iteration 2960: loss 0.0009054428664967418\n",
      "Iteration 2961: loss 0.0009054448455572128\n",
      "Iteration 2962: loss 0.0009054439142346382\n",
      "Iteration 2963: loss 0.0009054462425410748\n",
      "Iteration 2964: loss 0.0009054429829120636\n",
      "Iteration 2965: loss 0.000905445369426161\n",
      "Iteration 2966: loss 0.0009054457768797874\n",
      "Iteration 2967: loss 0.0009054441470652819\n",
      "Iteration 2968: loss 0.0009054450201801956\n",
      "Iteration 2969: loss 0.0009054441470652819\n",
      "Iteration 2970: loss 0.0009054411202669144\n",
      "Iteration 2971: loss 0.0009054398396983743\n",
      "Iteration 2972: loss 0.0009054437396116555\n",
      "Iteration 2973: loss 0.0009054435649886727\n",
      "Iteration 2974: loss 0.0009054408874362707\n",
      "Iteration 2975: loss 0.0009054412366822362\n",
      "Iteration 2976: loss 0.0009054424008354545\n",
      "Iteration 2977: loss 0.0009054404217749834\n",
      "Iteration 2978: loss 0.0009054384427145123\n",
      "Iteration 2979: loss 0.0009054373949766159\n",
      "Iteration 2980: loss 0.0009054387919604778\n",
      "Iteration 2981: loss 0.000905437977053225\n",
      "Iteration 2982: loss 0.0009054394904524088\n",
      "Iteration 2983: loss 0.0009054376278072596\n",
      "Iteration 2984: loss 0.000905435997992754\n",
      "Iteration 2985: loss 0.0009054408292286098\n",
      "Iteration 2986: loss 0.0009054425754584372\n",
      "Iteration 2987: loss 0.0009054406546056271\n",
      "Iteration 2988: loss 0.0009054401307366788\n",
      "Iteration 2989: loss 0.0009054377442225814\n",
      "Iteration 2990: loss 0.0009054397814907134\n",
      "Iteration 2991: loss 0.0009054426336660981\n",
      "Iteration 2992: loss 0.0009054439724422991\n",
      "Iteration 2993: loss 0.000905444088857621\n",
      "Iteration 2994: loss 0.0009054363472387195\n",
      "Iteration 2995: loss 0.0009054380934685469\n",
      "Iteration 2996: loss 0.0009054404799826443\n",
      "Iteration 2997: loss 0.0009054414695128798\n",
      "Iteration 2998: loss 0.0009054410038515925\n",
      "Iteration 2999: loss 0.000905441353097558\n",
      "Iteration 3000: loss 0.0009054425754584372\n",
      "Iteration 3001: loss 0.0009054418769665062\n",
      "Iteration 3002: loss 0.0009054439142346382\n",
      "Iteration 3003: loss 0.0009054391994141042\n",
      "Iteration 3004: loss 0.000905437977053225\n",
      "Iteration 3005: loss 0.0009054370457306504\n",
      "Iteration 3006: loss 0.0009054395486600697\n",
      "Iteration 3007: loss 0.0009054390247911215\n",
      "Iteration 3008: loss 0.0009054397814907134\n",
      "Iteration 3009: loss 0.0009054409456439316\n",
      "Iteration 3010: loss 0.0009054437978193164\n",
      "Iteration 3011: loss 0.0009054422844201326\n",
      "Iteration 3012: loss 0.0009054410038515925\n",
      "Iteration 3013: loss 0.0009054399561136961\n",
      "Iteration 3014: loss 0.0009054361144080758\n",
      "Iteration 3015: loss 0.0009054368129000068\n",
      "Iteration 3016: loss 0.0009054394904524088\n",
      "Iteration 3017: loss 0.0009054400725290179\n",
      "Iteration 3018: loss 0.000905438675545156\n",
      "Iteration 3019: loss 0.0009054371621459723\n",
      "Iteration 3020: loss 0.0009054371039383113\n",
      "Iteration 3021: loss 0.0009054404217749834\n",
      "Iteration 3022: loss 0.0009054391412064433\n",
      "Iteration 3023: loss 0.0009054397232830524\n",
      "Iteration 3024: loss 0.0009054399561136961\n",
      "Iteration 3025: loss 0.0009054429829120636\n",
      "Iteration 3026: loss 0.0009054418187588453\n",
      "Iteration 3027: loss 0.0009054408874362707\n",
      "Iteration 3028: loss 0.0009054390247911215\n",
      "Iteration 3029: loss 0.0009054410038515925\n",
      "Iteration 3030: loss 0.0009054373949766159\n",
      "Iteration 3031: loss 0.0009054401889443398\n",
      "Iteration 3032: loss 0.0009054395486600697\n",
      "Iteration 3033: loss 0.000905437977053225\n",
      "Iteration 3034: loss 0.0009054396068677306\n",
      "Iteration 3035: loss 0.000905440014321357\n",
      "Iteration 3036: loss 0.000905436638277024\n",
      "Iteration 3037: loss 0.0009054398396983743\n",
      "Iteration 3038: loss 0.0009054379188455641\n",
      "Iteration 3039: loss 0.0009054385591298342\n",
      "Iteration 3040: loss 0.0009054363472387195\n",
      "Iteration 3041: loss 0.0009054368711076677\n",
      "Iteration 3042: loss 0.0009054398396983743\n",
      "Iteration 3043: loss 0.0009054405381903052\n",
      "Iteration 3044: loss 0.0009054411202669144\n",
      "Iteration 3045: loss 0.0009054346010088921\n",
      "Iteration 3046: loss 0.0009054371621459723\n",
      "Iteration 3047: loss 0.0009054400725290179\n",
      "Iteration 3048: loss 0.000905438675545156\n",
      "Iteration 3049: loss 0.0009054368129000068\n",
      "Iteration 3050: loss 0.0009054376278072596\n",
      "Iteration 3051: loss 0.0009054383845068514\n",
      "Iteration 3052: loss 0.0009054387919604778\n",
      "Iteration 3053: loss 0.0009054407710209489\n",
      "Iteration 3054: loss 0.000905437977053225\n",
      "Iteration 3055: loss 0.000905439374037087\n",
      "Iteration 3056: loss 0.000905437336768955\n",
      "Iteration 3057: loss 0.0009054399561136961\n",
      "Iteration 3058: loss 0.0009054392576217651\n",
      "Iteration 3059: loss 0.000905438675545156\n",
      "Iteration 3060: loss 0.0009054371621459723\n",
      "Iteration 3061: loss 0.0009054399561136961\n",
      "Iteration 3062: loss 0.0009054380934685469\n",
      "Iteration 3063: loss 0.0009054383262991905\n",
      "Iteration 3064: loss 0.0009054363472387195\n",
      "Iteration 3065: loss 0.000905437977053225\n",
      "Iteration 3066: loss 0.0009054372785612941\n",
      "Iteration 3067: loss 0.0009054368129000068\n",
      "Iteration 3068: loss 0.0009054375695995986\n",
      "Iteration 3069: loss 0.0009054356487467885\n",
      "Iteration 3070: loss 0.0009054394904524088\n",
      "Iteration 3071: loss 0.0009054390829987824\n",
      "Iteration 3072: loss 0.0009054354159161448\n",
      "Iteration 3073: loss 0.0009054396068677306\n",
      "Iteration 3074: loss 0.0009054394904524088\n",
      "Iteration 3075: loss 0.0009054348338395357\n",
      "Iteration 3076: loss 0.0009054365800693631\n",
      "Iteration 3077: loss 0.0009054382098838687\n",
      "Iteration 3078: loss 0.0009054330876097083\n",
      "Iteration 3079: loss 0.0009054366964846849\n",
      "Iteration 3080: loss 0.0009054373949766159\n",
      "Iteration 3081: loss 0.0009054318652488291\n",
      "Iteration 3082: loss 0.0009054394322447479\n",
      "Iteration 3083: loss 0.0009054368129000068\n",
      "Iteration 3084: loss 0.0009054369293153286\n",
      "Iteration 3085: loss 0.0009054403053596616\n",
      "Iteration 3086: loss 0.0009054364636540413\n",
      "Iteration 3087: loss 0.0009054347756318748\n",
      "Iteration 3088: loss 0.0009054387337528169\n",
      "Iteration 3089: loss 0.000905440014321357\n",
      "Iteration 3090: loss 0.0009054369293153286\n",
      "Iteration 3091: loss 0.0009054362308233976\n",
      "Iteration 3092: loss 0.000905435299500823\n",
      "Iteration 3093: loss 0.0009054371621459723\n",
      "Iteration 3094: loss 0.0009054362890310585\n",
      "Iteration 3095: loss 0.0009054375113919377\n",
      "Iteration 3096: loss 0.0009054381516762078\n",
      "Iteration 3097: loss 0.0009054353577084839\n",
      "Iteration 3098: loss 0.0009054364636540413\n",
      "Iteration 3099: loss 0.0009054351830855012\n",
      "Iteration 3100: loss 0.0009054371621459723\n",
      "Iteration 3101: loss 0.0009054376278072596\n",
      "Iteration 3102: loss 0.0009054366964846849\n",
      "Iteration 3103: loss 0.0009054389083757997\n",
      "Iteration 3104: loss 0.0009054358815774322\n",
      "Iteration 3105: loss 0.0009054334368556738\n",
      "Iteration 3106: loss 0.0009054403053596616\n",
      "Iteration 3107: loss 0.0009054348338395357\n",
      "Iteration 3108: loss 0.0009054377442225814\n",
      "Iteration 3109: loss 0.0009054349502548575\n",
      "Iteration 3110: loss 0.0009054397814907134\n",
      "Iteration 3111: loss 0.000905439374037087\n",
      "Iteration 3112: loss 0.0009054340771399438\n",
      "Iteration 3113: loss 0.0009054349502548575\n",
      "Iteration 3114: loss 0.0009054354159161448\n",
      "Iteration 3115: loss 0.0009054363472387195\n",
      "Iteration 3116: loss 0.0009054351830855012\n",
      "Iteration 3117: loss 0.0009054361726157367\n",
      "Iteration 3118: loss 0.000905435299500823\n",
      "Iteration 3119: loss 0.0009054397232830524\n",
      "Iteration 3120: loss 0.0009054365800693631\n",
      "Iteration 3121: loss 0.0009054350084625185\n",
      "Iteration 3122: loss 0.0009054366964846849\n",
      "Iteration 3123: loss 0.0009054315742105246\n",
      "Iteration 3124: loss 0.0009054370457306504\n",
      "Iteration 3125: loss 0.000905438675545156\n",
      "Iteration 3126: loss 0.0009054370457306504\n",
      "Iteration 3127: loss 0.0009054360562004149\n",
      "Iteration 3128: loss 0.0009054340189322829\n",
      "Iteration 3129: loss 0.0009054346010088921\n",
      "Iteration 3130: loss 0.0009054350666701794\n",
      "Iteration 3131: loss 0.0009054385591298342\n",
      "Iteration 3132: loss 0.0009054344263859093\n",
      "Iteration 3133: loss 0.0009054366964846849\n",
      "Iteration 3134: loss 0.0009054385591298342\n",
      "Iteration 3135: loss 0.000905437977053225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3136: loss 0.0009054365800693631\n",
      "Iteration 3137: loss 0.0009054365218617022\n",
      "Iteration 3138: loss 0.0009054344845935702\n",
      "Iteration 3139: loss 0.0009054348920471966\n",
      "Iteration 3140: loss 0.0009054388501681387\n",
      "Iteration 3141: loss 0.0009054306428879499\n",
      "Iteration 3142: loss 0.0009054380934685469\n",
      "Iteration 3143: loss 0.0009054370457306504\n",
      "Iteration 3144: loss 0.0009054374531842768\n",
      "Iteration 3145: loss 0.0009054330294020474\n",
      "Iteration 3146: loss 0.0009054398396983743\n",
      "Iteration 3147: loss 0.0009054353577084839\n",
      "Iteration 3148: loss 0.0009054345428012311\n",
      "Iteration 3149: loss 0.0009054375113919377\n",
      "Iteration 3150: loss 0.0009054328547790647\n",
      "Iteration 3151: loss 0.0009054408874362707\n",
      "Iteration 3152: loss 0.000905433320440352\n",
      "Iteration 3153: loss 0.0009054366964846849\n",
      "Iteration 3154: loss 0.0009054358815774322\n",
      "Iteration 3155: loss 0.0009054312249645591\n",
      "Iteration 3156: loss 0.0009054369293153286\n",
      "Iteration 3157: loss 0.0009054347174242139\n",
      "Iteration 3158: loss 0.0009054321562871337\n",
      "Iteration 3159: loss 0.0009054371621459723\n",
      "Iteration 3160: loss 0.0009054348920471966\n",
      "Iteration 3161: loss 0.0009054341353476048\n",
      "Iteration 3162: loss 0.0009054372203536332\n",
      "Iteration 3163: loss 0.0009054334368556738\n",
      "Iteration 3164: loss 0.0009054391412064433\n",
      "Iteration 3165: loss 0.0009054324473254383\n",
      "Iteration 3166: loss 0.0009054404217749834\n",
      "Iteration 3167: loss 0.0009054327383637428\n",
      "Iteration 3168: loss 0.0009054359397850931\n",
      "Iteration 3169: loss 0.0009054362308233976\n",
      "Iteration 3170: loss 0.0009054358233697712\n",
      "Iteration 3171: loss 0.0009054376278072596\n",
      "Iteration 3172: loss 0.0009054368129000068\n",
      "Iteration 3173: loss 0.0009054322144947946\n",
      "Iteration 3174: loss 0.0009054357069544494\n",
      "Iteration 3175: loss 0.0009054331458173692\n",
      "Iteration 3176: loss 0.0009054362308233976\n",
      "Iteration 3177: loss 0.0009054342517629266\n",
      "Iteration 3178: loss 0.000905432621948421\n",
      "Iteration 3179: loss 0.0009054383262991905\n",
      "Iteration 3180: loss 0.0009054320398718119\n",
      "Iteration 3181: loss 0.0009054390247911215\n",
      "Iteration 3182: loss 0.0009054331458173692\n",
      "Iteration 3183: loss 0.0009054361144080758\n",
      "Iteration 3184: loss 0.0009054366964846849\n",
      "Iteration 3185: loss 0.0009054337278939784\n",
      "Iteration 3186: loss 0.0009054366964846849\n",
      "Iteration 3187: loss 0.0009054311085492373\n",
      "Iteration 3188: loss 0.000905438675545156\n",
      "Iteration 3189: loss 0.0009054332040250301\n",
      "Iteration 3190: loss 0.0009054340189322829\n",
      "Iteration 3191: loss 0.0009054344845935702\n",
      "Iteration 3192: loss 0.0009054369293153286\n",
      "Iteration 3193: loss 0.0009054354159161448\n",
      "Iteration 3194: loss 0.0009054383262991905\n",
      "Iteration 3195: loss 0.0009054328547790647\n",
      "Iteration 3196: loss 0.0009054348338395357\n",
      "Iteration 3197: loss 0.0009054347174242139\n",
      "Iteration 3198: loss 0.0009054348338395357\n",
      "Iteration 3199: loss 0.0009054383845068514\n",
      "Iteration 3200: loss 0.0009054347756318748\n",
      "Iteration 3201: loss 0.0009054359397850931\n",
      "Iteration 3202: loss 0.0009054346010088921\n",
      "Iteration 3203: loss 0.0009054346010088921\n",
      "Iteration 3204: loss 0.0009054382098838687\n",
      "Iteration 3205: loss 0.0009054327965714037\n",
      "Iteration 3206: loss 0.0009054364636540413\n",
      "Iteration 3207: loss 0.0009054332040250301\n",
      "Iteration 3208: loss 0.0009054341935552657\n",
      "Iteration 3209: loss 0.0009054381516762078\n",
      "Iteration 3210: loss 0.0009054318652488291\n",
      "Iteration 3211: loss 0.0009054401889443398\n",
      "Iteration 3212: loss 0.0009054336696863174\n",
      "Iteration 3213: loss 0.0009054325055330992\n",
      "Iteration 3214: loss 0.0009054385591298342\n",
      "Iteration 3215: loss 0.0009054314577952027\n",
      "Iteration 3216: loss 0.0009054363472387195\n",
      "Iteration 3217: loss 0.0009054364054463804\n",
      "Iteration 3218: loss 0.0009054335532709956\n",
      "Iteration 3219: loss 0.0009054396068677306\n",
      "Iteration 3220: loss 0.0009054316324181855\n",
      "Iteration 3221: loss 0.0009054332040250301\n",
      "Iteration 3222: loss 0.0009054348338395357\n",
      "Iteration 3223: loss 0.0009054342517629266\n",
      "Iteration 3224: loss 0.0009054351248778403\n",
      "Iteration 3225: loss 0.0009054357069544494\n",
      "Iteration 3226: loss 0.0009054375113919377\n",
      "Iteration 3227: loss 0.0009054316906258464\n",
      "Iteration 3228: loss 0.0009054354159161448\n",
      "Iteration 3229: loss 0.0009054344263859093\n",
      "Iteration 3230: loss 0.0009054397232830524\n",
      "Iteration 3231: loss 0.000905435997992754\n",
      "Iteration 3232: loss 0.0009054329711943865\n",
      "Iteration 3233: loss 0.0009054359397850931\n",
      "Iteration 3234: loss 0.0009054336696863174\n",
      "Iteration 3235: loss 0.0009054351830855012\n",
      "Iteration 3236: loss 0.0009054347174242139\n",
      "Iteration 3237: loss 0.0009054351830855012\n",
      "Iteration 3238: loss 0.0009054322144947946\n",
      "Iteration 3239: loss 0.0009054376860149205\n",
      "Iteration 3240: loss 0.0009054336114786565\n",
      "Iteration 3241: loss 0.0009054341935552657\n",
      "Iteration 3242: loss 0.0009054380934685469\n",
      "Iteration 3243: loss 0.0009054332040250301\n",
      "Iteration 3244: loss 0.0009054341353476048\n",
      "Iteration 3245: loss 0.0009054346010088921\n",
      "Iteration 3246: loss 0.0009054341353476048\n",
      "Iteration 3247: loss 0.0009054366964846849\n",
      "Iteration 3248: loss 0.0009054335532709956\n",
      "Iteration 3249: loss 0.0009054338443093002\n",
      "Iteration 3250: loss 0.0009054354159161448\n",
      "Iteration 3251: loss 0.0009054309921339154\n",
      "Iteration 3252: loss 0.0009054372785612941\n",
      "Iteration 3253: loss 0.0009054327965714037\n",
      "Iteration 3254: loss 0.0009054344845935702\n",
      "Iteration 3255: loss 0.0009054343681782484\n",
      "Iteration 3256: loss 0.0009054332040250301\n",
      "Iteration 3257: loss 0.0009054350666701794\n",
      "Iteration 3258: loss 0.0009054312249645591\n",
      "Iteration 3259: loss 0.0009054371621459723\n",
      "Iteration 3260: loss 0.0009054329711943865\n",
      "Iteration 3261: loss 0.0009054334368556738\n",
      "Iteration 3262: loss 0.0009054366964846849\n",
      "Iteration 3263: loss 0.0009054318652488291\n",
      "Iteration 3264: loss 0.0009054348338395357\n",
      "Iteration 3265: loss 0.0009054375113919377\n",
      "Iteration 3266: loss 0.0009054334368556738\n",
      "Iteration 3267: loss 0.0009054322727024555\n",
      "Iteration 3268: loss 0.0009054366964846849\n",
      "Iteration 3269: loss 0.0009054336696863174\n",
      "Iteration 3270: loss 0.0009054335532709956\n",
      "Iteration 3271: loss 0.000905435299500823\n",
      "Iteration 3272: loss 0.0009054340189322829\n",
      "Iteration 3273: loss 0.0009054334368556738\n",
      "Iteration 3274: loss 0.0009054351248778403\n",
      "Iteration 3275: loss 0.0009054349502548575\n",
      "Iteration 3276: loss 0.0009054348338395357\n",
      "Iteration 3277: loss 0.0009054323891177773\n",
      "Iteration 3278: loss 0.0009054343681782484\n",
      "Iteration 3279: loss 0.0009054340189322829\n",
      "Iteration 3280: loss 0.0009054318070411682\n",
      "Iteration 3281: loss 0.000905435997992754\n",
      "Iteration 3282: loss 0.0009054334950633347\n",
      "Iteration 3283: loss 0.0009054325055330992\n",
      "Iteration 3284: loss 0.0009054351830855012\n",
      "Iteration 3285: loss 0.0009054358815774322\n",
      "Iteration 3286: loss 0.000905433262232691\n",
      "Iteration 3287: loss 0.0009054357069544494\n",
      "Iteration 3288: loss 0.0009054375695995986\n",
      "Iteration 3289: loss 0.0009054332040250301\n",
      "Iteration 3290: loss 0.0009054338443093002\n",
      "Iteration 3291: loss 0.0009054340189322829\n",
      "Iteration 3292: loss 0.0009054346010088921\n",
      "Iteration 3293: loss 0.0009054341353476048\n",
      "Iteration 3294: loss 0.0009054328547790647\n",
      "Iteration 3295: loss 0.0009054342517629266\n",
      "Iteration 3296: loss 0.0009054346010088921\n",
      "Iteration 3297: loss 0.0009054371039383113\n",
      "Iteration 3298: loss 0.0009054315160028636\n",
      "Iteration 3299: loss 0.00090543192345649\n",
      "Iteration 3300: loss 0.0009054347174242139\n",
      "Iteration 3301: loss 0.0009054336696863174\n",
      "Iteration 3302: loss 0.0009054344845935702\n",
      "Iteration 3303: loss 0.0009054342517629266\n",
      "Iteration 3304: loss 0.0009054348920471966\n",
      "Iteration 3305: loss 0.0009054347174242139\n",
      "Iteration 3306: loss 0.0009054336696863174\n",
      "Iteration 3307: loss 0.0009054372785612941\n",
      "Iteration 3308: loss 0.0009054348338395357\n",
      "Iteration 3309: loss 0.0009054313413798809\n",
      "Iteration 3310: loss 0.0009054366964846849\n",
      "Iteration 3311: loss 0.0009054311667568982\n",
      "Iteration 3312: loss 0.0009054341935552657\n",
      "Iteration 3313: loss 0.0009054330876097083\n",
      "Iteration 3314: loss 0.0009054335532709956\n",
      "Iteration 3315: loss 0.0009054350666701794\n",
      "Iteration 3316: loss 0.0009054297115653753\n",
      "Iteration 3317: loss 0.0009054322727024555\n",
      "Iteration 3318: loss 0.0009054369293153286\n",
      "Iteration 3319: loss 0.0009054364636540413\n",
      "Iteration 3320: loss 0.0009054354159161448\n",
      "Iteration 3321: loss 0.0009054329711943865\n",
      "Iteration 3322: loss 0.0009054315742105246\n",
      "Iteration 3323: loss 0.0009054384427145123\n",
      "Iteration 3324: loss 0.0009054368711076677\n",
      "Iteration 3325: loss 0.0009054346010088921\n",
      "Iteration 3326: loss 0.0009054321562871337\n",
      "Iteration 3327: loss 0.0009054340189322829\n",
      "Iteration 3328: loss 0.0009054371039383113\n",
      "Iteration 3329: loss 0.0009054335532709956\n",
      "Iteration 3330: loss 0.000905433960724622\n",
      "Iteration 3331: loss 0.0009054313413798809\n",
      "Iteration 3332: loss 0.0009054340189322829\n",
      "Iteration 3333: loss 0.0009054354159161448\n",
      "Iteration 3334: loss 0.0009054357651621103\n",
      "Iteration 3335: loss 0.0009054336696863174\n",
      "Iteration 3336: loss 0.000905435299500823\n",
      "Iteration 3337: loss 0.0009054316906258464\n",
      "Iteration 3338: loss 0.0009054342517629266\n",
      "Iteration 3339: loss 0.0009054383262991905\n",
      "Iteration 3340: loss 0.0009054358815774322\n",
      "Iteration 3341: loss 0.0009054323891177773\n",
      "Iteration 3342: loss 0.0009054316906258464\n",
      "Iteration 3343: loss 0.0009054362308233976\n",
      "Iteration 3344: loss 0.000905437977053225\n",
      "Iteration 3345: loss 0.0009054293623194098\n",
      "Iteration 3346: loss 0.000905435299500823\n",
      "Iteration 3347: loss 0.0009054342517629266\n",
      "Iteration 3348: loss 0.0009054347756318748\n",
      "Iteration 3349: loss 0.0009054323891177773\n",
      "Iteration 3350: loss 0.0009054343681782484\n",
      "Iteration 3351: loss 0.0009054333786480129\n",
      "Iteration 3352: loss 0.00090543192345649\n",
      "Iteration 3353: loss 0.0009054328547790647\n",
      "Iteration 3354: loss 0.0009054401889443398\n",
      "Iteration 3355: loss 0.0009054323891177773\n",
      "Iteration 3356: loss 0.0009054318070411682\n",
      "Iteration 3357: loss 0.0009054339025169611\n",
      "Iteration 3358: loss 0.0009054347174242139\n",
      "Iteration 3359: loss 0.0009054336696863174\n",
      "Iteration 3360: loss 0.0009054318070411682\n",
      "Iteration 3361: loss 0.0009054329711943865\n",
      "Iteration 3362: loss 0.000905433320440352\n",
      "Iteration 3363: loss 0.0009054351830855012\n",
      "Iteration 3364: loss 0.0009054329711943865\n",
      "Iteration 3365: loss 0.0009054336696863174\n",
      "Iteration 3366: loss 0.0009054343681782484\n",
      "Iteration 3367: loss 0.000905435997992754\n",
      "Iteration 3368: loss 0.0009054327383637428\n",
      "Iteration 3369: loss 0.0009054329711943865\n",
      "Iteration 3370: loss 0.0009054340771399438\n",
      "Iteration 3371: loss 0.0009054337861016393\n",
      "Iteration 3372: loss 0.0009054322727024555\n",
      "Iteration 3373: loss 0.0009054356487467885\n",
      "Iteration 3374: loss 0.0009054355323314667\n",
      "Iteration 3375: loss 0.0009054344845935702\n",
      "Iteration 3376: loss 0.0009054342517629266\n",
      "Iteration 3377: loss 0.0009054344845935702\n",
      "Iteration 3378: loss 0.0009054361144080758\n",
      "Iteration 3379: loss 0.0009054329711943865\n",
      "Iteration 3380: loss 0.0009054368711076677\n",
      "Iteration 3381: loss 0.0009054344263859093\n",
      "Iteration 3382: loss 0.0009054372785612941\n",
      "Iteration 3383: loss 0.0009054327383637428\n",
      "Iteration 3384: loss 0.0009054337278939784\n",
      "Iteration 3385: loss 0.0009054356487467885\n",
      "Iteration 3386: loss 0.0009054364636540413\n",
      "Iteration 3387: loss 0.0009054329711943865\n",
      "Iteration 3388: loss 0.0009054362308233976\n",
      "Iteration 3389: loss 0.0009054350666701794\n",
      "Iteration 3390: loss 0.0009054369875229895\n",
      "Iteration 3391: loss 0.0009054332040250301\n",
      "Iteration 3392: loss 0.0009054366964846849\n",
      "Iteration 3393: loss 0.0009054341353476048\n",
      "Iteration 3394: loss 0.0009054343681782484\n",
      "Iteration 3395: loss 0.0009054329711943865\n",
      "Iteration 3396: loss 0.0009054354741238058\n",
      "Iteration 3397: loss 0.0009054348338395357\n",
      "Iteration 3398: loss 0.0009054348338395357\n",
      "Iteration 3399: loss 0.0009054365800693631\n",
      "Iteration 3400: loss 0.0009054351830855012\n",
      "Iteration 3401: loss 0.000905435997992754\n",
      "Iteration 3402: loss 0.0009054336114786565\n",
      "Iteration 3403: loss 0.0009054351830855012\n",
      "Iteration 3404: loss 0.0009054351830855012\n",
      "Iteration 3405: loss 0.0009054364636540413\n",
      "Iteration 3406: loss 0.0009054322727024555\n",
      "Iteration 3407: loss 0.0009054320980794728\n",
      "Iteration 3408: loss 0.0009054330294020474\n",
      "Iteration 3409: loss 0.0009054355323314667\n",
      "Iteration 3410: loss 0.0009054343681782484\n",
      "Iteration 3411: loss 0.0009054322727024555\n",
      "Iteration 3412: loss 0.0009054295951500535\n",
      "Iteration 3413: loss 0.0009054340189322829\n",
      "Iteration 3414: loss 0.0009054384427145123\n",
      "Iteration 3415: loss 0.0009054345428012311\n",
      "Iteration 3416: loss 0.0009054327383637428\n",
      "Iteration 3417: loss 0.0009054328547790647\n",
      "Iteration 3418: loss 0.0009054351248778403\n",
      "Iteration 3419: loss 0.0009054373949766159\n",
      "Iteration 3420: loss 0.0009054343099705875\n",
      "Iteration 3421: loss 0.000905433320440352\n",
      "Iteration 3422: loss 0.0009054332040250301\n",
      "Iteration 3423: loss 0.0009054371621459723\n",
      "Iteration 3424: loss 0.0009054362890310585\n",
      "Iteration 3425: loss 0.0009054351830855012\n",
      "Iteration 3426: loss 0.0009054334368556738\n",
      "Iteration 3427: loss 0.000905433262232691\n",
      "Iteration 3428: loss 0.0009054326801560819\n",
      "Iteration 3429: loss 0.0009054349502548575\n",
      "Iteration 3430: loss 0.0009054336696863174\n",
      "Iteration 3431: loss 0.0009054336696863174\n",
      "Iteration 3432: loss 0.0009054337861016393\n",
      "Iteration 3433: loss 0.0009054301772266626\n",
      "Iteration 3434: loss 0.0009054374531842768\n",
      "Iteration 3435: loss 0.0009054336696863174\n",
      "Iteration 3436: loss 0.0009054326801560819\n",
      "Iteration 3437: loss 0.0009054322727024555\n",
      "Iteration 3438: loss 0.0009054298279806972\n",
      "Iteration 3439: loss 0.0009054288966581225\n",
      "Iteration 3440: loss 0.0009054337861016393\n",
      "Iteration 3441: loss 0.0009054336696863174\n",
      "Iteration 3442: loss 0.0009054336696863174\n",
      "Iteration 3443: loss 0.0009054338443093002\n",
      "Iteration 3444: loss 0.0009054330294020474\n",
      "Iteration 3445: loss 0.0009054355905391276\n",
      "Iteration 3446: loss 0.0009054337861016393\n",
      "Iteration 3447: loss 0.0009054326801560819\n",
      "Iteration 3448: loss 0.0009054322727024555\n",
      "Iteration 3449: loss 0.0009054362308233976\n",
      "Iteration 3450: loss 0.0009054337278939784\n",
      "Iteration 3451: loss 0.0009054335532709956\n",
      "Iteration 3452: loss 0.0009054376860149205\n",
      "Iteration 3453: loss 0.0009054361726157367\n",
      "Iteration 3454: loss 0.0009054362308233976\n",
      "Iteration 3455: loss 0.0009054330876097083\n",
      "Iteration 3456: loss 0.0009054321562871337\n",
      "Iteration 3457: loss 0.0009054353577084839\n",
      "Iteration 3458: loss 0.0009054376278072596\n",
      "Iteration 3459: loss 0.0009054334368556738\n",
      "Iteration 3460: loss 0.0009054343681782484\n",
      "Iteration 3461: loss 0.0009054329711943865\n",
      "Iteration 3462: loss 0.0009054371621459723\n",
      "Iteration 3463: loss 0.0009054355905391276\n",
      "Iteration 3464: loss 0.0009054306428879499\n",
      "Iteration 3465: loss 0.00090543192345649\n",
      "Iteration 3466: loss 0.0009054325055330992\n",
      "Iteration 3467: loss 0.0009054336114786565\n",
      "Iteration 3468: loss 0.0009054338443093002\n",
      "Iteration 3469: loss 0.0009054368129000068\n",
      "Iteration 3470: loss 0.0009054348920471966\n",
      "Iteration 3471: loss 0.000905434659216553\n",
      "Iteration 3472: loss 0.0009054298861883581\n",
      "Iteration 3473: loss 0.0009054314577952027\n",
      "Iteration 3474: loss 0.0009054340771399438\n",
      "Iteration 3475: loss 0.0009054349502548575\n",
      "Iteration 3476: loss 0.0009054371621459723\n",
      "Iteration 3477: loss 0.0009054340771399438\n",
      "Iteration 3478: loss 0.000905432621948421\n",
      "Iteration 3479: loss 0.0009054308757185936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3480: loss 0.0009054334368556738\n",
      "Iteration 3481: loss 0.0009054353577084839\n",
      "Iteration 3482: loss 0.0009054330876097083\n",
      "Iteration 3483: loss 0.0009054356487467885\n",
      "Iteration 3484: loss 0.0009054351830855012\n",
      "Iteration 3485: loss 0.0009054325055330992\n",
      "Iteration 3486: loss 0.0009054337278939784\n",
      "Iteration 3487: loss 0.0009054308175109327\n",
      "Iteration 3488: loss 0.000905435299500823\n",
      "Iteration 3489: loss 0.0009054334950633347\n",
      "Iteration 3490: loss 0.0009054320980794728\n",
      "Iteration 3491: loss 0.0009054341935552657\n",
      "Iteration 3492: loss 0.0009054342517629266\n",
      "Iteration 3493: loss 0.0009054343681782484\n",
      "Iteration 3494: loss 0.0009054331458173692\n",
      "Iteration 3495: loss 0.0009054334950633347\n",
      "Iteration 3496: loss 0.00090543192345649\n",
      "Iteration 3497: loss 0.0009054375113919377\n",
      "Iteration 3498: loss 0.0009054342517629266\n",
      "Iteration 3499: loss 0.0009054337861016393\n",
      "Iteration 3500: loss 0.0009054329711943865\n",
      "Iteration 3501: loss 0.0009054357069544494\n",
      "Iteration 3502: loss 0.0009054344845935702\n",
      "Iteration 3503: loss 0.0009054361144080758\n",
      "Iteration 3504: loss 0.000905435299500823\n",
      "Iteration 3505: loss 0.0009054336696863174\n",
      "Iteration 3506: loss 0.0009054337861016393\n",
      "Iteration 3507: loss 0.0009054347756318748\n",
      "Iteration 3508: loss 0.0009054359397850931\n",
      "Iteration 3509: loss 0.0009054357651621103\n",
      "Iteration 3510: loss 0.0009054336696863174\n",
      "Iteration 3511: loss 0.0009054334368556738\n",
      "Iteration 3512: loss 0.0009054351830855012\n",
      "Iteration 3513: loss 0.0009054341353476048\n",
      "Iteration 3514: loss 0.0009054339025169611\n",
      "Iteration 3515: loss 0.0009054341353476048\n",
      "Iteration 3516: loss 0.0009054346010088921\n",
      "Iteration 3517: loss 0.0009054344845935702\n",
      "Iteration 3518: loss 0.0009054357651621103\n",
      "Iteration 3519: loss 0.0009054346010088921\n",
      "Iteration 3520: loss 0.0009054334368556738\n",
      "Iteration 3521: loss 0.0009054328547790647\n",
      "Iteration 3522: loss 0.0009054322727024555\n",
      "Iteration 3523: loss 0.0009054354159161448\n",
      "Iteration 3524: loss 0.0009054342517629266\n",
      "Iteration 3525: loss 0.0009054315742105246\n",
      "Iteration 3526: loss 0.0009054323309101164\n",
      "Iteration 3527: loss 0.0009054366964846849\n",
      "Iteration 3528: loss 0.0009054343099705875\n",
      "Iteration 3529: loss 0.0009054341353476048\n",
      "Iteration 3530: loss 0.000905433960724622\n",
      "Iteration 3531: loss 0.0009054371039383113\n",
      "Iteration 3532: loss 0.0009054336114786565\n",
      "Iteration 3533: loss 0.0009054295951500535\n",
      "Iteration 3534: loss 0.0009054301772266626\n",
      "Iteration 3535: loss 0.0009054281399585307\n",
      "Iteration 3536: loss 0.0009054320398718119\n",
      "Iteration 3537: loss 0.0009054339025169611\n",
      "Iteration 3538: loss 0.000905435299500823\n",
      "Iteration 3539: loss 0.0009054346010088921\n",
      "Iteration 3540: loss 0.0009054314577952027\n",
      "Iteration 3541: loss 0.0009054327383637428\n",
      "Iteration 3542: loss 0.0009054365800693631\n",
      "Iteration 3543: loss 0.0009054294787347317\n",
      "Iteration 3544: loss 0.0009054335532709956\n",
      "Iteration 3545: loss 0.0009054334368556738\n",
      "Iteration 3546: loss 0.0009054335532709956\n",
      "Iteration 3547: loss 0.0009054334368556738\n",
      "Iteration 3548: loss 0.0009054330294020474\n",
      "Iteration 3549: loss 0.0009054297697730362\n",
      "Iteration 3550: loss 0.0009054331458173692\n",
      "Iteration 3551: loss 0.0009054340189322829\n",
      "Iteration 3552: loss 0.0009054332040250301\n",
      "Iteration 3553: loss 0.0009054363472387195\n",
      "Iteration 3554: loss 0.0009054351830855012\n",
      "Iteration 3555: loss 0.0009054315742105246\n",
      "Iteration 3556: loss 0.0009054340189322829\n",
      "Iteration 3557: loss 0.0009054337278939784\n",
      "Iteration 3558: loss 0.0009054338443093002\n",
      "Iteration 3559: loss 0.000905432621948421\n",
      "Iteration 3560: loss 0.0009054337861016393\n",
      "Iteration 3561: loss 0.0009054315742105246\n",
      "Iteration 3562: loss 0.0009054312249645591\n",
      "Iteration 3563: loss 0.0009054343681782484\n",
      "Iteration 3564: loss 0.0009054334950633347\n",
      "Iteration 3565: loss 0.0009054365800693631\n",
      "Iteration 3566: loss 0.0009054349502548575\n",
      "Iteration 3567: loss 0.0009054344845935702\n",
      "Iteration 3568: loss 0.0009054362308233976\n",
      "Iteration 3569: loss 0.0009054334368556738\n",
      "Iteration 3570: loss 0.0009054347174242139\n",
      "Iteration 3571: loss 0.0009054337861016393\n",
      "Iteration 3572: loss 0.0009054347174242139\n",
      "Iteration 3573: loss 0.0009054354741238058\n",
      "Iteration 3574: loss 0.0009054358815774322\n",
      "Iteration 3575: loss 0.0009054321562871337\n",
      "Iteration 3576: loss 0.0009054285474121571\n",
      "Iteration 3577: loss 0.0009054356487467885\n",
      "Iteration 3578: loss 0.0009054364636540413\n",
      "Iteration 3579: loss 0.0009054355323314667\n",
      "Iteration 3580: loss 0.0009054379188455641\n",
      "Iteration 3581: loss 0.0009054371039383113\n",
      "Iteration 3582: loss 0.0009054326801560819\n",
      "Iteration 3583: loss 0.0009054339025169611\n",
      "Iteration 3584: loss 0.000905435997992754\n",
      "Iteration 3585: loss 0.0009054295951500535\n",
      "Iteration 3586: loss 0.000905430584680289\n",
      "Iteration 3587: loss 0.0009054307593032718\n",
      "Iteration 3588: loss 0.0009054362308233976\n",
      "Iteration 3589: loss 0.0009054327383637428\n",
      "Iteration 3590: loss 0.000905433320440352\n",
      "Iteration 3591: loss 0.0009054309921339154\n",
      "Iteration 3592: loss 0.0009054327965714037\n",
      "Iteration 3593: loss 0.0009054294205270708\n",
      "Iteration 3594: loss 0.0009054365218617022\n",
      "Iteration 3595: loss 0.0009054354159161448\n",
      "Iteration 3596: loss 0.000905436638277024\n",
      "Iteration 3597: loss 0.0009054340189322829\n",
      "Iteration 3598: loss 0.0009054355323314667\n",
      "Iteration 3599: loss 0.0009054339025169611\n",
      "Iteration 3600: loss 0.0009054376278072596\n",
      "Iteration 3601: loss 0.0009054339025169611\n",
      "Iteration 3602: loss 0.000905435299500823\n",
      "Iteration 3603: loss 0.000905433320440352\n",
      "Iteration 3604: loss 0.0009054311667568982\n",
      "Iteration 3605: loss 0.0009054322727024555\n",
      "Iteration 3606: loss 0.0009054351830855012\n",
      "Iteration 3607: loss 0.0009054350666701794\n",
      "Iteration 3608: loss 0.0009054307593032718\n",
      "Iteration 3609: loss 0.0009054288966581225\n",
      "Iteration 3610: loss 0.0009054321562871337\n",
      "Iteration 3611: loss 0.0009054362890310585\n",
      "Iteration 3612: loss 0.0009054340189322829\n",
      "Iteration 3613: loss 0.0009054369293153286\n",
      "Iteration 3614: loss 0.0009054362308233976\n",
      "Iteration 3615: loss 0.0009054361144080758\n",
      "Iteration 3616: loss 0.0009054355323314667\n",
      "Iteration 3617: loss 0.0009054347174242139\n",
      "Iteration 3618: loss 0.0009054352412931621\n",
      "Iteration 3619: loss 0.0009054315160028636\n",
      "Iteration 3620: loss 0.0009054328547790647\n",
      "Iteration 3621: loss 0.0009054357069544494\n",
      "Iteration 3622: loss 0.0009054346010088921\n",
      "Iteration 3623: loss 0.0009054352412931621\n",
      "Iteration 3624: loss 0.0009054297115653753\n",
      "Iteration 3625: loss 0.0009054291876964271\n",
      "Iteration 3626: loss 0.0009054294787347317\n",
      "Iteration 3627: loss 0.0009054336696863174\n",
      "Iteration 3628: loss 0.0009054330876097083\n",
      "Iteration 3629: loss 0.0009054378606379032\n",
      "Iteration 3630: loss 0.0009054380934685469\n",
      "Iteration 3631: loss 0.0009054332040250301\n",
      "Iteration 3632: loss 0.0009054320980794728\n",
      "Iteration 3633: loss 0.0009054340189322829\n",
      "Iteration 3634: loss 0.0009054354159161448\n",
      "Iteration 3635: loss 0.0009054330876097083\n",
      "Iteration 3636: loss 0.0009054320980794728\n",
      "Iteration 3637: loss 0.0009054339025169611\n",
      "Iteration 3638: loss 0.000905433262232691\n",
      "Iteration 3639: loss 0.0009054341353476048\n",
      "Iteration 3640: loss 0.0009054347174242139\n",
      "Iteration 3641: loss 0.0009054328547790647\n",
      "Iteration 3642: loss 0.0009054302936419845\n",
      "Iteration 3643: loss 0.0009054295951500535\n",
      "Iteration 3644: loss 0.0009054291294887662\n",
      "Iteration 3645: loss 0.0009054343681782484\n",
      "Iteration 3646: loss 0.0009054328547790647\n",
      "Iteration 3647: loss 0.0009054348338395357\n",
      "Iteration 3648: loss 0.0009054309339262545\n",
      "Iteration 3649: loss 0.0009054354159161448\n",
      "Iteration 3650: loss 0.000905435997992754\n",
      "Iteration 3651: loss 0.0009054325637407601\n",
      "Iteration 3652: loss 0.0009054352412931621\n",
      "Iteration 3653: loss 0.0009054340189322829\n",
      "Iteration 3654: loss 0.0009054326801560819\n",
      "Iteration 3655: loss 0.0009054350666701794\n",
      "Iteration 3656: loss 0.0009054333786480129\n",
      "Iteration 3657: loss 0.0009054320980794728\n",
      "Iteration 3658: loss 0.0009054340189322829\n",
      "Iteration 3659: loss 0.0009054325055330992\n",
      "Iteration 3660: loss 0.0009054326801560819\n",
      "Iteration 3661: loss 0.0009054307593032718\n",
      "Iteration 3662: loss 0.0009054329711943865\n",
      "Iteration 3663: loss 0.0009054307010956109\n",
      "Iteration 3664: loss 0.0009054308175109327\n",
      "Iteration 3665: loss 0.0009054301772266626\n",
      "Iteration 3666: loss 0.0009054353577084839\n",
      "Iteration 3667: loss 0.0009054344845935702\n",
      "Iteration 3668: loss 0.000905433320440352\n",
      "Iteration 3669: loss 0.0009054335532709956\n",
      "Iteration 3670: loss 0.0009054337278939784\n",
      "Iteration 3671: loss 0.0009054361144080758\n",
      "Iteration 3672: loss 0.000905437977053225\n",
      "Iteration 3673: loss 0.0009054350084625185\n",
      "Iteration 3674: loss 0.0009054328547790647\n",
      "Iteration 3675: loss 0.0009054328547790647\n",
      "Iteration 3676: loss 0.0009054325055330992\n",
      "Iteration 3677: loss 0.0009054340189322829\n",
      "Iteration 3678: loss 0.0009054336696863174\n",
      "Iteration 3679: loss 0.0009054314577952027\n",
      "Iteration 3680: loss 0.0009054321562871337\n",
      "Iteration 3681: loss 0.0009054325055330992\n",
      "Iteration 3682: loss 0.0009054348920471966\n",
      "Iteration 3683: loss 0.0009054329711943865\n",
      "Iteration 3684: loss 0.0009054315742105246\n",
      "Iteration 3685: loss 0.0009054308175109327\n",
      "Iteration 3686: loss 0.0009054315160028636\n",
      "Iteration 3687: loss 0.0009054324473254383\n",
      "Iteration 3688: loss 0.0009054344845935702\n",
      "Iteration 3689: loss 0.0009054334950633347\n",
      "Iteration 3690: loss 0.0009054343681782484\n",
      "Iteration 3691: loss 0.0009054304682649672\n",
      "Iteration 3692: loss 0.0009054344845935702\n",
      "Iteration 3693: loss 0.0009054357651621103\n",
      "Iteration 3694: loss 0.0009054357651621103\n",
      "Iteration 3695: loss 0.0009054341353476048\n",
      "Iteration 3696: loss 0.0009054335532709956\n",
      "Iteration 3697: loss 0.0009054324473254383\n",
      "Iteration 3698: loss 0.0009054322144947946\n",
      "Iteration 3699: loss 0.0009054332040250301\n",
      "Iteration 3700: loss 0.0009054323891177773\n",
      "Iteration 3701: loss 0.0009054334368556738\n",
      "Iteration 3702: loss 0.0009054356487467885\n",
      "Iteration 3703: loss 0.0009054357069544494\n",
      "Iteration 3704: loss 0.0009054350084625185\n",
      "Iteration 3705: loss 0.0009054368711076677\n",
      "Iteration 3706: loss 0.0009054332040250301\n",
      "Iteration 3707: loss 0.0009054302354343235\n",
      "Iteration 3708: loss 0.0009054302354343235\n",
      "Iteration 3709: loss 0.0009054309339262545\n",
      "Iteration 3710: loss 0.0009054362308233976\n",
      "Iteration 3711: loss 0.0009054358233697712\n",
      "Iteration 3712: loss 0.0009054340189322829\n",
      "Iteration 3713: loss 0.0009054355323314667\n",
      "Iteration 3714: loss 0.0009054357651621103\n",
      "Iteration 3715: loss 0.0009054353577084839\n",
      "Iteration 3716: loss 0.0009054317488335073\n",
      "Iteration 3717: loss 0.0009054305264726281\n",
      "Iteration 3718: loss 0.0009054301190190017\n",
      "Iteration 3719: loss 0.0009054316906258464\n",
      "Iteration 3720: loss 0.0009054361144080758\n",
      "Iteration 3721: loss 0.0009054330876097083\n",
      "Iteration 3722: loss 0.0009054327383637428\n",
      "Iteration 3723: loss 0.0009054322727024555\n",
      "Iteration 3724: loss 0.0009054317488335073\n",
      "Iteration 3725: loss 0.0009054333786480129\n",
      "Iteration 3726: loss 0.0009054349502548575\n",
      "Iteration 3727: loss 0.000905435299500823\n",
      "Iteration 3728: loss 0.0009054330294020474\n",
      "Iteration 3729: loss 0.0009054332040250301\n",
      "Iteration 3730: loss 0.0009054336114786565\n",
      "Iteration 3731: loss 0.0009054340189322829\n",
      "Iteration 3732: loss 0.0009054323891177773\n",
      "Iteration 3733: loss 0.0009054335532709956\n",
      "Iteration 3734: loss 0.0009054333786480129\n",
      "Iteration 3735: loss 0.0009054346010088921\n",
      "Iteration 3736: loss 0.0009054365800693631\n",
      "Iteration 3737: loss 0.0009054323891177773\n",
      "Iteration 3738: loss 0.0009054339025169611\n",
      "Iteration 3739: loss 0.0009054339025169611\n",
      "Iteration 3740: loss 0.0009054326801560819\n",
      "Iteration 3741: loss 0.0009054328547790647\n",
      "Iteration 3742: loss 0.0009054336696863174\n",
      "Iteration 3743: loss 0.0009054328547790647\n",
      "Iteration 3744: loss 0.0009054336696863174\n",
      "Iteration 3745: loss 0.000905433960724622\n",
      "Iteration 3746: loss 0.0009054336696863174\n",
      "Iteration 3747: loss 0.0009054344845935702\n",
      "Iteration 3748: loss 0.0009054317488335073\n",
      "Iteration 3749: loss 0.000905433262232691\n",
      "Iteration 3750: loss 0.0009054330876097083\n",
      "Iteration 3751: loss 0.0009054345428012311\n",
      "Iteration 3752: loss 0.0009054336114786565\n",
      "Iteration 3753: loss 0.000905433262232691\n",
      "Iteration 3754: loss 0.0009054313413798809\n",
      "Iteration 3755: loss 0.0009054311085492373\n",
      "Iteration 3756: loss 0.0009054327965714037\n",
      "Iteration 3757: loss 0.0009054339025169611\n",
      "Iteration 3758: loss 0.0009054371039383113\n",
      "Iteration 3759: loss 0.0009054340189322829\n",
      "Iteration 3760: loss 0.0009054312249645591\n",
      "Iteration 3761: loss 0.0009054327383637428\n",
      "Iteration 3762: loss 0.0009054344263859093\n",
      "Iteration 3763: loss 0.0009054320398718119\n",
      "Iteration 3764: loss 0.0009054314577952027\n",
      "Iteration 3765: loss 0.0009054330876097083\n",
      "Iteration 3766: loss 0.0009054341935552657\n",
      "Iteration 3767: loss 0.0009054313413798809\n",
      "Iteration 3768: loss 0.0009054340189322829\n",
      "Iteration 3769: loss 0.0009054345428012311\n",
      "Iteration 3770: loss 0.0009054354159161448\n",
      "Iteration 3771: loss 0.0009054342517629266\n",
      "Iteration 3772: loss 0.0009054329711943865\n",
      "Iteration 3773: loss 0.0009054316324181855\n",
      "Iteration 3774: loss 0.0009054311085492373\n",
      "Iteration 3775: loss 0.0009054321562871337\n",
      "Iteration 3776: loss 0.0009054362308233976\n",
      "Iteration 3777: loss 0.0009054372785612941\n",
      "Iteration 3778: loss 0.0009054337861016393\n",
      "Iteration 3779: loss 0.000905433960724622\n",
      "Iteration 3780: loss 0.0009054330876097083\n",
      "Iteration 3781: loss 0.0009054330876097083\n",
      "Iteration 3782: loss 0.0009054330876097083\n",
      "Iteration 3783: loss 0.0009054325055330992\n",
      "Iteration 3784: loss 0.0009054326801560819\n",
      "Iteration 3785: loss 0.000905433262232691\n",
      "Iteration 3786: loss 0.0009054325055330992\n",
      "Iteration 3787: loss 0.0009054326801560819\n",
      "Iteration 3788: loss 0.0009054357651621103\n",
      "Iteration 3789: loss 0.0009054368711076677\n",
      "Iteration 3790: loss 0.0009054325055330992\n",
      "Iteration 3791: loss 0.0009054321562871337\n",
      "Iteration 3792: loss 0.0009054313413798809\n",
      "Iteration 3793: loss 0.0009054310503415763\n",
      "Iteration 3794: loss 0.0009054359397850931\n",
      "Iteration 3795: loss 0.0009054358233697712\n",
      "Iteration 3796: loss 0.0009054323891177773\n",
      "Iteration 3797: loss 0.0009054295951500535\n",
      "Iteration 3798: loss 0.0009054295951500535\n",
      "Iteration 3799: loss 0.0009054336696863174\n",
      "Iteration 3800: loss 0.0009054335532709956\n",
      "Iteration 3801: loss 0.0009054363472387195\n",
      "Iteration 3802: loss 0.0009054354159161448\n",
      "Iteration 3803: loss 0.0009054377442225814\n",
      "Iteration 3804: loss 0.000905436638277024\n",
      "Iteration 3805: loss 0.0009054360562004149\n",
      "Iteration 3806: loss 0.0009054360562004149\n",
      "Iteration 3807: loss 0.0009054360562004149\n",
      "Iteration 3808: loss 0.0009054355323314667\n",
      "Iteration 3809: loss 0.0009054348338395357\n",
      "Iteration 3810: loss 0.0009054311667568982\n",
      "Iteration 3811: loss 0.0009054328547790647\n",
      "Iteration 3812: loss 0.0009054323309101164\n",
      "Iteration 3813: loss 0.0009054330876097083\n",
      "Iteration 3814: loss 0.000905435997992754\n",
      "Iteration 3815: loss 0.0009054341353476048\n",
      "Iteration 3816: loss 0.0009054347174242139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3817: loss 0.0009054331458173692\n",
      "Iteration 3818: loss 0.0009054322727024555\n",
      "Iteration 3819: loss 0.0009054343681782484\n",
      "Iteration 3820: loss 0.0009054337861016393\n",
      "Iteration 3821: loss 0.0009054340189322829\n",
      "Iteration 3822: loss 0.000905433320440352\n",
      "Iteration 3823: loss 0.0009054330876097083\n",
      "Iteration 3824: loss 0.0009054328547790647\n",
      "Iteration 3825: loss 0.0009054323891177773\n",
      "Iteration 3826: loss 0.0009054338443093002\n",
      "Iteration 3827: loss 0.0009054338443093002\n",
      "Iteration 3828: loss 0.0009054338443093002\n",
      "Iteration 3829: loss 0.000905433262232691\n",
      "Iteration 3830: loss 0.0009054334368556738\n",
      "Iteration 3831: loss 0.0009054351830855012\n",
      "Iteration 3832: loss 0.0009054344845935702\n",
      "Iteration 3833: loss 0.0009054329711943865\n",
      "Iteration 3834: loss 0.0009054331458173692\n",
      "Iteration 3835: loss 0.0009054325055330992\n",
      "Iteration 3836: loss 0.0009054325055330992\n",
      "Iteration 3837: loss 0.0009054326801560819\n",
      "Iteration 3838: loss 0.0009054341353476048\n",
      "Iteration 3839: loss 0.000905433320440352\n",
      "Iteration 3840: loss 0.0009054334368556738\n",
      "Iteration 3841: loss 0.0009054334368556738\n",
      "Iteration 3842: loss 0.0009054336114786565\n",
      "Iteration 3843: loss 0.0009054323891177773\n",
      "Iteration 3844: loss 0.0009054332040250301\n",
      "Iteration 3845: loss 0.0009054342517629266\n",
      "Iteration 3846: loss 0.0009054326801560819\n",
      "Iteration 3847: loss 0.0009054354159161448\n",
      "Iteration 3848: loss 0.0009054358233697712\n",
      "Iteration 3849: loss 0.0009054355323314667\n",
      "Iteration 3850: loss 0.0009054327383637428\n",
      "Iteration 3851: loss 0.0009054320398718119\n",
      "Iteration 3852: loss 0.0009054327383637428\n",
      "Iteration 3853: loss 0.0009054327383637428\n",
      "Iteration 3854: loss 0.0009054337861016393\n",
      "Iteration 3855: loss 0.0009054343681782484\n",
      "Iteration 3856: loss 0.0009054346010088921\n",
      "Iteration 3857: loss 0.0009054346010088921\n",
      "Iteration 3858: loss 0.0009054330876097083\n",
      "Iteration 3859: loss 0.00090543192345649\n",
      "Iteration 3860: loss 0.000905432621948421\n",
      "Iteration 3861: loss 0.000905432621948421\n",
      "Iteration 3862: loss 0.000905432621948421\n",
      "Iteration 3863: loss 0.0009054365800693631\n",
      "Iteration 3864: loss 0.0009054342517629266\n",
      "Iteration 3865: loss 0.0009054357651621103\n",
      "Iteration 3866: loss 0.0009054342517629266\n",
      "Iteration 3867: loss 0.000905434659216553\n",
      "Iteration 3868: loss 0.0009054355323314667\n",
      "Iteration 3869: loss 0.0009054356487467885\n",
      "Iteration 3870: loss 0.0009054356487467885\n",
      "Iteration 3871: loss 0.0009054348338395357\n",
      "Iteration 3872: loss 0.0009054345428012311\n",
      "Iteration 3873: loss 0.0009054338443093002\n",
      "Iteration 3874: loss 0.0009054338443093002\n",
      "Iteration 3875: loss 0.0009054338443093002\n",
      "Iteration 3876: loss 0.0009054338443093002\n",
      "Iteration 3877: loss 0.0009054340189322829\n",
      "Iteration 3878: loss 0.0009054316906258464\n",
      "Iteration 3879: loss 0.0009054344263859093\n",
      "Iteration 3880: loss 0.0009054345428012311\n",
      "Iteration 3881: loss 0.0009054336696863174\n",
      "Iteration 3882: loss 0.0009054336696863174\n",
      "Iteration 3883: loss 0.000905433320440352\n",
      "Iteration 3884: loss 0.00090543192345649\n",
      "Iteration 3885: loss 0.0009054308175109327\n",
      "Iteration 3886: loss 0.0009054322727024555\n",
      "Iteration 3887: loss 0.0009054323891177773\n",
      "Iteration 3888: loss 0.0009054338443093002\n",
      "Iteration 3889: loss 0.000905435299500823\n",
      "Iteration 3890: loss 0.0009054340189322829\n",
      "Iteration 3891: loss 0.0009054346010088921\n",
      "Iteration 3892: loss 0.0009054346010088921\n",
      "Iteration 3893: loss 0.0009054340771399438\n",
      "Iteration 3894: loss 0.0009054340771399438\n",
      "Iteration 3895: loss 0.0009054332040250301\n",
      "Iteration 3896: loss 0.0009054309921339154\n",
      "Iteration 3897: loss 0.0009054327965714037\n",
      "Iteration 3898: loss 0.0009054315742105246\n",
      "Iteration 3899: loss 0.0009054355323314667\n",
      "Iteration 3900: loss 0.0009054358815774322\n",
      "Iteration 3901: loss 0.0009054362308233976\n",
      "Iteration 3902: loss 0.0009054371621459723\n",
      "Iteration 3903: loss 0.0009054334368556738\n",
      "Iteration 3904: loss 0.0009054335532709956\n",
      "Iteration 3905: loss 0.0009054335532709956\n",
      "Iteration 3906: loss 0.0009054335532709956\n",
      "Iteration 3907: loss 0.0009054323891177773\n",
      "Iteration 3908: loss 0.0009054350666701794\n",
      "Iteration 3909: loss 0.0009054346010088921\n",
      "Iteration 3910: loss 0.0009054346010088921\n",
      "Iteration 3911: loss 0.0009054346010088921\n",
      "Iteration 3912: loss 0.0009054346010088921\n",
      "Iteration 3913: loss 0.0009054346010088921\n",
      "Iteration 3914: loss 0.0009054346010088921\n",
      "Iteration 3915: loss 0.0009054346010088921\n",
      "Iteration 3916: loss 0.0009054346010088921\n",
      "Iteration 3917: loss 0.0009054346010088921\n",
      "Iteration 3918: loss 0.0009054346010088921\n",
      "Iteration 3919: loss 0.0009054346010088921\n",
      "Iteration 3920: loss 0.0009054346010088921\n",
      "Iteration 3921: loss 0.0009054336696863174\n",
      "Iteration 3922: loss 0.0009054323891177773\n",
      "Iteration 3923: loss 0.0009054323891177773\n",
      "Iteration 3924: loss 0.0009054340771399438\n",
      "Iteration 3925: loss 0.0009054320980794728\n",
      "Iteration 3926: loss 0.0009054321562871337\n",
      "Iteration 3927: loss 0.0009054315742105246\n",
      "Iteration 3928: loss 0.0009054339025169611\n",
      "Iteration 3929: loss 0.0009054330294020474\n",
      "Iteration 3930: loss 0.0009054330294020474\n",
      "Iteration 3931: loss 0.0009054330294020474\n",
      "Iteration 3932: loss 0.0009054330294020474\n",
      "Iteration 3933: loss 0.0009054330294020474\n",
      "Iteration 3934: loss 0.0009054330294020474\n",
      "Iteration 3935: loss 0.0009054330294020474\n",
      "Iteration 3936: loss 0.0009054330294020474\n",
      "Iteration 3937: loss 0.0009054330294020474\n",
      "Iteration 3938: loss 0.0009054330294020474\n",
      "Iteration 3939: loss 0.0009054330294020474\n",
      "Iteration 3940: loss 0.0009054330294020474\n",
      "Iteration 3941: loss 0.0009054330294020474\n",
      "Iteration 3942: loss 0.0009054330294020474\n",
      "Iteration 3943: loss 0.0009054330294020474\n",
      "Iteration 3944: loss 0.0009054330294020474\n",
      "Iteration 3945: loss 0.0009054330294020474\n",
      "Iteration 3946: loss 0.0009054330294020474\n",
      "Iteration 3947: loss 0.0009054330294020474\n",
      "Iteration 3948: loss 0.0009054330294020474\n",
      "Iteration 3949: loss 0.0009054330294020474\n",
      "Iteration 3950: loss 0.0009054330294020474\n",
      "Iteration 3951: loss 0.0009054315742105246\n",
      "Iteration 3952: loss 0.0009054330294020474\n",
      "Iteration 3953: loss 0.0009054330294020474\n",
      "Iteration 3954: loss 0.0009054330294020474\n",
      "Iteration 3955: loss 0.0009054330294020474\n",
      "Iteration 3956: loss 0.0009054330294020474\n",
      "Iteration 3957: loss 0.0009054330294020474\n",
      "Iteration 3958: loss 0.0009054330294020474\n",
      "Iteration 3959: loss 0.0009054330294020474\n",
      "Iteration 3960: loss 0.0009054330294020474\n",
      "Iteration 3961: loss 0.0009054330294020474\n",
      "Iteration 3962: loss 0.0009054330294020474\n",
      "Iteration 3963: loss 0.0009054315742105246\n",
      "Iteration 3964: loss 0.000905433262232691\n",
      "Iteration 3965: loss 0.0009054285474121571\n",
      "Iteration 3966: loss 0.0009054342517629266\n",
      "Iteration 3967: loss 0.0009054329129867256\n",
      "Iteration 3968: loss 0.0009054329129867256\n",
      "Iteration 3969: loss 0.0009054329129867256\n",
      "Iteration 3970: loss 0.0009054329129867256\n",
      "Iteration 3971: loss 0.0009054330294020474\n",
      "Iteration 3972: loss 0.0009054330294020474\n",
      "Iteration 3973: loss 0.0009054330294020474\n",
      "Iteration 3974: loss 0.0009054330294020474\n",
      "Iteration 3975: loss 0.0009054329129867256\n",
      "Iteration 3976: loss 0.0009054330294020474\n",
      "Iteration 3977: loss 0.0009054330294020474\n",
      "Iteration 3978: loss 0.0009054330294020474\n",
      "Iteration 3979: loss 0.0009054330294020474\n",
      "Iteration 3980: loss 0.0009054330294020474\n",
      "Iteration 3981: loss 0.0009054329129867256\n",
      "Iteration 3982: loss 0.0009054324473254383\n",
      "Iteration 3983: loss 0.0009054330876097083\n",
      "Iteration 3984: loss 0.0009054340189322829\n",
      "Iteration 3985: loss 0.0009054347174242139\n",
      "Iteration 3986: loss 0.0009054348338395357\n",
      "Iteration 3987: loss 0.0009054348338395357\n",
      "Iteration 3988: loss 0.0009054340189322829\n",
      "Iteration 3989: loss 0.0009054347174242139\n",
      "Iteration 3990: loss 0.0009054348338395357\n",
      "Iteration 3991: loss 0.0009054340189322829\n",
      "Iteration 3992: loss 0.0009054351830855012\n",
      "Iteration 3993: loss 0.0009054351830855012\n",
      "Iteration 3994: loss 0.0009054344263859093\n",
      "Iteration 3995: loss 0.0009054344263859093\n",
      "Iteration 3996: loss 0.0009054344263859093\n",
      "Iteration 3997: loss 0.0009054344263859093\n",
      "Iteration 3998: loss 0.0009054344263859093\n",
      "Iteration 3999: loss 0.0009054344263859093\n",
      "Iteration 4000: loss 0.0009054344263859093\n",
      "Iteration 4001: loss 0.0009054344263859093\n",
      "Iteration 4002: loss 0.0009054344263859093\n",
      "Iteration 4003: loss 0.0009054344263859093\n",
      "Iteration 4004: loss 0.0009054344263859093\n",
      "Iteration 4005: loss 0.0009054344263859093\n",
      "Iteration 4006: loss 0.0009054344263859093\n",
      "Iteration 4007: loss 0.0009054344263859093\n",
      "Iteration 4008: loss 0.0009054344263859093\n",
      "Iteration 4009: loss 0.0009054344263859093\n",
      "Iteration 4010: loss 0.0009054344263859093\n",
      "Iteration 4011: loss 0.0009054344263859093\n",
      "Iteration 4012: loss 0.0009054344263859093\n",
      "Iteration 4013: loss 0.0009054344263859093\n",
      "Iteration 4014: loss 0.0009054344263859093\n",
      "Iteration 4015: loss 0.0009054344263859093\n",
      "Iteration 4016: loss 0.0009054344263859093\n",
      "Iteration 4017: loss 0.0009054344263859093\n",
      "Iteration 4018: loss 0.0009054344263859093\n",
      "Iteration 4019: loss 0.0009054344263859093\n",
      "Iteration 4020: loss 0.0009054344263859093\n",
      "Iteration 4021: loss 0.0009054344263859093\n",
      "Iteration 4022: loss 0.0009054344263859093\n",
      "Iteration 4023: loss 0.0009054344263859093\n",
      "Iteration 4024: loss 0.0009054344263859093\n",
      "Iteration 4025: loss 0.0009054344263859093\n",
      "Iteration 4026: loss 0.0009054344263859093\n",
      "Iteration 4027: loss 0.0009054344263859093\n",
      "Iteration 4028: loss 0.0009054344263859093\n",
      "Iteration 4029: loss 0.0009054344263859093\n",
      "Iteration 4030: loss 0.0009054344263859093\n",
      "Iteration 4031: loss 0.0009054344263859093\n",
      "Iteration 4032: loss 0.000905435299500823\n",
      "Iteration 4033: loss 0.0009054344263859093\n",
      "Iteration 4034: loss 0.0009054344263859093\n",
      "Iteration 4035: loss 0.0009054344263859093\n",
      "Iteration 4036: loss 0.0009054344263859093\n",
      "Iteration 4037: loss 0.0009054344263859093\n",
      "Iteration 4038: loss 0.0009054344263859093\n",
      "Iteration 4039: loss 0.0009054344263859093\n",
      "Iteration 4040: loss 0.0009054344263859093\n",
      "Iteration 4041: loss 0.0009054344263859093\n",
      "Iteration 4042: loss 0.0009054344263859093\n",
      "Iteration 4043: loss 0.0009054344263859093\n",
      "Iteration 4044: loss 0.000905435299500823\n",
      "Iteration 4045: loss 0.0009054344263859093\n",
      "Iteration 4046: loss 0.0009054344263859093\n",
      "Iteration 4047: loss 0.0009054344263859093\n",
      "Iteration 4048: loss 0.0009054344263859093\n",
      "Iteration 4049: loss 0.0009054344263859093\n",
      "Iteration 4050: loss 0.0009054344263859093\n",
      "Iteration 4051: loss 0.0009054344263859093\n",
      "Iteration 4052: loss 0.0009054344263859093\n",
      "Iteration 4053: loss 0.0009054344263859093\n",
      "Iteration 4054: loss 0.0009054344263859093\n",
      "Iteration 4055: loss 0.000905435299500823\n",
      "Iteration 4056: loss 0.0009054344263859093\n",
      "Iteration 4057: loss 0.0009054344263859093\n",
      "Iteration 4058: loss 0.0009054344263859093\n",
      "Iteration 4059: loss 0.0009054344263859093\n",
      "Iteration 4060: loss 0.0009054344263859093\n",
      "Iteration 4061: loss 0.0009054344263859093\n",
      "Iteration 4062: loss 0.0009054344263859093\n",
      "Iteration 4063: loss 0.0009054344263859093\n",
      "Iteration 4064: loss 0.0009054344263859093\n",
      "Iteration 4065: loss 0.0009054344263859093\n",
      "Iteration 4066: loss 0.0009054344263859093\n",
      "Iteration 4067: loss 0.000905435299500823\n",
      "Iteration 4068: loss 0.0009054344263859093\n",
      "Iteration 4069: loss 0.0009054344263859093\n",
      "Iteration 4070: loss 0.0009054344263859093\n",
      "Iteration 4071: loss 0.0009054344263859093\n",
      "Iteration 4072: loss 0.0009054344263859093\n",
      "Iteration 4073: loss 0.0009054344263859093\n",
      "Iteration 4074: loss 0.0009054344263859093\n",
      "Iteration 4075: loss 0.0009054344263859093\n",
      "Iteration 4076: loss 0.0009054344263859093\n",
      "Iteration 4077: loss 0.0009054344263859093\n",
      "Iteration 4078: loss 0.000905435299500823\n",
      "Iteration 4079: loss 0.0009054338443093002\n",
      "Iteration 4080: loss 0.0009054332040250301\n",
      "Iteration 4081: loss 0.0009054347174242139\n",
      "Iteration 4082: loss 0.000905432621948421\n",
      "Iteration 4083: loss 0.000905433320440352\n",
      "Iteration 4084: loss 0.000905433320440352\n",
      "Iteration 4085: loss 0.000905433320440352\n",
      "Iteration 4086: loss 0.000905433320440352\n",
      "Iteration 4087: loss 0.000905433320440352\n",
      "Iteration 4088: loss 0.000905433320440352\n",
      "Iteration 4089: loss 0.000905433320440352\n",
      "Iteration 4090: loss 0.000905433320440352\n",
      "Iteration 4091: loss 0.000905433320440352\n",
      "Iteration 4092: loss 0.000905433320440352\n",
      "Iteration 4093: loss 0.000905433320440352\n",
      "Iteration 4094: loss 0.0009054330876097083\n",
      "Iteration 4095: loss 0.000905433320440352\n",
      "Iteration 4096: loss 0.000905433320440352\n",
      "Iteration 4097: loss 0.0009054330876097083\n",
      "Iteration 4098: loss 0.000905433320440352\n",
      "Iteration 4099: loss 0.000905433320440352\n",
      "Iteration 4100: loss 0.0009054330876097083\n",
      "Iteration 4101: loss 0.000905433320440352\n",
      "Iteration 4102: loss 0.000905433320440352\n",
      "Iteration 4103: loss 0.0009054330876097083\n",
      "Iteration 4104: loss 0.000905433320440352\n",
      "Iteration 4105: loss 0.000905433320440352\n",
      "Iteration 4106: loss 0.0009054330876097083\n",
      "Iteration 4107: loss 0.000905433320440352\n",
      "Iteration 4108: loss 0.000905433320440352\n",
      "Iteration 4109: loss 0.0009054330876097083\n",
      "Iteration 4110: loss 0.000905433320440352\n",
      "Iteration 4111: loss 0.000905433320440352\n",
      "Iteration 4112: loss 0.0009054330876097083\n",
      "Iteration 4113: loss 0.000905433320440352\n",
      "Iteration 4114: loss 0.000905433320440352\n",
      "Iteration 4115: loss 0.0009054330876097083\n",
      "Iteration 4116: loss 0.000905433320440352\n",
      "Iteration 4117: loss 0.000905433320440352\n",
      "Iteration 4118: loss 0.0009054330876097083\n",
      "Iteration 4119: loss 0.000905433320440352\n",
      "Iteration 4120: loss 0.000905433320440352\n",
      "Iteration 4121: loss 0.0009054330876097083\n",
      "Iteration 4122: loss 0.000905433320440352\n",
      "Iteration 4123: loss 0.000905433320440352\n",
      "Iteration 4124: loss 0.0009054330876097083\n",
      "Iteration 4125: loss 0.000905433320440352\n",
      "Iteration 4126: loss 0.000905433320440352\n",
      "Iteration 4127: loss 0.0009054330876097083\n",
      "Iteration 4128: loss 0.000905433320440352\n",
      "Iteration 4129: loss 0.000905433320440352\n",
      "Iteration 4130: loss 0.0009054330876097083\n",
      "Iteration 4131: loss 0.000905433320440352\n",
      "Iteration 4132: loss 0.000905433320440352\n",
      "Iteration 4133: loss 0.0009054330876097083\n",
      "Iteration 4134: loss 0.000905433320440352\n",
      "Iteration 4135: loss 0.000905433320440352\n",
      "Iteration 4136: loss 0.0009054330876097083\n",
      "Iteration 4137: loss 0.000905433320440352\n",
      "Iteration 4138: loss 0.000905433320440352\n",
      "Iteration 4139: loss 0.0009054330876097083\n",
      "Iteration 4140: loss 0.000905433320440352\n",
      "Iteration 4141: loss 0.000905433320440352\n",
      "Iteration 4142: loss 0.0009054330876097083\n",
      "Iteration 4143: loss 0.000905433320440352\n",
      "Iteration 4144: loss 0.000905433320440352\n",
      "Iteration 4145: loss 0.0009054330876097083\n",
      "Iteration 4146: loss 0.000905433320440352\n",
      "Iteration 4147: loss 0.000905433320440352\n",
      "Iteration 4148: loss 0.0009054330876097083\n",
      "Iteration 4149: loss 0.000905433320440352\n",
      "Iteration 4150: loss 0.000905433320440352\n",
      "Iteration 4151: loss 0.0009054330876097083\n",
      "Iteration 4152: loss 0.000905433320440352\n",
      "Iteration 4153: loss 0.000905433320440352\n",
      "Iteration 4154: loss 0.0009054330876097083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4155: loss 0.000905433320440352\n",
      "Iteration 4156: loss 0.000905433320440352\n",
      "Iteration 4157: loss 0.0009054330876097083\n",
      "Iteration 4158: loss 0.000905433320440352\n",
      "Iteration 4159: loss 0.000905433320440352\n",
      "Iteration 4160: loss 0.0009054330876097083\n",
      "Iteration 4161: loss 0.000905433320440352\n",
      "Iteration 4162: loss 0.000905433320440352\n",
      "Iteration 4163: loss 0.0009054330876097083\n",
      "Iteration 4164: loss 0.000905433320440352\n",
      "Iteration 4165: loss 0.000905433320440352\n",
      "Iteration 4166: loss 0.0009054330876097083\n",
      "Iteration 4167: loss 0.000905433320440352\n",
      "Iteration 4168: loss 0.000905433320440352\n",
      "Iteration 4169: loss 0.0009054330876097083\n",
      "Iteration 4170: loss 0.000905433320440352\n",
      "Iteration 4171: loss 0.000905433320440352\n",
      "Iteration 4172: loss 0.0009054330876097083\n",
      "Iteration 4173: loss 0.000905433320440352\n",
      "Iteration 4174: loss 0.000905433320440352\n",
      "Iteration 4175: loss 0.0009054330876097083\n",
      "Iteration 4176: loss 0.000905433320440352\n",
      "Iteration 4177: loss 0.000905433320440352\n",
      "Iteration 4178: loss 0.0009054330876097083\n",
      "Iteration 4179: loss 0.000905433320440352\n",
      "Iteration 4180: loss 0.000905433320440352\n",
      "Iteration 4181: loss 0.0009054330876097083\n",
      "Iteration 4182: loss 0.000905433320440352\n",
      "Iteration 4183: loss 0.000905433320440352\n",
      "Iteration 4184: loss 0.0009054330876097083\n",
      "Iteration 4185: loss 0.000905433320440352\n",
      "Iteration 4186: loss 0.000905433320440352\n",
      "Iteration 4187: loss 0.0009054330876097083\n",
      "Iteration 4188: loss 0.000905433320440352\n",
      "Iteration 4189: loss 0.000905433320440352\n",
      "Iteration 4190: loss 0.0009054330876097083\n",
      "Iteration 4191: loss 0.000905433320440352\n",
      "Iteration 4192: loss 0.000905433320440352\n",
      "Iteration 4193: loss 0.0009054330876097083\n",
      "Iteration 4194: loss 0.000905433320440352\n",
      "Iteration 4195: loss 0.000905433320440352\n",
      "Iteration 4196: loss 0.0009054330876097083\n",
      "Iteration 4197: loss 0.000905433320440352\n",
      "Iteration 4198: loss 0.000905433320440352\n",
      "Iteration 4199: loss 0.0009054330876097083\n",
      "Iteration 4200: loss 0.000905433320440352\n",
      "Iteration 4201: loss 0.000905433320440352\n",
      "Iteration 4202: loss 0.0009054330876097083\n",
      "Iteration 4203: loss 0.000905433320440352\n",
      "Iteration 4204: loss 0.000905433320440352\n",
      "Iteration 4205: loss 0.0009054330876097083\n",
      "Iteration 4206: loss 0.000905433320440352\n",
      "Iteration 4207: loss 0.000905433320440352\n",
      "Iteration 4208: loss 0.0009054330876097083\n",
      "Iteration 4209: loss 0.000905433320440352\n",
      "Iteration 4210: loss 0.000905433320440352\n",
      "Iteration 4211: loss 0.0009054330876097083\n",
      "Iteration 4212: loss 0.000905433320440352\n",
      "Iteration 4213: loss 0.000905433320440352\n",
      "Iteration 4214: loss 0.0009054330876097083\n",
      "Iteration 4215: loss 0.000905433320440352\n",
      "Iteration 4216: loss 0.000905433320440352\n",
      "Iteration 4217: loss 0.0009054330876097083\n",
      "Iteration 4218: loss 0.000905433320440352\n",
      "Iteration 4219: loss 0.000905433320440352\n",
      "Iteration 4220: loss 0.0009054330876097083\n",
      "Iteration 4221: loss 0.000905433320440352\n",
      "Iteration 4222: loss 0.000905433320440352\n",
      "Iteration 4223: loss 0.0009054330876097083\n",
      "Iteration 4224: loss 0.000905433320440352\n",
      "Iteration 4225: loss 0.000905433320440352\n",
      "Iteration 4226: loss 0.0009054330876097083\n",
      "Iteration 4227: loss 0.000905433320440352\n",
      "Iteration 4228: loss 0.000905433320440352\n",
      "Iteration 4229: loss 0.0009054330876097083\n",
      "Iteration 4230: loss 0.000905433320440352\n",
      "Iteration 4231: loss 0.000905433320440352\n",
      "Iteration 4232: loss 0.0009054330876097083\n",
      "Iteration 4233: loss 0.000905433320440352\n",
      "Iteration 4234: loss 0.000905433320440352\n",
      "Iteration 4235: loss 0.0009054330876097083\n",
      "Iteration 4236: loss 0.000905433320440352\n",
      "Iteration 4237: loss 0.000905433320440352\n",
      "Iteration 4238: loss 0.0009054330876097083\n",
      "Iteration 4239: loss 0.000905433320440352\n",
      "Iteration 4240: loss 0.000905433320440352\n",
      "Iteration 4241: loss 0.0009054330876097083\n",
      "Iteration 4242: loss 0.000905433320440352\n",
      "Iteration 4243: loss 0.000905433320440352\n",
      "Iteration 4244: loss 0.0009054330876097083\n",
      "Iteration 4245: loss 0.000905433320440352\n",
      "Iteration 4246: loss 0.000905433320440352\n",
      "Iteration 4247: loss 0.0009054330876097083\n",
      "Iteration 4248: loss 0.000905433320440352\n",
      "Iteration 4249: loss 0.000905433320440352\n",
      "Iteration 4250: loss 0.0009054330876097083\n",
      "Iteration 4251: loss 0.000905433320440352\n",
      "Iteration 4252: loss 0.000905433320440352\n",
      "Iteration 4253: loss 0.0009054330876097083\n",
      "Iteration 4254: loss 0.000905433320440352\n",
      "Iteration 4255: loss 0.000905433320440352\n",
      "Iteration 4256: loss 0.0009054330876097083\n",
      "Iteration 4257: loss 0.000905433320440352\n",
      "Iteration 4258: loss 0.000905433320440352\n",
      "Iteration 4259: loss 0.0009054330876097083\n",
      "Iteration 4260: loss 0.000905433320440352\n",
      "Iteration 4261: loss 0.000905433320440352\n",
      "Iteration 4262: loss 0.0009054330876097083\n",
      "Iteration 4263: loss 0.000905433320440352\n",
      "Iteration 4264: loss 0.000905433320440352\n",
      "Iteration 4265: loss 0.0009054330876097083\n",
      "Iteration 4266: loss 0.000905433320440352\n",
      "Iteration 4267: loss 0.000905433320440352\n",
      "Iteration 4268: loss 0.0009054330876097083\n",
      "Iteration 4269: loss 0.000905433320440352\n",
      "Iteration 4270: loss 0.000905433320440352\n",
      "Iteration 4271: loss 0.0009054330876097083\n",
      "Iteration 4272: loss 0.000905433320440352\n",
      "Iteration 4273: loss 0.000905433320440352\n",
      "Iteration 4274: loss 0.0009054330876097083\n",
      "Iteration 4275: loss 0.000905433320440352\n",
      "Iteration 4276: loss 0.000905433320440352\n",
      "Iteration 4277: loss 0.0009054330876097083\n",
      "Iteration 4278: loss 0.000905433320440352\n",
      "Iteration 4279: loss 0.000905433320440352\n",
      "Iteration 4280: loss 0.0009054330876097083\n",
      "Iteration 4281: loss 0.000905433320440352\n",
      "Iteration 4282: loss 0.000905433320440352\n",
      "Iteration 4283: loss 0.0009054330876097083\n",
      "Iteration 4284: loss 0.000905433320440352\n",
      "Iteration 4285: loss 0.000905433320440352\n",
      "Iteration 4286: loss 0.0009054330876097083\n",
      "Iteration 4287: loss 0.000905433320440352\n",
      "Iteration 4288: loss 0.000905433320440352\n",
      "Iteration 4289: loss 0.0009054330876097083\n",
      "Iteration 4290: loss 0.000905433320440352\n",
      "Iteration 4291: loss 0.000905433320440352\n",
      "Iteration 4292: loss 0.0009054330876097083\n",
      "Iteration 4293: loss 0.000905433320440352\n",
      "Iteration 4294: loss 0.000905433320440352\n",
      "Iteration 4295: loss 0.0009054330876097083\n",
      "Iteration 4296: loss 0.000905433320440352\n",
      "Iteration 4297: loss 0.000905433320440352\n",
      "Iteration 4298: loss 0.0009054330876097083\n",
      "Iteration 4299: loss 0.000905433320440352\n",
      "Iteration 4300: loss 0.000905433320440352\n",
      "Iteration 4301: loss 0.0009054330876097083\n",
      "Iteration 4302: loss 0.000905433320440352\n",
      "Iteration 4303: loss 0.000905433320440352\n",
      "Iteration 4304: loss 0.0009054330876097083\n",
      "Iteration 4305: loss 0.000905433320440352\n",
      "Iteration 4306: loss 0.000905433320440352\n",
      "Iteration 4307: loss 0.0009054330876097083\n",
      "Iteration 4308: loss 0.000905433320440352\n",
      "Iteration 4309: loss 0.000905433320440352\n",
      "Iteration 4310: loss 0.0009054330876097083\n",
      "Iteration 4311: loss 0.000905433320440352\n",
      "Iteration 4312: loss 0.000905433320440352\n",
      "Iteration 4313: loss 0.0009054330876097083\n",
      "Iteration 4314: loss 0.000905433320440352\n",
      "Iteration 4315: loss 0.000905433320440352\n",
      "Iteration 4316: loss 0.0009054330876097083\n",
      "Iteration 4317: loss 0.000905433320440352\n",
      "Iteration 4318: loss 0.000905433320440352\n",
      "Iteration 4319: loss 0.0009054330876097083\n",
      "Iteration 4320: loss 0.000905433320440352\n",
      "Iteration 4321: loss 0.000905433320440352\n",
      "Iteration 4322: loss 0.0009054330876097083\n",
      "Iteration 4323: loss 0.000905433320440352\n",
      "Iteration 4324: loss 0.000905433320440352\n",
      "Iteration 4325: loss 0.0009054330876097083\n",
      "Iteration 4326: loss 0.000905433320440352\n",
      "Iteration 4327: loss 0.000905433320440352\n",
      "Iteration 4328: loss 0.0009054330876097083\n",
      "Iteration 4329: loss 0.000905433320440352\n",
      "Iteration 4330: loss 0.000905433320440352\n",
      "Iteration 4331: loss 0.0009054330876097083\n",
      "Iteration 4332: loss 0.000905433320440352\n",
      "Iteration 4333: loss 0.000905433320440352\n",
      "Iteration 4334: loss 0.0009054330876097083\n",
      "Iteration 4335: loss 0.000905433320440352\n",
      "Iteration 4336: loss 0.000905433320440352\n",
      "Iteration 4337: loss 0.0009054330876097083\n",
      "Iteration 4338: loss 0.000905433320440352\n",
      "Iteration 4339: loss 0.000905433320440352\n",
      "Iteration 4340: loss 0.0009054330876097083\n",
      "Iteration 4341: loss 0.000905433320440352\n",
      "Iteration 4342: loss 0.000905433320440352\n",
      "Iteration 4343: loss 0.0009054330876097083\n",
      "Iteration 4344: loss 0.000905433320440352\n",
      "Iteration 4345: loss 0.000905433320440352\n",
      "Iteration 4346: loss 0.0009054330876097083\n",
      "Iteration 4347: loss 0.000905433320440352\n",
      "Iteration 4348: loss 0.000905433320440352\n",
      "Iteration 4349: loss 0.0009054330876097083\n",
      "Iteration 4350: loss 0.000905433320440352\n",
      "Iteration 4351: loss 0.000905433320440352\n",
      "Iteration 4352: loss 0.0009054330876097083\n",
      "Iteration 4353: loss 0.000905433320440352\n",
      "Iteration 4354: loss 0.000905433320440352\n",
      "Iteration 4355: loss 0.0009054330876097083\n",
      "Iteration 4356: loss 0.000905433320440352\n",
      "Iteration 4357: loss 0.000905433320440352\n",
      "Iteration 4358: loss 0.0009054330876097083\n",
      "Iteration 4359: loss 0.000905433320440352\n",
      "Iteration 4360: loss 0.000905433320440352\n",
      "Iteration 4361: loss 0.0009054330876097083\n",
      "Iteration 4362: loss 0.000905433320440352\n",
      "Iteration 4363: loss 0.000905433320440352\n",
      "Iteration 4364: loss 0.0009054330876097083\n",
      "Iteration 4365: loss 0.000905433320440352\n",
      "Iteration 4366: loss 0.000905433320440352\n",
      "Iteration 4367: loss 0.0009054330876097083\n",
      "Iteration 4368: loss 0.000905433320440352\n",
      "Iteration 4369: loss 0.000905433320440352\n",
      "Iteration 4370: loss 0.0009054330876097083\n",
      "Iteration 4371: loss 0.000905433320440352\n",
      "Iteration 4372: loss 0.000905433320440352\n",
      "Iteration 4373: loss 0.0009054330876097083\n",
      "Iteration 4374: loss 0.000905433320440352\n",
      "Iteration 4375: loss 0.000905433320440352\n",
      "Iteration 4376: loss 0.0009054330876097083\n",
      "Iteration 4377: loss 0.000905433320440352\n",
      "Iteration 4378: loss 0.000905433320440352\n",
      "Iteration 4379: loss 0.0009054330876097083\n",
      "Iteration 4380: loss 0.000905433320440352\n",
      "Iteration 4381: loss 0.000905433320440352\n",
      "Iteration 4382: loss 0.0009054330876097083\n",
      "Iteration 4383: loss 0.000905433320440352\n",
      "Iteration 4384: loss 0.000905433320440352\n",
      "Iteration 4385: loss 0.0009054330876097083\n",
      "Iteration 4386: loss 0.000905433320440352\n",
      "Iteration 4387: loss 0.000905433320440352\n",
      "Iteration 4388: loss 0.0009054330876097083\n",
      "Iteration 4389: loss 0.000905433320440352\n",
      "Iteration 4390: loss 0.000905433320440352\n",
      "Iteration 4391: loss 0.0009054330876097083\n",
      "Iteration 4392: loss 0.000905433320440352\n",
      "Iteration 4393: loss 0.000905433320440352\n",
      "Iteration 4394: loss 0.0009054330876097083\n",
      "Iteration 4395: loss 0.000905433320440352\n",
      "Iteration 4396: loss 0.000905433320440352\n",
      "Iteration 4397: loss 0.0009054330876097083\n",
      "Iteration 4398: loss 0.000905433320440352\n",
      "Iteration 4399: loss 0.000905433320440352\n",
      "Iteration 4400: loss 0.0009054330876097083\n",
      "Iteration 4401: loss 0.000905433320440352\n",
      "Iteration 4402: loss 0.000905433320440352\n",
      "Iteration 4403: loss 0.0009054330876097083\n",
      "Iteration 4404: loss 0.000905433320440352\n",
      "Iteration 4405: loss 0.000905433320440352\n",
      "Iteration 4406: loss 0.0009054330876097083\n",
      "Iteration 4407: loss 0.000905433320440352\n",
      "Iteration 4408: loss 0.000905433320440352\n",
      "Iteration 4409: loss 0.0009054330876097083\n",
      "Iteration 4410: loss 0.000905433320440352\n",
      "Iteration 4411: loss 0.000905433320440352\n",
      "Iteration 4412: loss 0.0009054330876097083\n",
      "Iteration 4413: loss 0.000905433320440352\n",
      "Iteration 4414: loss 0.000905433320440352\n",
      "Iteration 4415: loss 0.0009054330876097083\n",
      "Iteration 4416: loss 0.000905433320440352\n",
      "Iteration 4417: loss 0.000905433320440352\n",
      "Iteration 4418: loss 0.0009054330876097083\n",
      "Iteration 4419: loss 0.000905433320440352\n",
      "Iteration 4420: loss 0.000905433320440352\n",
      "Iteration 4421: loss 0.0009054330876097083\n",
      "Iteration 4422: loss 0.000905433320440352\n",
      "Iteration 4423: loss 0.000905433320440352\n",
      "Iteration 4424: loss 0.0009054330876097083\n",
      "Iteration 4425: loss 0.000905433320440352\n",
      "Iteration 4426: loss 0.000905433320440352\n",
      "Iteration 4427: loss 0.0009054330876097083\n",
      "Iteration 4428: loss 0.000905433320440352\n",
      "Iteration 4429: loss 0.000905433320440352\n",
      "Iteration 4430: loss 0.0009054330876097083\n",
      "Iteration 4431: loss 0.000905433320440352\n",
      "Iteration 4432: loss 0.000905433320440352\n",
      "Iteration 4433: loss 0.0009054330876097083\n",
      "Iteration 4434: loss 0.000905433320440352\n",
      "Iteration 4435: loss 0.000905433320440352\n",
      "Iteration 4436: loss 0.0009054330876097083\n",
      "Iteration 4437: loss 0.000905433320440352\n",
      "Iteration 4438: loss 0.000905433320440352\n",
      "Iteration 4439: loss 0.0009054330876097083\n",
      "Iteration 4440: loss 0.000905433320440352\n",
      "Iteration 4441: loss 0.000905433320440352\n",
      "Iteration 4442: loss 0.0009054330876097083\n",
      "Iteration 4443: loss 0.000905433320440352\n",
      "Iteration 4444: loss 0.000905433320440352\n",
      "Iteration 4445: loss 0.0009054330876097083\n",
      "Iteration 4446: loss 0.000905433320440352\n",
      "Iteration 4447: loss 0.000905433320440352\n",
      "Iteration 4448: loss 0.0009054330876097083\n",
      "Iteration 4449: loss 0.000905433320440352\n",
      "Iteration 4450: loss 0.000905433320440352\n",
      "Iteration 4451: loss 0.0009054330876097083\n",
      "Iteration 4452: loss 0.000905433320440352\n",
      "Iteration 4453: loss 0.000905433320440352\n",
      "Iteration 4454: loss 0.0009054330876097083\n",
      "Iteration 4455: loss 0.000905433320440352\n",
      "Iteration 4456: loss 0.000905433320440352\n",
      "Iteration 4457: loss 0.0009054330876097083\n",
      "Iteration 4458: loss 0.000905433320440352\n",
      "Iteration 4459: loss 0.000905433320440352\n",
      "Iteration 4460: loss 0.0009054330876097083\n",
      "Iteration 4461: loss 0.000905433320440352\n",
      "Iteration 4462: loss 0.000905433320440352\n",
      "Iteration 4463: loss 0.0009054330876097083\n",
      "Iteration 4464: loss 0.000905433320440352\n",
      "Iteration 4465: loss 0.000905433320440352\n",
      "Iteration 4466: loss 0.0009054330876097083\n",
      "Iteration 4467: loss 0.000905433320440352\n",
      "Iteration 4468: loss 0.000905433320440352\n",
      "Iteration 4469: loss 0.0009054330876097083\n",
      "Iteration 4470: loss 0.000905433320440352\n",
      "Iteration 4471: loss 0.000905433320440352\n",
      "Iteration 4472: loss 0.0009054330876097083\n",
      "Iteration 4473: loss 0.000905433320440352\n",
      "Iteration 4474: loss 0.000905433320440352\n",
      "Iteration 4475: loss 0.0009054330876097083\n",
      "Iteration 4476: loss 0.000905433320440352\n",
      "Iteration 4477: loss 0.000905433320440352\n",
      "Iteration 4478: loss 0.0009054330876097083\n",
      "Iteration 4479: loss 0.000905433320440352\n",
      "Iteration 4480: loss 0.000905433320440352\n",
      "Iteration 4481: loss 0.0009054330876097083\n",
      "Iteration 4482: loss 0.000905433320440352\n",
      "Iteration 4483: loss 0.000905433320440352\n",
      "Iteration 4484: loss 0.0009054330876097083\n",
      "Iteration 4485: loss 0.000905433320440352\n",
      "Iteration 4486: loss 0.000905433320440352\n",
      "Iteration 4487: loss 0.0009054330876097083\n",
      "Iteration 4488: loss 0.000905433320440352\n",
      "Iteration 4489: loss 0.000905433320440352\n",
      "Iteration 4490: loss 0.0009054330876097083\n",
      "Iteration 4491: loss 0.000905433320440352\n",
      "Iteration 4492: loss 0.000905433320440352\n",
      "Iteration 4493: loss 0.0009054330876097083\n",
      "Iteration 4494: loss 0.000905433320440352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4495: loss 0.000905433320440352\n",
      "Iteration 4496: loss 0.0009054330876097083\n",
      "Iteration 4497: loss 0.000905433320440352\n",
      "Iteration 4498: loss 0.000905433320440352\n",
      "Iteration 4499: loss 0.0009054330876097083\n",
      "Iteration 4500: loss 0.000905433320440352\n",
      "Iteration 4501: loss 0.000905433320440352\n",
      "Iteration 4502: loss 0.0009054330876097083\n",
      "Iteration 4503: loss 0.000905433320440352\n",
      "Iteration 4504: loss 0.000905433320440352\n",
      "Iteration 4505: loss 0.0009054330876097083\n",
      "Iteration 4506: loss 0.000905433320440352\n",
      "Iteration 4507: loss 0.000905433320440352\n",
      "Iteration 4508: loss 0.0009054330876097083\n",
      "Iteration 4509: loss 0.000905433320440352\n",
      "Iteration 4510: loss 0.000905433320440352\n",
      "Iteration 4511: loss 0.0009054330876097083\n",
      "Iteration 4512: loss 0.000905433320440352\n",
      "Iteration 4513: loss 0.000905433320440352\n",
      "Iteration 4514: loss 0.0009054330876097083\n",
      "Iteration 4515: loss 0.000905433320440352\n",
      "Iteration 4516: loss 0.000905433320440352\n",
      "Iteration 4517: loss 0.0009054330876097083\n",
      "Iteration 4518: loss 0.000905433320440352\n",
      "Iteration 4519: loss 0.000905433320440352\n",
      "Iteration 4520: loss 0.0009054330876097083\n",
      "Iteration 4521: loss 0.000905433320440352\n",
      "Iteration 4522: loss 0.000905433320440352\n",
      "Iteration 4523: loss 0.0009054330876097083\n",
      "Iteration 4524: loss 0.000905433320440352\n",
      "Iteration 4525: loss 0.000905433320440352\n",
      "Iteration 4526: loss 0.0009054330876097083\n",
      "Iteration 4527: loss 0.000905433320440352\n",
      "Iteration 4528: loss 0.000905433320440352\n",
      "Iteration 4529: loss 0.0009054330876097083\n",
      "Iteration 4530: loss 0.000905433320440352\n",
      "Iteration 4531: loss 0.000905433320440352\n",
      "Iteration 4532: loss 0.0009054330876097083\n",
      "Iteration 4533: loss 0.000905433320440352\n",
      "Iteration 4534: loss 0.000905433320440352\n",
      "Iteration 4535: loss 0.0009054330876097083\n",
      "Iteration 4536: loss 0.000905433320440352\n",
      "Iteration 4537: loss 0.000905433320440352\n",
      "Iteration 4538: loss 0.0009054330876097083\n",
      "Iteration 4539: loss 0.000905433320440352\n",
      "Iteration 4540: loss 0.000905433320440352\n",
      "Iteration 4541: loss 0.0009054330876097083\n",
      "Iteration 4542: loss 0.000905433320440352\n",
      "Iteration 4543: loss 0.000905433320440352\n",
      "Iteration 4544: loss 0.0009054330876097083\n",
      "Iteration 4545: loss 0.000905433320440352\n",
      "Iteration 4546: loss 0.000905433320440352\n",
      "Iteration 4547: loss 0.0009054330876097083\n",
      "Iteration 4548: loss 0.000905433320440352\n",
      "Iteration 4549: loss 0.000905433320440352\n",
      "Iteration 4550: loss 0.0009054330876097083\n",
      "Iteration 4551: loss 0.000905433320440352\n",
      "Iteration 4552: loss 0.000905433320440352\n",
      "Iteration 4553: loss 0.0009054330876097083\n",
      "Iteration 4554: loss 0.000905433320440352\n",
      "Iteration 4555: loss 0.000905433320440352\n",
      "Iteration 4556: loss 0.0009054330876097083\n",
      "Iteration 4557: loss 0.000905433320440352\n",
      "Iteration 4558: loss 0.000905433320440352\n",
      "Iteration 4559: loss 0.0009054330876097083\n",
      "Iteration 4560: loss 0.000905433320440352\n",
      "Iteration 4561: loss 0.000905433320440352\n",
      "Iteration 4562: loss 0.0009054330876097083\n",
      "Iteration 4563: loss 0.000905433320440352\n",
      "Iteration 4564: loss 0.000905433320440352\n",
      "Iteration 4565: loss 0.0009054330876097083\n",
      "Iteration 4566: loss 0.000905433320440352\n",
      "Iteration 4567: loss 0.000905433320440352\n",
      "Iteration 4568: loss 0.0009054330876097083\n",
      "Iteration 4569: loss 0.000905433320440352\n",
      "Iteration 4570: loss 0.000905433320440352\n",
      "Iteration 4571: loss 0.0009054330876097083\n",
      "Iteration 4572: loss 0.000905433320440352\n",
      "Iteration 4573: loss 0.000905433320440352\n",
      "Iteration 4574: loss 0.0009054330876097083\n",
      "Iteration 4575: loss 0.000905433320440352\n",
      "Iteration 4576: loss 0.000905433320440352\n",
      "Iteration 4577: loss 0.0009054330876097083\n",
      "Iteration 4578: loss 0.000905433320440352\n",
      "Iteration 4579: loss 0.000905433320440352\n",
      "Iteration 4580: loss 0.0009054330876097083\n",
      "Iteration 4581: loss 0.000905433320440352\n",
      "Iteration 4582: loss 0.000905433320440352\n",
      "Iteration 4583: loss 0.0009054330876097083\n",
      "Iteration 4584: loss 0.000905433320440352\n",
      "Iteration 4585: loss 0.000905433320440352\n",
      "Iteration 4586: loss 0.0009054330876097083\n",
      "Iteration 4587: loss 0.000905433320440352\n",
      "Iteration 4588: loss 0.000905433320440352\n",
      "Iteration 4589: loss 0.0009054330876097083\n",
      "Iteration 4590: loss 0.000905433320440352\n",
      "Iteration 4591: loss 0.000905433320440352\n",
      "Iteration 4592: loss 0.0009054330876097083\n",
      "Iteration 4593: loss 0.000905433320440352\n",
      "Iteration 4594: loss 0.000905433320440352\n",
      "Iteration 4595: loss 0.0009054330876097083\n",
      "Iteration 4596: loss 0.000905433320440352\n",
      "Iteration 4597: loss 0.000905433320440352\n",
      "Iteration 4598: loss 0.0009054330876097083\n",
      "Iteration 4599: loss 0.000905433320440352\n",
      "Iteration 4600: loss 0.000905433320440352\n",
      "Iteration 4601: loss 0.0009054330876097083\n",
      "Iteration 4602: loss 0.000905433320440352\n",
      "Iteration 4603: loss 0.000905433320440352\n",
      "Iteration 4604: loss 0.0009054330876097083\n",
      "Iteration 4605: loss 0.000905433320440352\n",
      "Iteration 4606: loss 0.000905433320440352\n",
      "Iteration 4607: loss 0.0009054330876097083\n",
      "Iteration 4608: loss 0.000905433320440352\n",
      "Iteration 4609: loss 0.000905433320440352\n",
      "Iteration 4610: loss 0.0009054330876097083\n",
      "Iteration 4611: loss 0.000905433320440352\n",
      "Iteration 4612: loss 0.000905433320440352\n",
      "Iteration 4613: loss 0.0009054330876097083\n",
      "Iteration 4614: loss 0.000905433320440352\n",
      "Iteration 4615: loss 0.000905433320440352\n",
      "Iteration 4616: loss 0.0009054330876097083\n",
      "Iteration 4617: loss 0.000905433320440352\n",
      "Iteration 4618: loss 0.000905433320440352\n",
      "Iteration 4619: loss 0.0009054330876097083\n",
      "Iteration 4620: loss 0.000905433320440352\n",
      "Iteration 4621: loss 0.000905433320440352\n",
      "Iteration 4622: loss 0.0009054330876097083\n",
      "Iteration 4623: loss 0.000905433320440352\n",
      "Iteration 4624: loss 0.000905433320440352\n",
      "Iteration 4625: loss 0.0009054330876097083\n",
      "Iteration 4626: loss 0.000905433320440352\n",
      "Iteration 4627: loss 0.000905433320440352\n",
      "Iteration 4628: loss 0.0009054330876097083\n",
      "Iteration 4629: loss 0.000905433320440352\n",
      "Iteration 4630: loss 0.000905433320440352\n",
      "Iteration 4631: loss 0.0009054330876097083\n",
      "Iteration 4632: loss 0.000905433320440352\n",
      "Iteration 4633: loss 0.000905433320440352\n",
      "Iteration 4634: loss 0.0009054330876097083\n",
      "Iteration 4635: loss 0.000905433320440352\n",
      "Iteration 4636: loss 0.000905433320440352\n",
      "Iteration 4637: loss 0.0009054330876097083\n",
      "Iteration 4638: loss 0.000905433320440352\n",
      "Iteration 4639: loss 0.000905433320440352\n",
      "Iteration 4640: loss 0.0009054330876097083\n",
      "Iteration 4641: loss 0.000905433320440352\n",
      "Iteration 4642: loss 0.000905433320440352\n",
      "Iteration 4643: loss 0.0009054330876097083\n",
      "Iteration 4644: loss 0.000905433320440352\n",
      "Iteration 4645: loss 0.000905433320440352\n",
      "Iteration 4646: loss 0.0009054330876097083\n",
      "Iteration 4647: loss 0.000905433320440352\n",
      "Iteration 4648: loss 0.000905433320440352\n",
      "Iteration 4649: loss 0.0009054330876097083\n",
      "Iteration 4650: loss 0.000905433320440352\n",
      "Iteration 4651: loss 0.000905433320440352\n",
      "Iteration 4652: loss 0.0009054330876097083\n",
      "Iteration 4653: loss 0.000905433320440352\n",
      "Iteration 4654: loss 0.000905433320440352\n",
      "Iteration 4655: loss 0.0009054330876097083\n",
      "Iteration 4656: loss 0.000905433320440352\n",
      "Iteration 4657: loss 0.000905433320440352\n",
      "Iteration 4658: loss 0.0009054330876097083\n",
      "Iteration 4659: loss 0.000905433320440352\n",
      "Iteration 4660: loss 0.000905433320440352\n",
      "Iteration 4661: loss 0.0009054330876097083\n",
      "Iteration 4662: loss 0.000905433320440352\n",
      "Iteration 4663: loss 0.000905433320440352\n",
      "Iteration 4664: loss 0.0009054330876097083\n",
      "Iteration 4665: loss 0.000905433320440352\n",
      "Iteration 4666: loss 0.000905433320440352\n",
      "Iteration 4667: loss 0.0009054330876097083\n",
      "Iteration 4668: loss 0.000905433320440352\n",
      "Iteration 4669: loss 0.000905433320440352\n",
      "Iteration 4670: loss 0.0009054330876097083\n",
      "Iteration 4671: loss 0.000905433320440352\n",
      "Iteration 4672: loss 0.000905433320440352\n",
      "Iteration 4673: loss 0.0009054330876097083\n",
      "Iteration 4674: loss 0.000905433320440352\n",
      "Iteration 4675: loss 0.000905433320440352\n",
      "Iteration 4676: loss 0.0009054330876097083\n",
      "Iteration 4677: loss 0.000905433320440352\n",
      "Iteration 4678: loss 0.000905433320440352\n",
      "Iteration 4679: loss 0.0009054330876097083\n",
      "Iteration 4680: loss 0.000905433320440352\n",
      "Iteration 4681: loss 0.000905433320440352\n",
      "Iteration 4682: loss 0.0009054330876097083\n",
      "Iteration 4683: loss 0.000905433320440352\n",
      "Iteration 4684: loss 0.000905433320440352\n",
      "Iteration 4685: loss 0.0009054330876097083\n",
      "Iteration 4686: loss 0.000905433320440352\n",
      "Iteration 4687: loss 0.000905433320440352\n",
      "Iteration 4688: loss 0.0009054330876097083\n",
      "Iteration 4689: loss 0.000905433320440352\n",
      "Iteration 4690: loss 0.000905433320440352\n",
      "Iteration 4691: loss 0.0009054330876097083\n",
      "Iteration 4692: loss 0.000905433320440352\n",
      "Iteration 4693: loss 0.000905433320440352\n",
      "Iteration 4694: loss 0.0009054330876097083\n",
      "Iteration 4695: loss 0.000905433320440352\n",
      "Iteration 4696: loss 0.000905433320440352\n",
      "Iteration 4697: loss 0.0009054330876097083\n",
      "Iteration 4698: loss 0.000905433320440352\n",
      "Iteration 4699: loss 0.000905433320440352\n",
      "Iteration 4700: loss 0.0009054330876097083\n",
      "Iteration 4701: loss 0.000905433320440352\n",
      "Iteration 4702: loss 0.000905433320440352\n",
      "Iteration 4703: loss 0.0009054330876097083\n",
      "Iteration 4704: loss 0.000905433320440352\n",
      "Iteration 4705: loss 0.000905433320440352\n",
      "Iteration 4706: loss 0.0009054330876097083\n",
      "Iteration 4707: loss 0.000905433320440352\n",
      "Iteration 4708: loss 0.000905433320440352\n",
      "Iteration 4709: loss 0.0009054330876097083\n",
      "Iteration 4710: loss 0.000905433320440352\n",
      "Iteration 4711: loss 0.000905433320440352\n",
      "Iteration 4712: loss 0.0009054330876097083\n",
      "Iteration 4713: loss 0.000905433320440352\n",
      "Iteration 4714: loss 0.000905433320440352\n",
      "Iteration 4715: loss 0.0009054330876097083\n",
      "Iteration 4716: loss 0.000905433320440352\n",
      "Iteration 4717: loss 0.000905433320440352\n",
      "Iteration 4718: loss 0.0009054330876097083\n",
      "Iteration 4719: loss 0.000905433320440352\n",
      "Iteration 4720: loss 0.000905433320440352\n",
      "Iteration 4721: loss 0.0009054330876097083\n",
      "Iteration 4722: loss 0.000905433320440352\n",
      "Iteration 4723: loss 0.000905433320440352\n",
      "Iteration 4724: loss 0.0009054330876097083\n",
      "Iteration 4725: loss 0.000905433320440352\n",
      "Iteration 4726: loss 0.000905433320440352\n",
      "Iteration 4727: loss 0.0009054330876097083\n",
      "Iteration 4728: loss 0.000905433320440352\n",
      "Iteration 4729: loss 0.000905433320440352\n",
      "Iteration 4730: loss 0.0009054330876097083\n",
      "Iteration 4731: loss 0.000905433320440352\n",
      "Iteration 4732: loss 0.000905433320440352\n",
      "Iteration 4733: loss 0.0009054330876097083\n",
      "Iteration 4734: loss 0.000905433320440352\n",
      "Iteration 4735: loss 0.000905433320440352\n",
      "Iteration 4736: loss 0.0009054330876097083\n",
      "Iteration 4737: loss 0.000905433320440352\n",
      "Iteration 4738: loss 0.000905433320440352\n",
      "Iteration 4739: loss 0.0009054330876097083\n",
      "Iteration 4740: loss 0.000905433320440352\n",
      "Iteration 4741: loss 0.000905433320440352\n",
      "Iteration 4742: loss 0.0009054330876097083\n",
      "Iteration 4743: loss 0.000905433320440352\n",
      "Iteration 4744: loss 0.000905433320440352\n",
      "Iteration 4745: loss 0.0009054330876097083\n",
      "Iteration 4746: loss 0.000905433320440352\n",
      "Iteration 4747: loss 0.000905433320440352\n",
      "Iteration 4748: loss 0.0009054330876097083\n",
      "Iteration 4749: loss 0.000905433320440352\n",
      "Iteration 4750: loss 0.000905433320440352\n",
      "Iteration 4751: loss 0.0009054330876097083\n",
      "Iteration 4752: loss 0.000905433320440352\n",
      "Iteration 4753: loss 0.000905433320440352\n",
      "Iteration 4754: loss 0.0009054330876097083\n",
      "Iteration 4755: loss 0.000905433320440352\n",
      "Iteration 4756: loss 0.000905433320440352\n",
      "Iteration 4757: loss 0.0009054330876097083\n",
      "Iteration 4758: loss 0.000905433320440352\n",
      "Iteration 4759: loss 0.000905433320440352\n",
      "Iteration 4760: loss 0.0009054330876097083\n",
      "Iteration 4761: loss 0.000905433320440352\n",
      "Iteration 4762: loss 0.000905433320440352\n",
      "Iteration 4763: loss 0.0009054330876097083\n",
      "Iteration 4764: loss 0.000905433320440352\n",
      "Iteration 4765: loss 0.000905433320440352\n",
      "Iteration 4766: loss 0.0009054330876097083\n",
      "Iteration 4767: loss 0.000905433320440352\n",
      "Iteration 4768: loss 0.000905433320440352\n",
      "Iteration 4769: loss 0.0009054330876097083\n",
      "Iteration 4770: loss 0.000905433320440352\n",
      "Iteration 4771: loss 0.000905433320440352\n",
      "Iteration 4772: loss 0.0009054330876097083\n",
      "Iteration 4773: loss 0.000905433320440352\n",
      "Iteration 4774: loss 0.000905433320440352\n",
      "Iteration 4775: loss 0.0009054330876097083\n",
      "Iteration 4776: loss 0.000905433320440352\n",
      "Iteration 4777: loss 0.000905433320440352\n",
      "Iteration 4778: loss 0.0009054330876097083\n",
      "Iteration 4779: loss 0.000905433320440352\n",
      "Iteration 4780: loss 0.000905433320440352\n",
      "Iteration 4781: loss 0.0009054330876097083\n",
      "Iteration 4782: loss 0.000905433320440352\n",
      "Iteration 4783: loss 0.000905433320440352\n",
      "Iteration 4784: loss 0.0009054330876097083\n",
      "Iteration 4785: loss 0.000905433320440352\n",
      "Iteration 4786: loss 0.000905433320440352\n",
      "Iteration 4787: loss 0.0009054330876097083\n",
      "Iteration 4788: loss 0.000905433320440352\n",
      "Iteration 4789: loss 0.000905433320440352\n",
      "Iteration 4790: loss 0.0009054330876097083\n",
      "Iteration 4791: loss 0.000905433320440352\n",
      "Iteration 4792: loss 0.000905433320440352\n",
      "Iteration 4793: loss 0.0009054330876097083\n",
      "Iteration 4794: loss 0.000905433320440352\n",
      "Iteration 4795: loss 0.000905433320440352\n",
      "Iteration 4796: loss 0.0009054330876097083\n",
      "Iteration 4797: loss 0.000905433320440352\n",
      "Iteration 4798: loss 0.000905433320440352\n",
      "Iteration 4799: loss 0.0009054330876097083\n",
      "Iteration 4800: loss 0.000905433320440352\n",
      "Iteration 4801: loss 0.000905433320440352\n",
      "Iteration 4802: loss 0.0009054330876097083\n",
      "Iteration 4803: loss 0.000905433320440352\n",
      "Iteration 4804: loss 0.000905433320440352\n",
      "Iteration 4805: loss 0.0009054330876097083\n",
      "Iteration 4806: loss 0.000905433320440352\n",
      "Iteration 4807: loss 0.000905433320440352\n",
      "Iteration 4808: loss 0.0009054330876097083\n",
      "Iteration 4809: loss 0.000905433320440352\n",
      "Iteration 4810: loss 0.000905433320440352\n",
      "Iteration 4811: loss 0.0009054330876097083\n",
      "Iteration 4812: loss 0.000905433320440352\n",
      "Iteration 4813: loss 0.000905433320440352\n",
      "Iteration 4814: loss 0.0009054330876097083\n",
      "Iteration 4815: loss 0.000905433320440352\n",
      "Iteration 4816: loss 0.000905433320440352\n",
      "Iteration 4817: loss 0.0009054330876097083\n",
      "Iteration 4818: loss 0.000905433320440352\n",
      "Iteration 4819: loss 0.000905433320440352\n",
      "Iteration 4820: loss 0.0009054330876097083\n",
      "Iteration 4821: loss 0.000905433320440352\n",
      "Iteration 4822: loss 0.000905433320440352\n",
      "Iteration 4823: loss 0.0009054330876097083\n",
      "Iteration 4824: loss 0.000905433320440352\n",
      "Iteration 4825: loss 0.000905433320440352\n",
      "Iteration 4826: loss 0.0009054330876097083\n",
      "Iteration 4827: loss 0.000905433320440352\n",
      "Iteration 4828: loss 0.000905433320440352\n",
      "Iteration 4829: loss 0.0009054330876097083\n",
      "Iteration 4830: loss 0.000905433320440352\n",
      "Iteration 4831: loss 0.000905433320440352\n",
      "Iteration 4832: loss 0.0009054330876097083\n",
      "Iteration 4833: loss 0.000905433320440352\n",
      "Iteration 4834: loss 0.000905433320440352\n",
      "Iteration 4835: loss 0.0009054330876097083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4836: loss 0.000905433320440352\n",
      "Iteration 4837: loss 0.000905433320440352\n",
      "Iteration 4838: loss 0.0009054330876097083\n",
      "Iteration 4839: loss 0.000905433320440352\n",
      "Iteration 4840: loss 0.000905433320440352\n",
      "Iteration 4841: loss 0.0009054330876097083\n",
      "Iteration 4842: loss 0.000905433320440352\n",
      "Iteration 4843: loss 0.000905433320440352\n",
      "Iteration 4844: loss 0.0009054330876097083\n",
      "Iteration 4845: loss 0.000905433320440352\n",
      "Iteration 4846: loss 0.000905433320440352\n",
      "Iteration 4847: loss 0.0009054330876097083\n",
      "Iteration 4848: loss 0.000905433320440352\n",
      "Iteration 4849: loss 0.000905433320440352\n",
      "Iteration 4850: loss 0.0009054330876097083\n",
      "Iteration 4851: loss 0.000905433320440352\n",
      "Iteration 4852: loss 0.000905433320440352\n",
      "Iteration 4853: loss 0.0009054330876097083\n",
      "Iteration 4854: loss 0.000905433320440352\n",
      "Iteration 4855: loss 0.000905433320440352\n",
      "Iteration 4856: loss 0.0009054330876097083\n",
      "Iteration 4857: loss 0.000905433320440352\n",
      "Iteration 4858: loss 0.000905433320440352\n",
      "Iteration 4859: loss 0.0009054330876097083\n",
      "Iteration 4860: loss 0.000905433320440352\n",
      "Iteration 4861: loss 0.000905433320440352\n",
      "Iteration 4862: loss 0.0009054330876097083\n",
      "Iteration 4863: loss 0.000905433320440352\n",
      "Iteration 4864: loss 0.000905433320440352\n",
      "Iteration 4865: loss 0.0009054330876097083\n",
      "Iteration 4866: loss 0.000905433320440352\n",
      "Iteration 4867: loss 0.000905433320440352\n",
      "Iteration 4868: loss 0.0009054330876097083\n",
      "Iteration 4869: loss 0.000905433320440352\n",
      "Iteration 4870: loss 0.000905433320440352\n",
      "Iteration 4871: loss 0.0009054330876097083\n",
      "Iteration 4872: loss 0.000905433320440352\n",
      "Iteration 4873: loss 0.000905433320440352\n",
      "Iteration 4874: loss 0.0009054330876097083\n",
      "Iteration 4875: loss 0.000905433320440352\n",
      "Iteration 4876: loss 0.000905433320440352\n",
      "Iteration 4877: loss 0.0009054330876097083\n",
      "Iteration 4878: loss 0.000905433320440352\n",
      "Iteration 4879: loss 0.000905433320440352\n",
      "Iteration 4880: loss 0.0009054330876097083\n",
      "Iteration 4881: loss 0.000905433320440352\n",
      "Iteration 4882: loss 0.000905433320440352\n",
      "Iteration 4883: loss 0.0009054330876097083\n",
      "Iteration 4884: loss 0.000905433320440352\n",
      "Iteration 4885: loss 0.000905433320440352\n",
      "Iteration 4886: loss 0.0009054330876097083\n",
      "Iteration 4887: loss 0.000905433320440352\n",
      "Iteration 4888: loss 0.000905433320440352\n",
      "Iteration 4889: loss 0.0009054330876097083\n",
      "Iteration 4890: loss 0.000905433320440352\n",
      "Iteration 4891: loss 0.000905433320440352\n",
      "Iteration 4892: loss 0.0009054330876097083\n",
      "Iteration 4893: loss 0.000905433320440352\n",
      "Iteration 4894: loss 0.000905433320440352\n",
      "Iteration 4895: loss 0.0009054330876097083\n",
      "Iteration 4896: loss 0.000905433320440352\n",
      "Iteration 4897: loss 0.000905433320440352\n",
      "Iteration 4898: loss 0.0009054330876097083\n",
      "Iteration 4899: loss 0.000905433320440352\n",
      "Iteration 4900: loss 0.000905433320440352\n",
      "Iteration 4901: loss 0.0009054330876097083\n",
      "Iteration 4902: loss 0.000905433320440352\n",
      "Iteration 4903: loss 0.000905433320440352\n",
      "Iteration 4904: loss 0.0009054330876097083\n",
      "Iteration 4905: loss 0.000905433320440352\n",
      "Iteration 4906: loss 0.000905433320440352\n",
      "Iteration 4907: loss 0.0009054330876097083\n",
      "Iteration 4908: loss 0.000905433320440352\n",
      "Iteration 4909: loss 0.000905433320440352\n",
      "Iteration 4910: loss 0.0009054330876097083\n",
      "Iteration 4911: loss 0.000905433320440352\n",
      "Iteration 4912: loss 0.000905433320440352\n",
      "Iteration 4913: loss 0.0009054330876097083\n",
      "Iteration 4914: loss 0.000905433320440352\n",
      "Iteration 4915: loss 0.000905433320440352\n",
      "Iteration 4916: loss 0.0009054330876097083\n",
      "Iteration 4917: loss 0.000905433320440352\n",
      "Iteration 4918: loss 0.000905433320440352\n",
      "Iteration 4919: loss 0.0009054330876097083\n",
      "Iteration 4920: loss 0.000905433320440352\n",
      "Iteration 4921: loss 0.000905433320440352\n",
      "Iteration 4922: loss 0.0009054330876097083\n",
      "Iteration 4923: loss 0.000905433320440352\n",
      "Iteration 4924: loss 0.000905433320440352\n",
      "Iteration 4925: loss 0.0009054330876097083\n",
      "Iteration 4926: loss 0.000905433320440352\n",
      "Iteration 4927: loss 0.000905433320440352\n",
      "Iteration 4928: loss 0.0009054330876097083\n",
      "Iteration 4929: loss 0.000905433320440352\n",
      "Iteration 4930: loss 0.000905433320440352\n",
      "Iteration 4931: loss 0.0009054330876097083\n",
      "Iteration 4932: loss 0.000905433320440352\n",
      "Iteration 4933: loss 0.000905433320440352\n",
      "Iteration 4934: loss 0.0009054330876097083\n",
      "Iteration 4935: loss 0.000905433320440352\n",
      "Iteration 4936: loss 0.000905433320440352\n",
      "Iteration 4937: loss 0.0009054330876097083\n",
      "Iteration 4938: loss 0.000905433320440352\n",
      "Iteration 4939: loss 0.000905433320440352\n",
      "Iteration 4940: loss 0.0009054330876097083\n",
      "Iteration 4941: loss 0.000905433320440352\n",
      "Iteration 4942: loss 0.000905433320440352\n",
      "Iteration 4943: loss 0.0009054330876097083\n",
      "Iteration 4944: loss 0.000905433320440352\n",
      "Iteration 4945: loss 0.000905433320440352\n",
      "Iteration 4946: loss 0.0009054330876097083\n",
      "Iteration 4947: loss 0.000905433320440352\n",
      "Iteration 4948: loss 0.000905433320440352\n",
      "Iteration 4949: loss 0.0009054330876097083\n",
      "Iteration 4950: loss 0.000905433320440352\n",
      "Iteration 4951: loss 0.000905433320440352\n",
      "Iteration 4952: loss 0.0009054330876097083\n",
      "Iteration 4953: loss 0.000905433320440352\n",
      "Iteration 4954: loss 0.000905433320440352\n",
      "Iteration 4955: loss 0.0009054330876097083\n",
      "Iteration 4956: loss 0.000905433320440352\n",
      "Iteration 4957: loss 0.000905433320440352\n",
      "Iteration 4958: loss 0.0009054330876097083\n",
      "Iteration 4959: loss 0.000905433320440352\n",
      "Iteration 4960: loss 0.000905433320440352\n",
      "Iteration 4961: loss 0.0009054330876097083\n",
      "Iteration 4962: loss 0.000905433320440352\n",
      "Iteration 4963: loss 0.000905433320440352\n",
      "Iteration 4964: loss 0.0009054330876097083\n",
      "Iteration 4965: loss 0.000905433320440352\n",
      "Iteration 4966: loss 0.000905433320440352\n",
      "Iteration 4967: loss 0.0009054330876097083\n",
      "Iteration 4968: loss 0.000905433320440352\n",
      "Iteration 4969: loss 0.000905433320440352\n",
      "Iteration 4970: loss 0.0009054330876097083\n",
      "Iteration 4971: loss 0.000905433320440352\n",
      "Iteration 4972: loss 0.000905433320440352\n",
      "Iteration 4973: loss 0.0009054330876097083\n",
      "Iteration 4974: loss 0.000905433320440352\n",
      "Iteration 4975: loss 0.000905433320440352\n",
      "Iteration 4976: loss 0.0009054330876097083\n",
      "Iteration 4977: loss 0.000905433320440352\n",
      "Iteration 4978: loss 0.000905433320440352\n",
      "Iteration 4979: loss 0.0009054330876097083\n",
      "Iteration 4980: loss 0.000905433320440352\n",
      "Iteration 4981: loss 0.000905433320440352\n",
      "Iteration 4982: loss 0.0009054330876097083\n",
      "Iteration 4983: loss 0.000905433320440352\n",
      "Iteration 4984: loss 0.000905433320440352\n",
      "Iteration 4985: loss 0.0009054330876097083\n",
      "Iteration 4986: loss 0.000905433320440352\n",
      "Iteration 4987: loss 0.000905433320440352\n",
      "Iteration 4988: loss 0.0009054330876097083\n",
      "Iteration 4989: loss 0.000905433320440352\n",
      "Iteration 4990: loss 0.000905433320440352\n",
      "Iteration 4991: loss 0.0009054330876097083\n",
      "Iteration 4992: loss 0.000905433320440352\n",
      "Iteration 4993: loss 0.000905433320440352\n",
      "Iteration 4994: loss 0.0009054330876097083\n",
      "Iteration 4995: loss 0.000905433320440352\n",
      "Iteration 4996: loss 0.000905433320440352\n",
      "Iteration 4997: loss 0.0009054330876097083\n",
      "Iteration 4998: loss 0.000905433320440352\n",
      "Iteration 4999: loss 0.000905433320440352\n",
      "Iteration 5000: loss 0.0009054330876097083\n",
      "Iteration 5001: loss 0.000905433320440352\n",
      "Iteration 5002: loss 0.000905433320440352\n",
      "Iteration 5003: loss 0.0009054330876097083\n",
      "Iteration 5004: loss 0.000905433320440352\n",
      "Iteration 5005: loss 0.000905433320440352\n",
      "Iteration 5006: loss 0.0009054330876097083\n",
      "Iteration 5007: loss 0.000905433320440352\n",
      "Iteration 5008: loss 0.000905433320440352\n",
      "Iteration 5009: loss 0.0009054330876097083\n",
      "Iteration 5010: loss 0.000905433320440352\n",
      "Iteration 5011: loss 0.000905433320440352\n",
      "Iteration 5012: loss 0.0009054330876097083\n",
      "Iteration 5013: loss 0.000905433320440352\n",
      "Iteration 5014: loss 0.000905433320440352\n",
      "Iteration 5015: loss 0.0009054330876097083\n",
      "Iteration 5016: loss 0.000905433320440352\n",
      "Iteration 5017: loss 0.000905433320440352\n",
      "Iteration 5018: loss 0.0009054330876097083\n",
      "Iteration 5019: loss 0.000905433320440352\n",
      "Iteration 5020: loss 0.000905433320440352\n",
      "Iteration 5021: loss 0.0009054330876097083\n",
      "Iteration 5022: loss 0.000905433320440352\n",
      "Iteration 5023: loss 0.000905433320440352\n",
      "Iteration 5024: loss 0.0009054330876097083\n",
      "Iteration 5025: loss 0.000905433320440352\n",
      "Iteration 5026: loss 0.000905433320440352\n",
      "Iteration 5027: loss 0.0009054330876097083\n",
      "Iteration 5028: loss 0.000905433320440352\n",
      "Iteration 5029: loss 0.000905433320440352\n",
      "Iteration 5030: loss 0.0009054330876097083\n",
      "Iteration 5031: loss 0.000905433320440352\n",
      "Iteration 5032: loss 0.000905433320440352\n",
      "Iteration 5033: loss 0.0009054330876097083\n",
      "Iteration 5034: loss 0.000905433320440352\n",
      "Iteration 5035: loss 0.000905433320440352\n",
      "Iteration 5036: loss 0.0009054330876097083\n",
      "Iteration 5037: loss 0.000905433320440352\n",
      "Iteration 5038: loss 0.000905433320440352\n",
      "Iteration 5039: loss 0.0009054330876097083\n",
      "Iteration 5040: loss 0.000905433320440352\n",
      "Iteration 5041: loss 0.000905433320440352\n",
      "Iteration 5042: loss 0.0009054330876097083\n",
      "Iteration 5043: loss 0.000905433320440352\n",
      "Iteration 5044: loss 0.000905433320440352\n",
      "Iteration 5045: loss 0.0009054330876097083\n",
      "Iteration 5046: loss 0.000905433320440352\n",
      "Iteration 5047: loss 0.000905433320440352\n",
      "Iteration 5048: loss 0.0009054330876097083\n",
      "Iteration 5049: loss 0.000905433320440352\n",
      "Iteration 5050: loss 0.000905433320440352\n",
      "Iteration 5051: loss 0.0009054330876097083\n",
      "Iteration 5052: loss 0.000905433320440352\n",
      "Iteration 5053: loss 0.000905433320440352\n",
      "Iteration 5054: loss 0.0009054330876097083\n",
      "Iteration 5055: loss 0.000905433320440352\n",
      "Iteration 5056: loss 0.000905433320440352\n",
      "Iteration 5057: loss 0.0009054330876097083\n",
      "Iteration 5058: loss 0.000905433320440352\n",
      "Iteration 5059: loss 0.000905433320440352\n",
      "Iteration 5060: loss 0.0009054330876097083\n",
      "Iteration 5061: loss 0.000905433320440352\n",
      "Iteration 5062: loss 0.000905433320440352\n",
      "Iteration 5063: loss 0.0009054330876097083\n",
      "Iteration 5064: loss 0.000905433320440352\n",
      "Iteration 5065: loss 0.000905433320440352\n",
      "Iteration 5066: loss 0.0009054330876097083\n",
      "Iteration 5067: loss 0.000905433320440352\n",
      "Iteration 5068: loss 0.000905433320440352\n",
      "Iteration 5069: loss 0.0009054330876097083\n",
      "Iteration 5070: loss 0.000905433320440352\n",
      "Iteration 5071: loss 0.000905433320440352\n",
      "Iteration 5072: loss 0.0009054330876097083\n",
      "Iteration 5073: loss 0.000905433320440352\n",
      "Iteration 5074: loss 0.000905433320440352\n",
      "Iteration 5075: loss 0.0009054330876097083\n",
      "Iteration 5076: loss 0.000905433320440352\n",
      "Iteration 5077: loss 0.000905433320440352\n",
      "Iteration 5078: loss 0.0009054330876097083\n",
      "Iteration 5079: loss 0.000905433320440352\n",
      "Iteration 5080: loss 0.000905433320440352\n",
      "Iteration 5081: loss 0.0009054330876097083\n",
      "Iteration 5082: loss 0.000905433320440352\n",
      "Iteration 5083: loss 0.000905433320440352\n",
      "Iteration 5084: loss 0.0009054330876097083\n",
      "Iteration 5085: loss 0.000905433320440352\n",
      "Iteration 5086: loss 0.000905433320440352\n",
      "Iteration 5087: loss 0.0009054330876097083\n",
      "Iteration 5088: loss 0.000905433320440352\n",
      "Iteration 5089: loss 0.000905433320440352\n",
      "Iteration 5090: loss 0.0009054330876097083\n",
      "Iteration 5091: loss 0.000905433320440352\n",
      "Iteration 5092: loss 0.000905433320440352\n",
      "Iteration 5093: loss 0.0009054330876097083\n",
      "Iteration 5094: loss 0.000905433320440352\n",
      "Iteration 5095: loss 0.000905433320440352\n",
      "Iteration 5096: loss 0.0009054330876097083\n",
      "Iteration 5097: loss 0.000905433320440352\n",
      "Iteration 5098: loss 0.000905433320440352\n",
      "Iteration 5099: loss 0.0009054330876097083\n",
      "Iteration 5100: loss 0.000905433320440352\n",
      "Iteration 5101: loss 0.000905433320440352\n",
      "Iteration 5102: loss 0.0009054330876097083\n",
      "Iteration 5103: loss 0.000905433320440352\n",
      "Iteration 5104: loss 0.000905433320440352\n",
      "Iteration 5105: loss 0.0009054330876097083\n",
      "Iteration 5106: loss 0.000905433320440352\n",
      "Iteration 5107: loss 0.000905433320440352\n",
      "Iteration 5108: loss 0.0009054330876097083\n",
      "Iteration 5109: loss 0.000905433320440352\n",
      "Iteration 5110: loss 0.000905433320440352\n",
      "Iteration 5111: loss 0.0009054330876097083\n",
      "Iteration 5112: loss 0.000905433320440352\n",
      "Iteration 5113: loss 0.000905433320440352\n",
      "Iteration 5114: loss 0.0009054330876097083\n",
      "Iteration 5115: loss 0.000905433320440352\n",
      "Iteration 5116: loss 0.000905433320440352\n",
      "Iteration 5117: loss 0.0009054330876097083\n",
      "Iteration 5118: loss 0.000905433320440352\n",
      "Iteration 5119: loss 0.000905433320440352\n",
      "Iteration 5120: loss 0.0009054330876097083\n",
      "Iteration 5121: loss 0.000905433320440352\n",
      "Iteration 5122: loss 0.000905433320440352\n",
      "Iteration 5123: loss 0.0009054330876097083\n",
      "Iteration 5124: loss 0.000905433320440352\n",
      "Iteration 5125: loss 0.000905433320440352\n",
      "Iteration 5126: loss 0.0009054330876097083\n",
      "Iteration 5127: loss 0.000905433320440352\n",
      "Iteration 5128: loss 0.000905433320440352\n",
      "Iteration 5129: loss 0.0009054330876097083\n",
      "Iteration 5130: loss 0.000905433320440352\n",
      "Iteration 5131: loss 0.000905433320440352\n",
      "Iteration 5132: loss 0.0009054330876097083\n",
      "Iteration 5133: loss 0.000905433320440352\n",
      "Iteration 5134: loss 0.000905433320440352\n",
      "Iteration 5135: loss 0.0009054330876097083\n",
      "Iteration 5136: loss 0.000905433320440352\n",
      "Iteration 5137: loss 0.000905433320440352\n",
      "Iteration 5138: loss 0.0009054330876097083\n",
      "Iteration 5139: loss 0.000905433320440352\n",
      "Iteration 5140: loss 0.000905433320440352\n",
      "Iteration 5141: loss 0.0009054330876097083\n",
      "Iteration 5142: loss 0.000905433320440352\n",
      "Iteration 5143: loss 0.000905433320440352\n",
      "Iteration 5144: loss 0.0009054330876097083\n",
      "Iteration 5145: loss 0.000905433320440352\n",
      "Iteration 5146: loss 0.000905433320440352\n",
      "Iteration 5147: loss 0.0009054330876097083\n",
      "Iteration 5148: loss 0.000905433320440352\n",
      "Iteration 5149: loss 0.000905433320440352\n",
      "Iteration 5150: loss 0.0009054330876097083\n",
      "Iteration 5151: loss 0.000905433320440352\n",
      "Iteration 5152: loss 0.000905433320440352\n",
      "Iteration 5153: loss 0.0009054330876097083\n",
      "Iteration 5154: loss 0.000905433320440352\n",
      "Iteration 5155: loss 0.000905433320440352\n",
      "Iteration 5156: loss 0.0009054330876097083\n",
      "Iteration 5157: loss 0.000905433320440352\n",
      "Iteration 5158: loss 0.000905433320440352\n",
      "Iteration 5159: loss 0.0009054330876097083\n",
      "Iteration 5160: loss 0.000905433320440352\n",
      "Iteration 5161: loss 0.000905433320440352\n",
      "Iteration 5162: loss 0.0009054330876097083\n",
      "Iteration 5163: loss 0.000905433320440352\n",
      "Iteration 5164: loss 0.000905433320440352\n",
      "Iteration 5165: loss 0.0009054330876097083\n",
      "Iteration 5166: loss 0.000905433320440352\n",
      "Iteration 5167: loss 0.000905433320440352\n",
      "Iteration 5168: loss 0.0009054330876097083\n",
      "Iteration 5169: loss 0.000905433320440352\n",
      "Iteration 5170: loss 0.000905433320440352\n",
      "Iteration 5171: loss 0.0009054330876097083\n",
      "Iteration 5172: loss 0.000905433320440352\n",
      "Iteration 5173: loss 0.000905433320440352\n",
      "Iteration 5174: loss 0.0009054330876097083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5175: loss 0.000905433320440352\n",
      "Iteration 5176: loss 0.000905433320440352\n",
      "Iteration 5177: loss 0.0009054330876097083\n",
      "Iteration 5178: loss 0.000905433320440352\n",
      "Iteration 5179: loss 0.000905433320440352\n",
      "Iteration 5180: loss 0.0009054330876097083\n",
      "Iteration 5181: loss 0.000905433320440352\n",
      "Iteration 5182: loss 0.000905433320440352\n",
      "Iteration 5183: loss 0.0009054330876097083\n",
      "Iteration 5184: loss 0.000905433320440352\n",
      "Iteration 5185: loss 0.000905433320440352\n",
      "Iteration 5186: loss 0.0009054330876097083\n",
      "Iteration 5187: loss 0.000905433320440352\n",
      "Iteration 5188: loss 0.000905433320440352\n",
      "Iteration 5189: loss 0.0009054330876097083\n",
      "Iteration 5190: loss 0.000905433320440352\n",
      "Iteration 5191: loss 0.000905433320440352\n",
      "Iteration 5192: loss 0.0009054330876097083\n",
      "Iteration 5193: loss 0.000905433320440352\n",
      "Iteration 5194: loss 0.000905433320440352\n",
      "Iteration 5195: loss 0.0009054330876097083\n",
      "Iteration 5196: loss 0.000905433320440352\n",
      "Iteration 5197: loss 0.000905433320440352\n",
      "Iteration 5198: loss 0.0009054330876097083\n",
      "Iteration 5199: loss 0.000905433320440352\n",
      "Iteration 5200: loss 0.000905433320440352\n",
      "Iteration 5201: loss 0.0009054330876097083\n",
      "Iteration 5202: loss 0.000905433320440352\n",
      "Iteration 5203: loss 0.000905433320440352\n",
      "Iteration 5204: loss 0.0009054330876097083\n",
      "Iteration 5205: loss 0.000905433320440352\n",
      "Iteration 5206: loss 0.000905433320440352\n",
      "Iteration 5207: loss 0.0009054330876097083\n",
      "Iteration 5208: loss 0.000905433320440352\n",
      "Iteration 5209: loss 0.000905433320440352\n",
      "Iteration 5210: loss 0.0009054330876097083\n",
      "Iteration 5211: loss 0.000905433320440352\n",
      "Iteration 5212: loss 0.000905433320440352\n",
      "Iteration 5213: loss 0.0009054330876097083\n",
      "Iteration 5214: loss 0.000905433320440352\n",
      "Iteration 5215: loss 0.000905433320440352\n",
      "Iteration 5216: loss 0.0009054330876097083\n",
      "Iteration 5217: loss 0.000905433320440352\n",
      "Iteration 5218: loss 0.000905433320440352\n",
      "Iteration 5219: loss 0.0009054330876097083\n",
      "Iteration 5220: loss 0.000905433320440352\n",
      "Iteration 5221: loss 0.000905433320440352\n",
      "Iteration 5222: loss 0.0009054330876097083\n",
      "Iteration 5223: loss 0.000905433320440352\n",
      "Iteration 5224: loss 0.000905433320440352\n",
      "Iteration 5225: loss 0.0009054330876097083\n",
      "Iteration 5226: loss 0.000905433320440352\n",
      "Iteration 5227: loss 0.000905433320440352\n",
      "Iteration 5228: loss 0.0009054330876097083\n",
      "Iteration 5229: loss 0.000905433320440352\n",
      "Iteration 5230: loss 0.000905433320440352\n",
      "Iteration 5231: loss 0.0009054330876097083\n",
      "Iteration 5232: loss 0.000905433320440352\n",
      "Iteration 5233: loss 0.000905433320440352\n",
      "Iteration 5234: loss 0.0009054330876097083\n",
      "Iteration 5235: loss 0.000905433320440352\n",
      "Iteration 5236: loss 0.000905433320440352\n",
      "Iteration 5237: loss 0.0009054330876097083\n",
      "Iteration 5238: loss 0.000905433320440352\n",
      "Iteration 5239: loss 0.000905433320440352\n",
      "Iteration 5240: loss 0.0009054330876097083\n",
      "Iteration 5241: loss 0.000905433320440352\n",
      "Iteration 5242: loss 0.000905433320440352\n",
      "Iteration 5243: loss 0.0009054330876097083\n",
      "Iteration 5244: loss 0.000905433320440352\n",
      "Iteration 5245: loss 0.000905433320440352\n",
      "Iteration 5246: loss 0.0009054330876097083\n",
      "Iteration 5247: loss 0.000905433320440352\n",
      "Iteration 5248: loss 0.000905433320440352\n",
      "Iteration 5249: loss 0.0009054330876097083\n",
      "Iteration 5250: loss 0.000905433320440352\n",
      "Iteration 5251: loss 0.000905433320440352\n",
      "Iteration 5252: loss 0.0009054330876097083\n",
      "Iteration 5253: loss 0.000905433320440352\n",
      "Iteration 5254: loss 0.000905433320440352\n",
      "Iteration 5255: loss 0.0009054330876097083\n",
      "Iteration 5256: loss 0.000905433320440352\n",
      "Iteration 5257: loss 0.000905433320440352\n",
      "Iteration 5258: loss 0.0009054330876097083\n",
      "Iteration 5259: loss 0.000905433320440352\n",
      "Iteration 5260: loss 0.000905433320440352\n",
      "Iteration 5261: loss 0.0009054330876097083\n",
      "Iteration 5262: loss 0.000905433320440352\n",
      "Iteration 5263: loss 0.000905433320440352\n",
      "Iteration 5264: loss 0.0009054330876097083\n",
      "Iteration 5265: loss 0.000905433320440352\n",
      "Iteration 5266: loss 0.000905433320440352\n",
      "Iteration 5267: loss 0.0009054330876097083\n",
      "Iteration 5268: loss 0.000905433320440352\n",
      "Iteration 5269: loss 0.000905433320440352\n",
      "Iteration 5270: loss 0.0009054330876097083\n",
      "Iteration 5271: loss 0.000905433320440352\n",
      "Iteration 5272: loss 0.000905433320440352\n",
      "Iteration 5273: loss 0.0009054330876097083\n",
      "Iteration 5274: loss 0.000905433320440352\n",
      "Iteration 5275: loss 0.000905433320440352\n",
      "Iteration 5276: loss 0.0009054330876097083\n",
      "Iteration 5277: loss 0.000905433320440352\n",
      "Iteration 5278: loss 0.000905433320440352\n",
      "Iteration 5279: loss 0.0009054330876097083\n",
      "Iteration 5280: loss 0.000905433320440352\n",
      "Iteration 5281: loss 0.000905433320440352\n",
      "Iteration 5282: loss 0.0009054330876097083\n",
      "Iteration 5283: loss 0.000905433320440352\n",
      "Iteration 5284: loss 0.000905433320440352\n",
      "Iteration 5285: loss 0.0009054330876097083\n",
      "Iteration 5286: loss 0.000905433320440352\n",
      "Iteration 5287: loss 0.000905433320440352\n",
      "Iteration 5288: loss 0.0009054330876097083\n",
      "Iteration 5289: loss 0.000905433320440352\n",
      "Iteration 5290: loss 0.000905433320440352\n",
      "Iteration 5291: loss 0.0009054330876097083\n",
      "Iteration 5292: loss 0.000905433320440352\n",
      "Iteration 5293: loss 0.000905433320440352\n",
      "Iteration 5294: loss 0.0009054330876097083\n",
      "Iteration 5295: loss 0.000905433320440352\n",
      "Iteration 5296: loss 0.000905433320440352\n",
      "Iteration 5297: loss 0.0009054330876097083\n",
      "Iteration 5298: loss 0.000905433320440352\n",
      "Iteration 5299: loss 0.000905433320440352\n",
      "Iteration 5300: loss 0.0009054330876097083\n",
      "Iteration 5301: loss 0.000905433320440352\n",
      "Iteration 5302: loss 0.000905433320440352\n",
      "Iteration 5303: loss 0.0009054330876097083\n",
      "Iteration 5304: loss 0.000905433320440352\n",
      "Iteration 5305: loss 0.000905433320440352\n",
      "Iteration 5306: loss 0.0009054330876097083\n",
      "Iteration 5307: loss 0.000905433320440352\n",
      "Iteration 5308: loss 0.000905433320440352\n",
      "Iteration 5309: loss 0.0009054330876097083\n",
      "Iteration 5310: loss 0.000905433320440352\n",
      "Iteration 5311: loss 0.000905433320440352\n",
      "Iteration 5312: loss 0.0009054330876097083\n",
      "Iteration 5313: loss 0.000905433320440352\n",
      "Iteration 5314: loss 0.000905433320440352\n",
      "Iteration 5315: loss 0.0009054330876097083\n",
      "Iteration 5316: loss 0.000905433320440352\n",
      "Iteration 5317: loss 0.000905433320440352\n",
      "Iteration 5318: loss 0.0009054330876097083\n",
      "Iteration 5319: loss 0.000905433320440352\n",
      "Iteration 5320: loss 0.000905433320440352\n",
      "Iteration 5321: loss 0.0009054330876097083\n",
      "Iteration 5322: loss 0.000905433320440352\n",
      "Iteration 5323: loss 0.000905433320440352\n",
      "Iteration 5324: loss 0.0009054330876097083\n",
      "Iteration 5325: loss 0.000905433320440352\n",
      "Iteration 5326: loss 0.000905433320440352\n",
      "Iteration 5327: loss 0.0009054330876097083\n",
      "Iteration 5328: loss 0.000905433320440352\n",
      "Iteration 5329: loss 0.000905433320440352\n",
      "Iteration 5330: loss 0.0009054330876097083\n",
      "Iteration 5331: loss 0.000905433320440352\n",
      "Iteration 5332: loss 0.000905433320440352\n",
      "Iteration 5333: loss 0.0009054330876097083\n",
      "Iteration 5334: loss 0.000905433320440352\n",
      "Iteration 5335: loss 0.000905433320440352\n",
      "Iteration 5336: loss 0.0009054330876097083\n",
      "Iteration 5337: loss 0.000905433320440352\n",
      "Iteration 5338: loss 0.000905433320440352\n",
      "Iteration 5339: loss 0.0009054330876097083\n",
      "Iteration 5340: loss 0.000905433320440352\n",
      "Iteration 5341: loss 0.000905433320440352\n",
      "Iteration 5342: loss 0.0009054330876097083\n",
      "Iteration 5343: loss 0.000905433320440352\n",
      "Iteration 5344: loss 0.000905433320440352\n",
      "Iteration 5345: loss 0.0009054330876097083\n",
      "Iteration 5346: loss 0.000905433320440352\n",
      "Iteration 5347: loss 0.000905433320440352\n",
      "Iteration 5348: loss 0.0009054330876097083\n",
      "Iteration 5349: loss 0.000905433320440352\n",
      "Iteration 5350: loss 0.000905433320440352\n",
      "Iteration 5351: loss 0.0009054330876097083\n",
      "Iteration 5352: loss 0.000905433320440352\n",
      "Iteration 5353: loss 0.000905433320440352\n",
      "Iteration 5354: loss 0.0009054330876097083\n",
      "Iteration 5355: loss 0.000905433320440352\n",
      "Iteration 5356: loss 0.000905433320440352\n",
      "Iteration 5357: loss 0.0009054330876097083\n",
      "Iteration 5358: loss 0.000905433320440352\n",
      "Iteration 5359: loss 0.000905433320440352\n",
      "Iteration 5360: loss 0.0009054330876097083\n",
      "Iteration 5361: loss 0.000905433320440352\n",
      "Iteration 5362: loss 0.000905433320440352\n",
      "Iteration 5363: loss 0.0009054330876097083\n",
      "Iteration 5364: loss 0.000905433320440352\n",
      "Iteration 5365: loss 0.000905433320440352\n",
      "Iteration 5366: loss 0.0009054330876097083\n",
      "Iteration 5367: loss 0.000905433320440352\n",
      "Iteration 5368: loss 0.000905433320440352\n",
      "Iteration 5369: loss 0.0009054330876097083\n",
      "Iteration 5370: loss 0.000905433320440352\n",
      "Iteration 5371: loss 0.000905433320440352\n",
      "Iteration 5372: loss 0.0009054330876097083\n",
      "Iteration 5373: loss 0.000905433320440352\n",
      "Iteration 5374: loss 0.000905433320440352\n",
      "Iteration 5375: loss 0.0009054330876097083\n",
      "Iteration 5376: loss 0.000905433320440352\n",
      "Iteration 5377: loss 0.000905433320440352\n",
      "Iteration 5378: loss 0.0009054330876097083\n",
      "Iteration 5379: loss 0.000905433320440352\n",
      "Iteration 5380: loss 0.000905433320440352\n",
      "Iteration 5381: loss 0.0009054330876097083\n",
      "Iteration 5382: loss 0.000905433320440352\n",
      "Iteration 5383: loss 0.000905433320440352\n",
      "Iteration 5384: loss 0.0009054330876097083\n",
      "Iteration 5385: loss 0.000905433320440352\n",
      "Iteration 5386: loss 0.000905433320440352\n",
      "Iteration 5387: loss 0.0009054330876097083\n",
      "Iteration 5388: loss 0.000905433320440352\n",
      "Iteration 5389: loss 0.000905433320440352\n",
      "Iteration 5390: loss 0.0009054330876097083\n",
      "Iteration 5391: loss 0.000905433320440352\n",
      "Iteration 5392: loss 0.000905433320440352\n",
      "Iteration 5393: loss 0.0009054330876097083\n",
      "Iteration 5394: loss 0.000905433320440352\n",
      "Iteration 5395: loss 0.000905433320440352\n",
      "Iteration 5396: loss 0.0009054330876097083\n",
      "Iteration 5397: loss 0.000905433320440352\n",
      "Iteration 5398: loss 0.000905433320440352\n",
      "Iteration 5399: loss 0.0009054330876097083\n",
      "Iteration 5400: loss 0.000905433320440352\n",
      "Iteration 5401: loss 0.000905433320440352\n",
      "Iteration 5402: loss 0.0009054330876097083\n",
      "Iteration 5403: loss 0.000905433320440352\n",
      "Iteration 5404: loss 0.000905433320440352\n",
      "Iteration 5405: loss 0.0009054330876097083\n",
      "Iteration 5406: loss 0.000905433320440352\n",
      "Iteration 5407: loss 0.000905433320440352\n",
      "Iteration 5408: loss 0.0009054330876097083\n",
      "Iteration 5409: loss 0.000905433320440352\n",
      "Iteration 5410: loss 0.000905433320440352\n",
      "Iteration 5411: loss 0.0009054330876097083\n",
      "Iteration 5412: loss 0.000905433320440352\n",
      "Iteration 5413: loss 0.000905433320440352\n",
      "Iteration 5414: loss 0.0009054330876097083\n",
      "Iteration 5415: loss 0.000905433320440352\n",
      "Iteration 5416: loss 0.000905433320440352\n",
      "Iteration 5417: loss 0.0009054330876097083\n",
      "Iteration 5418: loss 0.000905433320440352\n",
      "Iteration 5419: loss 0.000905433320440352\n",
      "Iteration 5420: loss 0.0009054330876097083\n",
      "Iteration 5421: loss 0.000905433320440352\n",
      "Iteration 5422: loss 0.000905433320440352\n",
      "Iteration 5423: loss 0.0009054330876097083\n",
      "Iteration 5424: loss 0.000905433320440352\n",
      "Iteration 5425: loss 0.000905433320440352\n",
      "Iteration 5426: loss 0.0009054330876097083\n",
      "Iteration 5427: loss 0.000905433320440352\n",
      "Iteration 5428: loss 0.000905433320440352\n",
      "Iteration 5429: loss 0.0009054330876097083\n",
      "Iteration 5430: loss 0.000905433320440352\n",
      "Iteration 5431: loss 0.000905433320440352\n",
      "Iteration 5432: loss 0.0009054330876097083\n",
      "Iteration 5433: loss 0.000905433320440352\n",
      "Iteration 5434: loss 0.000905433320440352\n",
      "Iteration 5435: loss 0.0009054330876097083\n",
      "Iteration 5436: loss 0.000905433320440352\n",
      "Iteration 5437: loss 0.000905433320440352\n",
      "Iteration 5438: loss 0.0009054330876097083\n",
      "Iteration 5439: loss 0.000905433320440352\n",
      "Iteration 5440: loss 0.000905433320440352\n",
      "Iteration 5441: loss 0.0009054330876097083\n",
      "Iteration 5442: loss 0.000905433320440352\n",
      "Iteration 5443: loss 0.000905433320440352\n",
      "Iteration 5444: loss 0.0009054330876097083\n",
      "Iteration 5445: loss 0.000905433320440352\n",
      "Iteration 5446: loss 0.000905433320440352\n",
      "Iteration 5447: loss 0.0009054330876097083\n",
      "Iteration 5448: loss 0.000905433320440352\n",
      "Iteration 5449: loss 0.000905433320440352\n",
      "Iteration 5450: loss 0.0009054330876097083\n",
      "Iteration 5451: loss 0.000905433320440352\n",
      "Iteration 5452: loss 0.000905433320440352\n",
      "Iteration 5453: loss 0.0009054330876097083\n",
      "Iteration 5454: loss 0.000905433320440352\n",
      "Iteration 5455: loss 0.000905433320440352\n",
      "Iteration 5456: loss 0.0009054330876097083\n",
      "Iteration 5457: loss 0.000905433320440352\n",
      "Iteration 5458: loss 0.000905433320440352\n",
      "Iteration 5459: loss 0.0009054330876097083\n",
      "Iteration 5460: loss 0.000905433320440352\n",
      "Iteration 5461: loss 0.000905433320440352\n",
      "Iteration 5462: loss 0.0009054330876097083\n",
      "Iteration 5463: loss 0.000905433320440352\n",
      "Iteration 5464: loss 0.000905433320440352\n",
      "Iteration 5465: loss 0.0009054330876097083\n",
      "Iteration 5466: loss 0.000905433320440352\n",
      "Iteration 5467: loss 0.000905433320440352\n",
      "Iteration 5468: loss 0.0009054330876097083\n",
      "Iteration 5469: loss 0.000905433320440352\n",
      "Iteration 5470: loss 0.000905433320440352\n",
      "Iteration 5471: loss 0.0009054330876097083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5472: loss 0.000905433320440352\n",
      "Iteration 5473: loss 0.000905433320440352\n",
      "Iteration 5474: loss 0.0009054330876097083\n",
      "Iteration 5475: loss 0.000905433320440352\n",
      "Iteration 5476: loss 0.000905433320440352\n",
      "Iteration 5477: loss 0.0009054330876097083\n",
      "Iteration 5478: loss 0.000905433320440352\n",
      "Iteration 5479: loss 0.000905433320440352\n",
      "Iteration 5480: loss 0.0009054330876097083\n",
      "Iteration 5481: loss 0.000905433320440352\n",
      "Iteration 5482: loss 0.000905433320440352\n",
      "Iteration 5483: loss 0.0009054330876097083\n",
      "Iteration 5484: loss 0.000905433320440352\n",
      "Iteration 5485: loss 0.000905433320440352\n",
      "Iteration 5486: loss 0.0009054330876097083\n",
      "Iteration 5487: loss 0.000905433320440352\n",
      "Iteration 5488: loss 0.000905433320440352\n",
      "Iteration 5489: loss 0.0009054330876097083\n",
      "Iteration 5490: loss 0.000905433320440352\n",
      "Iteration 5491: loss 0.000905433320440352\n",
      "Iteration 5492: loss 0.0009054330876097083\n",
      "Iteration 5493: loss 0.000905433320440352\n",
      "Iteration 5494: loss 0.000905433320440352\n",
      "Iteration 5495: loss 0.0009054330876097083\n",
      "Iteration 5496: loss 0.000905433320440352\n",
      "Iteration 5497: loss 0.000905433320440352\n",
      "Iteration 5498: loss 0.0009054330876097083\n",
      "Iteration 5499: loss 0.000905433320440352\n",
      "Iteration 5500: loss 0.000905433320440352\n",
      "Iteration 5501: loss 0.0009054330876097083\n",
      "Iteration 5502: loss 0.000905433320440352\n",
      "Iteration 5503: loss 0.000905433320440352\n",
      "Iteration 5504: loss 0.0009054330876097083\n",
      "Iteration 5505: loss 0.000905433320440352\n",
      "Iteration 5506: loss 0.000905433320440352\n",
      "Iteration 5507: loss 0.0009054330876097083\n",
      "Iteration 5508: loss 0.000905433320440352\n",
      "Iteration 5509: loss 0.000905433320440352\n",
      "Iteration 5510: loss 0.0009054330876097083\n",
      "Iteration 5511: loss 0.000905433320440352\n",
      "Iteration 5512: loss 0.000905433320440352\n",
      "Iteration 5513: loss 0.0009054330876097083\n",
      "Iteration 5514: loss 0.000905433320440352\n",
      "Iteration 5515: loss 0.000905433320440352\n",
      "Iteration 5516: loss 0.0009054330876097083\n",
      "Iteration 5517: loss 0.000905433320440352\n",
      "Iteration 5518: loss 0.000905433320440352\n",
      "Iteration 5519: loss 0.0009054330876097083\n",
      "Iteration 5520: loss 0.000905433320440352\n",
      "Iteration 5521: loss 0.000905433320440352\n",
      "Iteration 5522: loss 0.0009054330876097083\n",
      "Iteration 5523: loss 0.000905433320440352\n",
      "Iteration 5524: loss 0.000905433320440352\n",
      "Iteration 5525: loss 0.0009054330876097083\n",
      "Iteration 5526: loss 0.000905433320440352\n",
      "Iteration 5527: loss 0.000905433320440352\n",
      "Iteration 5528: loss 0.0009054330876097083\n",
      "Iteration 5529: loss 0.000905433320440352\n",
      "Iteration 5530: loss 0.000905433320440352\n",
      "Iteration 5531: loss 0.0009054330876097083\n",
      "Iteration 5532: loss 0.000905433320440352\n",
      "Iteration 5533: loss 0.000905433320440352\n",
      "Iteration 5534: loss 0.0009054330876097083\n",
      "Iteration 5535: loss 0.000905433320440352\n",
      "Iteration 5536: loss 0.000905433320440352\n",
      "Iteration 5537: loss 0.0009054330876097083\n",
      "Iteration 5538: loss 0.000905433320440352\n",
      "Iteration 5539: loss 0.000905433320440352\n",
      "Iteration 5540: loss 0.0009054330876097083\n",
      "Iteration 5541: loss 0.000905433320440352\n",
      "Iteration 5542: loss 0.000905433320440352\n",
      "Iteration 5543: loss 0.0009054330876097083\n",
      "Iteration 5544: loss 0.000905433320440352\n",
      "Iteration 5545: loss 0.000905433320440352\n",
      "Iteration 5546: loss 0.0009054330876097083\n",
      "Iteration 5547: loss 0.000905433320440352\n",
      "Iteration 5548: loss 0.000905433320440352\n",
      "Iteration 5549: loss 0.0009054330876097083\n",
      "Iteration 5550: loss 0.000905433320440352\n",
      "Iteration 5551: loss 0.000905433320440352\n",
      "Iteration 5552: loss 0.0009054330876097083\n",
      "Iteration 5553: loss 0.000905433320440352\n",
      "Iteration 5554: loss 0.000905433320440352\n",
      "Iteration 5555: loss 0.0009054330876097083\n",
      "Iteration 5556: loss 0.000905433320440352\n",
      "Iteration 5557: loss 0.000905433320440352\n",
      "Iteration 5558: loss 0.0009054330876097083\n",
      "Iteration 5559: loss 0.000905433320440352\n",
      "Iteration 5560: loss 0.000905433320440352\n",
      "Iteration 5561: loss 0.0009054330876097083\n",
      "Iteration 5562: loss 0.000905433320440352\n",
      "Iteration 5563: loss 0.000905433320440352\n",
      "Iteration 5564: loss 0.0009054330876097083\n",
      "Iteration 5565: loss 0.000905433320440352\n",
      "Iteration 5566: loss 0.000905433320440352\n",
      "Iteration 5567: loss 0.0009054330876097083\n",
      "Iteration 5568: loss 0.000905433320440352\n",
      "Iteration 5569: loss 0.000905433320440352\n",
      "Iteration 5570: loss 0.0009054330876097083\n",
      "Iteration 5571: loss 0.000905433320440352\n",
      "Iteration 5572: loss 0.000905433320440352\n",
      "Iteration 5573: loss 0.0009054330876097083\n",
      "Iteration 5574: loss 0.000905433320440352\n",
      "Iteration 5575: loss 0.000905433320440352\n",
      "Iteration 5576: loss 0.0009054330876097083\n",
      "Iteration 5577: loss 0.000905433320440352\n",
      "Iteration 5578: loss 0.000905433320440352\n",
      "Iteration 5579: loss 0.0009054330876097083\n",
      "Iteration 5580: loss 0.000905433320440352\n",
      "Iteration 5581: loss 0.000905433320440352\n",
      "Iteration 5582: loss 0.0009054330876097083\n",
      "Iteration 5583: loss 0.000905433320440352\n",
      "Iteration 5584: loss 0.000905433320440352\n",
      "Iteration 5585: loss 0.0009054330876097083\n",
      "Iteration 5586: loss 0.000905433320440352\n",
      "Iteration 5587: loss 0.000905433320440352\n",
      "Iteration 5588: loss 0.0009054330876097083\n",
      "Iteration 5589: loss 0.000905433320440352\n",
      "Iteration 5590: loss 0.000905433320440352\n",
      "Iteration 5591: loss 0.0009054330876097083\n",
      "Iteration 5592: loss 0.000905433320440352\n",
      "Iteration 5593: loss 0.000905433320440352\n",
      "Iteration 5594: loss 0.0009054330876097083\n",
      "Iteration 5595: loss 0.000905433320440352\n",
      "Iteration 5596: loss 0.000905433320440352\n",
      "Iteration 5597: loss 0.0009054330876097083\n",
      "Iteration 5598: loss 0.000905433320440352\n",
      "Iteration 5599: loss 0.000905433320440352\n",
      "Iteration 5600: loss 0.0009054330876097083\n",
      "Iteration 5601: loss 0.000905433320440352\n",
      "Iteration 5602: loss 0.000905433320440352\n",
      "Iteration 5603: loss 0.0009054330876097083\n",
      "Iteration 5604: loss 0.000905433320440352\n",
      "Iteration 5605: loss 0.000905433320440352\n",
      "Iteration 5606: loss 0.0009054330876097083\n",
      "Iteration 5607: loss 0.000905433320440352\n",
      "Iteration 5608: loss 0.000905433320440352\n",
      "Iteration 5609: loss 0.0009054330876097083\n",
      "Iteration 5610: loss 0.000905433320440352\n",
      "Iteration 5611: loss 0.000905433320440352\n",
      "Iteration 5612: loss 0.0009054330876097083\n",
      "Iteration 5613: loss 0.000905433320440352\n",
      "Iteration 5614: loss 0.000905433320440352\n",
      "Iteration 5615: loss 0.0009054330876097083\n",
      "Iteration 5616: loss 0.000905433320440352\n",
      "Iteration 5617: loss 0.000905433320440352\n",
      "Iteration 5618: loss 0.0009054330876097083\n",
      "Iteration 5619: loss 0.000905433320440352\n",
      "Iteration 5620: loss 0.000905433320440352\n",
      "Iteration 5621: loss 0.0009054330876097083\n",
      "Iteration 5622: loss 0.000905433320440352\n",
      "Iteration 5623: loss 0.000905433320440352\n",
      "Iteration 5624: loss 0.0009054330876097083\n",
      "Iteration 5625: loss 0.000905433320440352\n",
      "Iteration 5626: loss 0.000905433320440352\n",
      "Iteration 5627: loss 0.0009054330876097083\n",
      "Iteration 5628: loss 0.000905433320440352\n",
      "Iteration 5629: loss 0.000905433320440352\n",
      "Iteration 5630: loss 0.0009054330876097083\n",
      "Iteration 5631: loss 0.000905433320440352\n",
      "Iteration 5632: loss 0.000905433320440352\n",
      "Iteration 5633: loss 0.0009054330876097083\n",
      "Iteration 5634: loss 0.000905433320440352\n",
      "Iteration 5635: loss 0.000905433320440352\n",
      "Iteration 5636: loss 0.0009054330876097083\n",
      "Iteration 5637: loss 0.000905433320440352\n",
      "Iteration 5638: loss 0.000905433320440352\n",
      "Iteration 5639: loss 0.0009054330876097083\n",
      "Iteration 5640: loss 0.000905433320440352\n",
      "Iteration 5641: loss 0.000905433320440352\n",
      "Iteration 5642: loss 0.0009054330876097083\n",
      "Iteration 5643: loss 0.000905433320440352\n",
      "Iteration 5644: loss 0.000905433320440352\n",
      "Iteration 5645: loss 0.0009054330876097083\n",
      "Iteration 5646: loss 0.000905433320440352\n",
      "Iteration 5647: loss 0.000905433320440352\n",
      "Iteration 5648: loss 0.0009054330876097083\n",
      "Iteration 5649: loss 0.000905433320440352\n",
      "Iteration 5650: loss 0.000905433320440352\n",
      "Iteration 5651: loss 0.0009054330876097083\n",
      "Iteration 5652: loss 0.000905433320440352\n",
      "Iteration 5653: loss 0.000905433320440352\n",
      "Iteration 5654: loss 0.0009054330876097083\n",
      "Iteration 5655: loss 0.000905433320440352\n",
      "Iteration 5656: loss 0.000905433320440352\n",
      "Iteration 5657: loss 0.0009054330876097083\n",
      "Iteration 5658: loss 0.000905433320440352\n",
      "Iteration 5659: loss 0.000905433320440352\n",
      "Iteration 5660: loss 0.0009054330876097083\n",
      "Iteration 5661: loss 0.000905433320440352\n",
      "Iteration 5662: loss 0.000905433320440352\n",
      "Iteration 5663: loss 0.0009054330876097083\n",
      "Iteration 5664: loss 0.000905433320440352\n",
      "Iteration 5665: loss 0.000905433320440352\n",
      "Iteration 5666: loss 0.0009054330876097083\n",
      "Iteration 5667: loss 0.000905433320440352\n",
      "Iteration 5668: loss 0.000905433320440352\n",
      "Iteration 5669: loss 0.0009054330876097083\n",
      "Iteration 5670: loss 0.000905433320440352\n",
      "Iteration 5671: loss 0.000905433320440352\n",
      "Iteration 5672: loss 0.0009054330876097083\n",
      "Iteration 5673: loss 0.000905433320440352\n",
      "Iteration 5674: loss 0.000905433320440352\n",
      "Iteration 5675: loss 0.0009054330876097083\n",
      "Iteration 5676: loss 0.000905433320440352\n",
      "Iteration 5677: loss 0.000905433320440352\n",
      "Iteration 5678: loss 0.0009054330876097083\n",
      "Iteration 5679: loss 0.000905433320440352\n",
      "Iteration 5680: loss 0.000905433320440352\n",
      "Iteration 5681: loss 0.0009054330876097083\n",
      "Iteration 5682: loss 0.000905433320440352\n",
      "Iteration 5683: loss 0.000905433320440352\n",
      "Iteration 5684: loss 0.0009054330876097083\n",
      "Iteration 5685: loss 0.000905433320440352\n",
      "Iteration 5686: loss 0.000905433320440352\n",
      "Iteration 5687: loss 0.0009054330876097083\n",
      "Iteration 5688: loss 0.000905433320440352\n",
      "Iteration 5689: loss 0.000905433320440352\n",
      "Iteration 5690: loss 0.0009054330876097083\n",
      "Iteration 5691: loss 0.000905433320440352\n",
      "Iteration 5692: loss 0.000905433320440352\n",
      "Iteration 5693: loss 0.0009054330876097083\n",
      "Iteration 5694: loss 0.000905433320440352\n",
      "Iteration 5695: loss 0.000905433320440352\n",
      "Iteration 5696: loss 0.0009054330876097083\n",
      "Iteration 5697: loss 0.000905433320440352\n",
      "Iteration 5698: loss 0.000905433320440352\n",
      "Iteration 5699: loss 0.0009054330876097083\n",
      "Iteration 5700: loss 0.000905433320440352\n",
      "Iteration 5701: loss 0.000905433320440352\n",
      "Iteration 5702: loss 0.0009054330876097083\n",
      "Iteration 5703: loss 0.000905433320440352\n",
      "Iteration 5704: loss 0.000905433320440352\n",
      "Iteration 5705: loss 0.0009054330876097083\n",
      "Iteration 5706: loss 0.000905433320440352\n",
      "Iteration 5707: loss 0.000905433320440352\n",
      "Iteration 5708: loss 0.0009054330876097083\n",
      "Iteration 5709: loss 0.000905433320440352\n",
      "Iteration 5710: loss 0.000905433320440352\n",
      "Iteration 5711: loss 0.0009054330876097083\n",
      "Iteration 5712: loss 0.000905433320440352\n",
      "Iteration 5713: loss 0.000905433320440352\n",
      "Iteration 5714: loss 0.0009054330876097083\n",
      "Iteration 5715: loss 0.000905433320440352\n",
      "Iteration 5716: loss 0.000905433320440352\n",
      "Iteration 5717: loss 0.0009054330876097083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5718: loss 0.000905433320440352\n",
      "Iteration 5719: loss 0.000905433320440352\n",
      "Iteration 5720: loss 0.0009054330876097083\n",
      "Iteration 5721: loss 0.000905433320440352\n",
      "Iteration 5722: loss 0.000905433320440352\n",
      "Iteration 5723: loss 0.0009054330876097083\n",
      "Iteration 5724: loss 0.000905433320440352\n",
      "Iteration 5725: loss 0.000905433320440352\n",
      "Iteration 5726: loss 0.0009054330876097083\n",
      "Iteration 5727: loss 0.000905433320440352\n",
      "Iteration 5728: loss 0.000905433320440352\n",
      "Iteration 5729: loss 0.0009054330876097083\n",
      "Iteration 5730: loss 0.000905433320440352\n",
      "Iteration 5731: loss 0.000905433320440352\n",
      "Iteration 5732: loss 0.0009054330876097083\n",
      "Iteration 5733: loss 0.000905433320440352\n",
      "Iteration 5734: loss 0.000905433320440352\n",
      "Iteration 5735: loss 0.0009054330876097083\n",
      "Iteration 5736: loss 0.000905433320440352\n",
      "Iteration 5737: loss 0.000905433320440352\n",
      "Iteration 5738: loss 0.0009054330876097083\n",
      "Iteration 5739: loss 0.000905433320440352\n",
      "Iteration 5740: loss 0.000905433320440352\n",
      "Iteration 5741: loss 0.0009054330876097083\n",
      "Iteration 5742: loss 0.000905433320440352\n",
      "Iteration 5743: loss 0.000905433320440352\n",
      "Iteration 5744: loss 0.0009054330876097083\n",
      "Iteration 5745: loss 0.000905433320440352\n",
      "Iteration 5746: loss 0.000905433320440352\n",
      "Iteration 5747: loss 0.0009054330876097083\n",
      "Iteration 5748: loss 0.000905433320440352\n",
      "Iteration 5749: loss 0.000905433320440352\n",
      "Iteration 5750: loss 0.0009054330876097083\n",
      "Iteration 5751: loss 0.000905433320440352\n",
      "Iteration 5752: loss 0.000905433320440352\n",
      "Iteration 5753: loss 0.0009054330876097083\n",
      "Iteration 5754: loss 0.000905433320440352\n",
      "Iteration 5755: loss 0.000905433320440352\n",
      "Iteration 5756: loss 0.0009054330876097083\n",
      "Iteration 5757: loss 0.000905433320440352\n",
      "Iteration 5758: loss 0.000905433320440352\n",
      "Iteration 5759: loss 0.0009054330876097083\n",
      "Iteration 5760: loss 0.000905433320440352\n",
      "Iteration 5761: loss 0.000905433320440352\n",
      "Iteration 5762: loss 0.0009054330876097083\n",
      "Iteration 5763: loss 0.000905433320440352\n",
      "Iteration 5764: loss 0.000905433320440352\n",
      "Iteration 5765: loss 0.0009054330876097083\n",
      "Iteration 5766: loss 0.000905433320440352\n",
      "Iteration 5767: loss 0.000905433320440352\n",
      "Iteration 5768: loss 0.0009054330876097083\n",
      "Iteration 5769: loss 0.000905433320440352\n",
      "Iteration 5770: loss 0.000905433320440352\n",
      "Iteration 5771: loss 0.0009054330876097083\n",
      "Iteration 5772: loss 0.000905433320440352\n",
      "Iteration 5773: loss 0.000905433320440352\n",
      "Iteration 5774: loss 0.0009054330876097083\n",
      "Iteration 5775: loss 0.000905433320440352\n",
      "Iteration 5776: loss 0.000905433320440352\n",
      "Iteration 5777: loss 0.0009054330876097083\n",
      "Iteration 5778: loss 0.000905433320440352\n",
      "Iteration 5779: loss 0.000905433320440352\n",
      "Iteration 5780: loss 0.0009054330876097083\n",
      "Iteration 5781: loss 0.000905433320440352\n",
      "Iteration 5782: loss 0.000905433320440352\n",
      "Iteration 5783: loss 0.0009054330876097083\n",
      "Iteration 5784: loss 0.000905433320440352\n",
      "Iteration 5785: loss 0.000905433320440352\n",
      "Iteration 5786: loss 0.0009054330876097083\n",
      "Iteration 5787: loss 0.000905433320440352\n",
      "Iteration 5788: loss 0.000905433320440352\n",
      "Iteration 5789: loss 0.0009054330876097083\n",
      "Iteration 5790: loss 0.000905433320440352\n",
      "Iteration 5791: loss 0.000905433320440352\n",
      "Iteration 5792: loss 0.0009054330876097083\n",
      "Iteration 5793: loss 0.000905433320440352\n",
      "Iteration 5794: loss 0.000905433320440352\n",
      "Iteration 5795: loss 0.0009054330876097083\n",
      "Iteration 5796: loss 0.000905433320440352\n",
      "Iteration 5797: loss 0.000905433320440352\n",
      "Iteration 5798: loss 0.0009054330876097083\n",
      "Iteration 5799: loss 0.000905433320440352\n",
      "Iteration 5800: loss 0.000905433320440352\n",
      "Iteration 5801: loss 0.0009054330876097083\n",
      "Iteration 5802: loss 0.000905433320440352\n",
      "Iteration 5803: loss 0.000905433320440352\n",
      "Iteration 5804: loss 0.0009054330876097083\n",
      "Iteration 5805: loss 0.000905433320440352\n",
      "Iteration 5806: loss 0.000905433320440352\n",
      "Iteration 5807: loss 0.0009054330876097083\n",
      "Iteration 5808: loss 0.000905433320440352\n",
      "Iteration 5809: loss 0.000905433320440352\n",
      "Iteration 5810: loss 0.0009054330876097083\n",
      "Iteration 5811: loss 0.000905433320440352\n",
      "Iteration 5812: loss 0.000905433320440352\n",
      "Iteration 5813: loss 0.0009054330876097083\n",
      "Iteration 5814: loss 0.000905433320440352\n",
      "Iteration 5815: loss 0.000905433320440352\n",
      "Iteration 5816: loss 0.0009054330876097083\n",
      "Iteration 5817: loss 0.000905433320440352\n",
      "Iteration 5818: loss 0.000905433320440352\n",
      "Iteration 5819: loss 0.0009054330876097083\n",
      "Iteration 5820: loss 0.000905433320440352\n",
      "Iteration 5821: loss 0.000905433320440352\n",
      "Iteration 5822: loss 0.0009054330876097083\n",
      "Iteration 5823: loss 0.000905433320440352\n",
      "Iteration 5824: loss 0.000905433320440352\n",
      "Iteration 5825: loss 0.0009054330876097083\n",
      "Iteration 5826: loss 0.000905433320440352\n",
      "Iteration 5827: loss 0.000905433320440352\n",
      "Iteration 5828: loss 0.0009054330876097083\n",
      "Iteration 5829: loss 0.000905433320440352\n",
      "Iteration 5830: loss 0.000905433320440352\n",
      "Iteration 5831: loss 0.0009054330876097083\n",
      "Iteration 5832: loss 0.000905433320440352\n",
      "Iteration 5833: loss 0.000905433320440352\n",
      "Iteration 5834: loss 0.0009054330876097083\n",
      "Iteration 5835: loss 0.000905433320440352\n",
      "Iteration 5836: loss 0.000905433320440352\n",
      "Iteration 5837: loss 0.0009054330876097083\n",
      "Iteration 5838: loss 0.000905433320440352\n",
      "Iteration 5839: loss 0.000905433320440352\n",
      "Iteration 5840: loss 0.0009054330876097083\n",
      "Iteration 5841: loss 0.000905433320440352\n",
      "Iteration 5842: loss 0.000905433320440352\n",
      "Iteration 5843: loss 0.0009054330876097083\n",
      "Iteration 5844: loss 0.000905433320440352\n",
      "Iteration 5845: loss 0.000905433320440352\n",
      "Iteration 5846: loss 0.0009054330876097083\n",
      "Iteration 5847: loss 0.000905433320440352\n",
      "Iteration 5848: loss 0.000905433320440352\n",
      "Iteration 5849: loss 0.0009054330876097083\n",
      "Iteration 5850: loss 0.000905433320440352\n",
      "Iteration 5851: loss 0.000905433320440352\n",
      "Iteration 5852: loss 0.0009054330876097083\n",
      "Iteration 5853: loss 0.000905433320440352\n",
      "Iteration 5854: loss 0.000905433320440352\n",
      "Iteration 5855: loss 0.0009054330876097083\n",
      "Iteration 5856: loss 0.000905433320440352\n",
      "Iteration 5857: loss 0.000905433320440352\n",
      "Iteration 5858: loss 0.0009054330876097083\n",
      "Iteration 5859: loss 0.000905433320440352\n",
      "Iteration 5860: loss 0.000905433320440352\n",
      "Iteration 5861: loss 0.0009054330876097083\n",
      "Iteration 5862: loss 0.000905433320440352\n",
      "Iteration 5863: loss 0.000905433320440352\n",
      "Iteration 5864: loss 0.0009054330876097083\n",
      "Iteration 5865: loss 0.000905433320440352\n",
      "Iteration 5866: loss 0.000905433320440352\n",
      "Iteration 5867: loss 0.0009054330876097083\n",
      "Iteration 5868: loss 0.000905433320440352\n",
      "Iteration 5869: loss 0.000905433320440352\n",
      "Iteration 5870: loss 0.0009054330876097083\n",
      "Iteration 5871: loss 0.000905433320440352\n",
      "Iteration 5872: loss 0.000905433320440352\n",
      "Iteration 5873: loss 0.0009054330876097083\n",
      "Iteration 5874: loss 0.000905433320440352\n",
      "Iteration 5875: loss 0.000905433320440352\n",
      "Iteration 5876: loss 0.0009054330876097083\n",
      "Iteration 5877: loss 0.000905433320440352\n",
      "Iteration 5878: loss 0.000905433320440352\n",
      "Iteration 5879: loss 0.0009054330876097083\n",
      "Iteration 5880: loss 0.000905433320440352\n",
      "Iteration 5881: loss 0.000905433320440352\n",
      "Iteration 5882: loss 0.0009054330876097083\n",
      "Iteration 5883: loss 0.000905433320440352\n",
      "Iteration 5884: loss 0.000905433320440352\n",
      "Iteration 5885: loss 0.0009054330876097083\n",
      "Iteration 5886: loss 0.000905433320440352\n",
      "Iteration 5887: loss 0.000905433320440352\n",
      "Iteration 5888: loss 0.0009054330876097083\n",
      "Iteration 5889: loss 0.000905433320440352\n",
      "Iteration 5890: loss 0.000905433320440352\n",
      "Iteration 5891: loss 0.0009054330876097083\n",
      "Iteration 5892: loss 0.000905433320440352\n",
      "Iteration 5893: loss 0.000905433320440352\n",
      "Iteration 5894: loss 0.0009054330876097083\n",
      "Iteration 5895: loss 0.000905433320440352\n",
      "Iteration 5896: loss 0.000905433320440352\n",
      "Iteration 5897: loss 0.0009054330876097083\n",
      "Iteration 5898: loss 0.000905433320440352\n",
      "Iteration 5899: loss 0.000905433320440352\n",
      "Iteration 5900: loss 0.0009054330876097083\n",
      "Iteration 5901: loss 0.000905433320440352\n",
      "Iteration 5902: loss 0.000905433320440352\n",
      "Iteration 5903: loss 0.0009054330876097083\n",
      "Iteration 5904: loss 0.000905433320440352\n",
      "Iteration 5905: loss 0.000905433320440352\n",
      "Iteration 5906: loss 0.0009054330876097083\n",
      "Iteration 5907: loss 0.000905433320440352\n",
      "Iteration 5908: loss 0.000905433320440352\n",
      "Iteration 5909: loss 0.0009054330876097083\n",
      "Iteration 5910: loss 0.000905433320440352\n",
      "Iteration 5911: loss 0.000905433320440352\n",
      "Iteration 5912: loss 0.0009054330876097083\n",
      "Iteration 5913: loss 0.000905433320440352\n",
      "Iteration 5914: loss 0.000905433320440352\n",
      "Iteration 5915: loss 0.0009054330876097083\n",
      "Iteration 5916: loss 0.000905433320440352\n",
      "Iteration 5917: loss 0.000905433320440352\n",
      "Iteration 5918: loss 0.0009054330876097083\n",
      "Iteration 5919: loss 0.000905433320440352\n",
      "Iteration 5920: loss 0.000905433320440352\n",
      "Iteration 5921: loss 0.0009054330876097083\n",
      "Iteration 5922: loss 0.000905433320440352\n",
      "Iteration 5923: loss 0.000905433320440352\n",
      "Iteration 5924: loss 0.0009054330876097083\n",
      "Iteration 5925: loss 0.000905433320440352\n",
      "Iteration 5926: loss 0.000905433320440352\n",
      "Iteration 5927: loss 0.0009054330876097083\n",
      "Iteration 5928: loss 0.000905433320440352\n",
      "Iteration 5929: loss 0.000905433320440352\n",
      "Iteration 5930: loss 0.0009054330876097083\n",
      "Iteration 5931: loss 0.000905433320440352\n",
      "Iteration 5932: loss 0.000905433320440352\n",
      "Iteration 5933: loss 0.0009054330876097083\n",
      "Iteration 5934: loss 0.000905433320440352\n",
      "Iteration 5935: loss 0.000905433320440352\n",
      "Iteration 5936: loss 0.0009054330876097083\n",
      "Iteration 5937: loss 0.000905433320440352\n",
      "Iteration 5938: loss 0.000905433320440352\n",
      "Iteration 5939: loss 0.0009054330876097083\n",
      "Iteration 5940: loss 0.000905433320440352\n",
      "Iteration 5941: loss 0.000905433320440352\n",
      "Iteration 5942: loss 0.0009054330876097083\n",
      "Iteration 5943: loss 0.000905433320440352\n",
      "Iteration 5944: loss 0.000905433320440352\n",
      "Iteration 5945: loss 0.0009054330876097083\n",
      "Iteration 5946: loss 0.000905433320440352\n",
      "Iteration 5947: loss 0.000905433320440352\n",
      "Iteration 5948: loss 0.0009054330876097083\n",
      "Iteration 5949: loss 0.000905433320440352\n",
      "Iteration 5950: loss 0.000905433320440352\n",
      "Iteration 5951: loss 0.0009054330876097083\n",
      "Iteration 5952: loss 0.000905433320440352\n",
      "Iteration 5953: loss 0.000905433320440352\n",
      "Iteration 5954: loss 0.0009054330876097083\n",
      "Iteration 5955: loss 0.000905433320440352\n",
      "Iteration 5956: loss 0.000905433320440352\n",
      "Iteration 5957: loss 0.0009054330876097083\n",
      "Iteration 5958: loss 0.000905433320440352\n",
      "Iteration 5959: loss 0.000905433320440352\n",
      "Iteration 5960: loss 0.0009054330876097083\n",
      "Iteration 5961: loss 0.000905433320440352\n",
      "Iteration 5962: loss 0.000905433320440352\n",
      "Iteration 5963: loss 0.0009054330876097083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5964: loss 0.000905433320440352\n",
      "Iteration 5965: loss 0.000905433320440352\n",
      "Iteration 5966: loss 0.0009054330876097083\n",
      "Iteration 5967: loss 0.000905433320440352\n",
      "Iteration 5968: loss 0.000905433320440352\n",
      "Iteration 5969: loss 0.0009054330876097083\n",
      "Iteration 5970: loss 0.000905433320440352\n",
      "Iteration 5971: loss 0.000905433320440352\n",
      "Iteration 5972: loss 0.0009054330876097083\n",
      "Iteration 5973: loss 0.000905433320440352\n",
      "Iteration 5974: loss 0.000905433320440352\n",
      "Iteration 5975: loss 0.0009054330876097083\n",
      "Iteration 5976: loss 0.000905433320440352\n",
      "Iteration 5977: loss 0.000905433320440352\n",
      "Iteration 5978: loss 0.0009054330876097083\n",
      "Iteration 5979: loss 0.000905433320440352\n",
      "Iteration 5980: loss 0.000905433320440352\n",
      "Iteration 5981: loss 0.0009054330876097083\n",
      "Iteration 5982: loss 0.000905433320440352\n",
      "Iteration 5983: loss 0.000905433320440352\n",
      "Iteration 5984: loss 0.0009054330876097083\n",
      "Iteration 5985: loss 0.000905433320440352\n",
      "Iteration 5986: loss 0.000905433320440352\n",
      "Iteration 5987: loss 0.0009054330876097083\n",
      "Iteration 5988: loss 0.000905433320440352\n",
      "Iteration 5989: loss 0.000905433320440352\n",
      "Iteration 5990: loss 0.0009054330876097083\n",
      "Iteration 5991: loss 0.000905433320440352\n",
      "Iteration 5992: loss 0.000905433320440352\n",
      "Iteration 5993: loss 0.0009054330876097083\n",
      "Iteration 5994: loss 0.000905433320440352\n",
      "Iteration 5995: loss 0.000905433320440352\n",
      "Iteration 5996: loss 0.0009054330876097083\n",
      "Iteration 5997: loss 0.000905433320440352\n",
      "Iteration 5998: loss 0.000905433320440352\n",
      "Iteration 5999: loss 0.0009054330876097083\n",
      "Iteration 6000: loss 0.000905433320440352\n",
      "Iteration 6001: loss 0.000905433320440352\n",
      "Iteration 6002: loss 0.0009054330876097083\n",
      "Iteration 6003: loss 0.000905433320440352\n",
      "Iteration 6004: loss 0.000905433320440352\n",
      "Iteration 6005: loss 0.0009054330876097083\n",
      "Iteration 6006: loss 0.000905433320440352\n",
      "Iteration 6007: loss 0.000905433320440352\n",
      "Iteration 6008: loss 0.0009054330876097083\n",
      "Iteration 6009: loss 0.000905433320440352\n",
      "Iteration 6010: loss 0.000905433320440352\n",
      "Iteration 6011: loss 0.0009054330876097083\n",
      "Iteration 6012: loss 0.000905433320440352\n",
      "Iteration 6013: loss 0.000905433320440352\n",
      "Iteration 6014: loss 0.0009054330876097083\n",
      "Iteration 6015: loss 0.000905433320440352\n",
      "Iteration 6016: loss 0.000905433320440352\n",
      "Iteration 6017: loss 0.0009054330876097083\n",
      "Iteration 6018: loss 0.000905433320440352\n",
      "Iteration 6019: loss 0.000905433320440352\n",
      "Iteration 6020: loss 0.0009054330876097083\n",
      "Iteration 6021: loss 0.000905433320440352\n",
      "Iteration 6022: loss 0.000905433320440352\n",
      "Iteration 6023: loss 0.0009054330876097083\n",
      "Iteration 6024: loss 0.000905433320440352\n",
      "Iteration 6025: loss 0.000905433320440352\n",
      "Iteration 6026: loss 0.0009054330876097083\n",
      "Iteration 6027: loss 0.000905433320440352\n",
      "Iteration 6028: loss 0.000905433320440352\n",
      "Iteration 6029: loss 0.0009054330876097083\n",
      "Iteration 6030: loss 0.000905433320440352\n",
      "Iteration 6031: loss 0.000905433320440352\n",
      "Iteration 6032: loss 0.0009054330876097083\n",
      "Iteration 6033: loss 0.000905433320440352\n",
      "Iteration 6034: loss 0.000905433320440352\n",
      "Iteration 6035: loss 0.0009054330876097083\n",
      "Iteration 6036: loss 0.000905433320440352\n",
      "Iteration 6037: loss 0.000905433320440352\n",
      "Iteration 6038: loss 0.0009054330876097083\n",
      "Iteration 6039: loss 0.000905433320440352\n",
      "Iteration 6040: loss 0.000905433320440352\n",
      "Iteration 6041: loss 0.0009054330876097083\n",
      "Iteration 6042: loss 0.000905433320440352\n",
      "Iteration 6043: loss 0.000905433320440352\n",
      "Iteration 6044: loss 0.0009054330876097083\n",
      "Iteration 6045: loss 0.000905433320440352\n",
      "Iteration 6046: loss 0.000905433320440352\n",
      "Iteration 6047: loss 0.0009054330876097083\n",
      "Iteration 6048: loss 0.000905433320440352\n",
      "Iteration 6049: loss 0.000905433320440352\n",
      "Iteration 6050: loss 0.0009054330876097083\n",
      "Iteration 6051: loss 0.000905433320440352\n",
      "Iteration 6052: loss 0.000905433320440352\n",
      "Iteration 6053: loss 0.0009054330876097083\n",
      "Iteration 6054: loss 0.000905433320440352\n",
      "Iteration 6055: loss 0.000905433320440352\n",
      "Iteration 6056: loss 0.0009054330876097083\n",
      "Iteration 6057: loss 0.000905433320440352\n",
      "Iteration 6058: loss 0.000905433320440352\n",
      "Iteration 6059: loss 0.0009054330876097083\n",
      "Iteration 6060: loss 0.000905433320440352\n",
      "Iteration 6061: loss 0.000905433320440352\n",
      "Iteration 6062: loss 0.0009054330876097083\n",
      "Iteration 6063: loss 0.000905433320440352\n",
      "Iteration 6064: loss 0.000905433320440352\n",
      "Iteration 6065: loss 0.0009054330876097083\n",
      "Iteration 6066: loss 0.000905433320440352\n",
      "Iteration 6067: loss 0.000905433320440352\n",
      "Iteration 6068: loss 0.0009054330876097083\n",
      "Iteration 6069: loss 0.000905433320440352\n",
      "Iteration 6070: loss 0.000905433320440352\n",
      "Iteration 6071: loss 0.0009054330876097083\n",
      "Iteration 6072: loss 0.000905433320440352\n",
      "Iteration 6073: loss 0.000905433320440352\n",
      "Iteration 6074: loss 0.0009054330876097083\n",
      "Iteration 6075: loss 0.000905433320440352\n",
      "Iteration 6076: loss 0.000905433320440352\n",
      "Iteration 6077: loss 0.0009054330876097083\n",
      "Iteration 6078: loss 0.000905433320440352\n",
      "Iteration 6079: loss 0.000905433320440352\n",
      "Iteration 6080: loss 0.0009054330876097083\n",
      "Iteration 6081: loss 0.000905433320440352\n",
      "Iteration 6082: loss 0.000905433320440352\n",
      "Iteration 6083: loss 0.0009054330876097083\n",
      "Iteration 6084: loss 0.000905433320440352\n",
      "Iteration 6085: loss 0.000905433320440352\n",
      "Iteration 6086: loss 0.0009054330876097083\n",
      "Iteration 6087: loss 0.000905433320440352\n",
      "Iteration 6088: loss 0.000905433320440352\n",
      "Iteration 6089: loss 0.0009054330876097083\n",
      "Iteration 6090: loss 0.000905433320440352\n",
      "Iteration 6091: loss 0.000905433320440352\n",
      "Iteration 6092: loss 0.0009054330876097083\n",
      "Iteration 6093: loss 0.000905433320440352\n",
      "Iteration 6094: loss 0.000905433320440352\n",
      "Iteration 6095: loss 0.0009054330876097083\n",
      "Iteration 6096: loss 0.000905433320440352\n",
      "Iteration 6097: loss 0.000905433320440352\n",
      "Iteration 6098: loss 0.0009054330876097083\n",
      "Iteration 6099: loss 0.000905433320440352\n",
      "Iteration 6100: loss 0.000905433320440352\n",
      "Iteration 6101: loss 0.0009054330876097083\n",
      "Iteration 6102: loss 0.000905433320440352\n",
      "Iteration 6103: loss 0.000905433320440352\n",
      "Iteration 6104: loss 0.0009054330876097083\n",
      "Iteration 6105: loss 0.000905433320440352\n",
      "Iteration 6106: loss 0.000905433320440352\n",
      "Iteration 6107: loss 0.0009054330876097083\n",
      "Iteration 6108: loss 0.000905433320440352\n",
      "Iteration 6109: loss 0.000905433320440352\n",
      "Iteration 6110: loss 0.0009054330876097083\n",
      "Iteration 6111: loss 0.000905433320440352\n",
      "Iteration 6112: loss 0.000905433320440352\n",
      "Iteration 6113: loss 0.0009054330876097083\n",
      "Iteration 6114: loss 0.000905433320440352\n",
      "Iteration 6115: loss 0.000905433320440352\n",
      "Iteration 6116: loss 0.0009054330876097083\n",
      "Iteration 6117: loss 0.000905433320440352\n",
      "Iteration 6118: loss 0.000905433320440352\n",
      "Iteration 6119: loss 0.0009054330876097083\n",
      "Iteration 6120: loss 0.000905433320440352\n",
      "Iteration 6121: loss 0.000905433320440352\n",
      "Iteration 6122: loss 0.0009054330876097083\n",
      "Iteration 6123: loss 0.000905433320440352\n",
      "Iteration 6124: loss 0.000905433320440352\n",
      "Iteration 6125: loss 0.0009054330876097083\n",
      "Iteration 6126: loss 0.000905433320440352\n",
      "Iteration 6127: loss 0.000905433320440352\n",
      "Iteration 6128: loss 0.0009054330876097083\n",
      "Iteration 6129: loss 0.000905433320440352\n",
      "Iteration 6130: loss 0.000905433320440352\n",
      "Iteration 6131: loss 0.0009054330876097083\n",
      "Iteration 6132: loss 0.000905433320440352\n",
      "Iteration 6133: loss 0.000905433320440352\n",
      "Iteration 6134: loss 0.0009054330876097083\n",
      "Iteration 6135: loss 0.000905433320440352\n",
      "Iteration 6136: loss 0.000905433320440352\n",
      "Iteration 6137: loss 0.0009054330876097083\n",
      "Iteration 6138: loss 0.000905433320440352\n",
      "Iteration 6139: loss 0.000905433320440352\n",
      "Iteration 6140: loss 0.0009054330876097083\n",
      "Iteration 6141: loss 0.000905433320440352\n",
      "Iteration 6142: loss 0.000905433320440352\n",
      "Iteration 6143: loss 0.0009054330876097083\n",
      "Iteration 6144: loss 0.000905433320440352\n",
      "Iteration 6145: loss 0.000905433320440352\n",
      "Iteration 6146: loss 0.0009054330876097083\n",
      "Iteration 6147: loss 0.000905433320440352\n",
      "Iteration 6148: loss 0.000905433320440352\n",
      "Iteration 6149: loss 0.0009054330876097083\n",
      "Iteration 6150: loss 0.000905433320440352\n",
      "Iteration 6151: loss 0.000905433320440352\n",
      "Iteration 6152: loss 0.0009054330876097083\n",
      "Iteration 6153: loss 0.000905433320440352\n",
      "Iteration 6154: loss 0.000905433320440352\n",
      "Iteration 6155: loss 0.0009054330876097083\n",
      "Iteration 6156: loss 0.000905433320440352\n",
      "Iteration 6157: loss 0.000905433320440352\n",
      "Iteration 6158: loss 0.0009054330876097083\n",
      "Iteration 6159: loss 0.000905433320440352\n",
      "Iteration 6160: loss 0.000905433320440352\n",
      "Iteration 6161: loss 0.0009054330876097083\n",
      "Iteration 6162: loss 0.000905433320440352\n",
      "Iteration 6163: loss 0.000905433320440352\n",
      "Iteration 6164: loss 0.0009054330876097083\n",
      "Iteration 6165: loss 0.000905433320440352\n",
      "Iteration 6166: loss 0.000905433320440352\n",
      "Iteration 6167: loss 0.0009054330876097083\n",
      "Iteration 6168: loss 0.000905433320440352\n",
      "Iteration 6169: loss 0.000905433320440352\n",
      "Iteration 6170: loss 0.0009054330876097083\n",
      "Iteration 6171: loss 0.000905433320440352\n",
      "Iteration 6172: loss 0.000905433320440352\n",
      "Iteration 6173: loss 0.0009054330876097083\n",
      "Iteration 6174: loss 0.000905433320440352\n",
      "Iteration 6175: loss 0.000905433320440352\n",
      "Iteration 6176: loss 0.0009054330876097083\n",
      "Iteration 6177: loss 0.000905433320440352\n",
      "Iteration 6178: loss 0.000905433320440352\n",
      "Iteration 6179: loss 0.0009054330876097083\n",
      "Iteration 6180: loss 0.000905433320440352\n",
      "Iteration 6181: loss 0.000905433320440352\n",
      "Iteration 6182: loss 0.0009054330876097083\n",
      "Iteration 6183: loss 0.000905433320440352\n",
      "Iteration 6184: loss 0.000905433320440352\n",
      "Iteration 6185: loss 0.0009054330876097083\n",
      "Iteration 6186: loss 0.000905433320440352\n",
      "Iteration 6187: loss 0.000905433320440352\n",
      "Iteration 6188: loss 0.0009054330876097083\n",
      "Iteration 6189: loss 0.000905433320440352\n",
      "Iteration 6190: loss 0.000905433320440352\n",
      "Iteration 6191: loss 0.0009054330876097083\n",
      "Iteration 6192: loss 0.000905433320440352\n",
      "Iteration 6193: loss 0.000905433320440352\n",
      "Iteration 6194: loss 0.0009054330876097083\n",
      "Iteration 6195: loss 0.000905433320440352\n",
      "Iteration 6196: loss 0.000905433320440352\n",
      "Iteration 6197: loss 0.0009054330876097083\n",
      "Iteration 6198: loss 0.000905433320440352\n",
      "Iteration 6199: loss 0.000905433320440352\n",
      "Iteration 6200: loss 0.0009054330876097083\n",
      "Iteration 6201: loss 0.000905433320440352\n",
      "Iteration 6202: loss 0.000905433320440352\n",
      "Iteration 6203: loss 0.0009054330876097083\n",
      "Iteration 6204: loss 0.000905433320440352\n",
      "Iteration 6205: loss 0.000905433320440352\n",
      "Iteration 6206: loss 0.0009054330876097083\n",
      "Iteration 6207: loss 0.000905433320440352\n",
      "Iteration 6208: loss 0.000905433320440352\n",
      "Iteration 6209: loss 0.0009054330876097083\n",
      "Iteration 6210: loss 0.000905433320440352\n",
      "Iteration 6211: loss 0.000905433320440352\n",
      "Iteration 6212: loss 0.0009054330876097083\n",
      "Iteration 6213: loss 0.000905433320440352\n",
      "Iteration 6214: loss 0.000905433320440352\n",
      "Iteration 6215: loss 0.0009054330876097083\n",
      "Iteration 6216: loss 0.000905433320440352\n",
      "Iteration 6217: loss 0.000905433320440352\n",
      "Iteration 6218: loss 0.0009054330876097083\n",
      "Iteration 6219: loss 0.000905433320440352\n",
      "Iteration 6220: loss 0.000905433320440352\n",
      "Iteration 6221: loss 0.0009054330876097083\n",
      "Iteration 6222: loss 0.000905433320440352\n",
      "Iteration 6223: loss 0.000905433320440352\n",
      "Iteration 6224: loss 0.0009054330876097083\n",
      "Iteration 6225: loss 0.000905433320440352\n",
      "Iteration 6226: loss 0.000905433320440352\n",
      "Iteration 6227: loss 0.0009054330876097083\n",
      "Iteration 6228: loss 0.000905433320440352\n",
      "Iteration 6229: loss 0.000905433320440352\n",
      "Iteration 6230: loss 0.0009054330876097083\n",
      "Iteration 6231: loss 0.000905433320440352\n",
      "Iteration 6232: loss 0.000905433320440352\n",
      "Iteration 6233: loss 0.0009054330876097083\n",
      "Iteration 6234: loss 0.000905433320440352\n",
      "Iteration 6235: loss 0.000905433320440352\n",
      "Iteration 6236: loss 0.0009054330876097083\n",
      "Iteration 6237: loss 0.000905433320440352\n",
      "Iteration 6238: loss 0.000905433320440352\n",
      "Iteration 6239: loss 0.0009054330876097083\n",
      "Iteration 6240: loss 0.000905433320440352\n",
      "Iteration 6241: loss 0.000905433320440352\n",
      "Iteration 6242: loss 0.0009054330876097083\n",
      "Iteration 6243: loss 0.000905433320440352\n",
      "Iteration 6244: loss 0.000905433320440352\n",
      "Iteration 6245: loss 0.0009054330876097083\n",
      "Iteration 6246: loss 0.000905433320440352\n",
      "Iteration 6247: loss 0.000905433320440352\n",
      "Iteration 6248: loss 0.0009054330876097083\n",
      "Iteration 6249: loss 0.000905433320440352\n",
      "Iteration 6250: loss 0.000905433320440352\n",
      "Iteration 6251: loss 0.0009054330876097083\n",
      "Iteration 6252: loss 0.000905433320440352\n",
      "Iteration 6253: loss 0.000905433320440352\n",
      "Iteration 6254: loss 0.0009054330876097083\n",
      "Iteration 6255: loss 0.000905433320440352\n",
      "Iteration 6256: loss 0.000905433320440352\n",
      "Iteration 6257: loss 0.0009054330876097083\n",
      "Iteration 6258: loss 0.000905433320440352\n",
      "Iteration 6259: loss 0.000905433320440352\n",
      "Iteration 6260: loss 0.0009054330876097083\n",
      "Iteration 6261: loss 0.000905433320440352\n",
      "Iteration 6262: loss 0.000905433320440352\n",
      "Iteration 6263: loss 0.0009054330876097083\n",
      "Iteration 6264: loss 0.000905433320440352\n",
      "Iteration 6265: loss 0.000905433320440352\n",
      "Iteration 6266: loss 0.0009054330876097083\n",
      "Iteration 6267: loss 0.000905433320440352\n",
      "Iteration 6268: loss 0.000905433320440352\n",
      "Iteration 6269: loss 0.0009054330876097083\n",
      "Iteration 6270: loss 0.000905433320440352\n",
      "Iteration 6271: loss 0.000905433320440352\n",
      "Iteration 6272: loss 0.0009054330876097083\n",
      "Iteration 6273: loss 0.000905433320440352\n",
      "Iteration 6274: loss 0.000905433320440352\n",
      "Iteration 6275: loss 0.0009054330876097083\n",
      "Iteration 6276: loss 0.000905433320440352\n",
      "Iteration 6277: loss 0.000905433320440352\n",
      "Iteration 6278: loss 0.0009054330876097083\n",
      "Iteration 6279: loss 0.000905433320440352\n",
      "Iteration 6280: loss 0.000905433320440352\n",
      "Iteration 6281: loss 0.0009054330876097083\n",
      "Iteration 6282: loss 0.000905433320440352\n",
      "Iteration 6283: loss 0.000905433320440352\n",
      "Iteration 6284: loss 0.0009054330876097083\n",
      "Iteration 6285: loss 0.000905433320440352\n",
      "Iteration 6286: loss 0.000905433320440352\n",
      "Iteration 6287: loss 0.0009054330876097083\n",
      "Iteration 6288: loss 0.000905433320440352\n",
      "Iteration 6289: loss 0.000905433320440352\n",
      "Iteration 6290: loss 0.0009054330876097083\n",
      "Iteration 6291: loss 0.000905433320440352\n",
      "Iteration 6292: loss 0.000905433320440352\n",
      "Iteration 6293: loss 0.0009054330876097083\n",
      "Iteration 6294: loss 0.000905433320440352\n",
      "Iteration 6295: loss 0.000905433320440352\n",
      "Iteration 6296: loss 0.0009054330876097083\n",
      "Iteration 6297: loss 0.000905433320440352\n",
      "Iteration 6298: loss 0.000905433320440352\n",
      "Iteration 6299: loss 0.0009054330876097083\n",
      "Iteration 6300: loss 0.000905433320440352\n",
      "Iteration 6301: loss 0.000905433320440352\n",
      "Iteration 6302: loss 0.0009054330876097083\n",
      "Iteration 6303: loss 0.000905433320440352\n",
      "Iteration 6304: loss 0.000905433320440352\n",
      "Iteration 6305: loss 0.0009054330876097083\n",
      "Iteration 6306: loss 0.000905433320440352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6307: loss 0.000905433320440352\n",
      "Iteration 6308: loss 0.0009054330876097083\n",
      "Iteration 6309: loss 0.000905433320440352\n",
      "Iteration 6310: loss 0.000905433320440352\n",
      "Iteration 6311: loss 0.0009054330876097083\n",
      "Iteration 6312: loss 0.000905433320440352\n",
      "Iteration 6313: loss 0.000905433320440352\n",
      "Iteration 6314: loss 0.0009054330876097083\n",
      "Iteration 6315: loss 0.000905433320440352\n",
      "Iteration 6316: loss 0.000905433320440352\n",
      "Iteration 6317: loss 0.0009054330876097083\n",
      "Iteration 6318: loss 0.000905433320440352\n",
      "Iteration 6319: loss 0.000905433320440352\n",
      "Iteration 6320: loss 0.0009054330876097083\n",
      "Iteration 6321: loss 0.000905433320440352\n",
      "Iteration 6322: loss 0.000905433320440352\n",
      "Iteration 6323: loss 0.0009054330876097083\n",
      "Iteration 6324: loss 0.000905433320440352\n",
      "Iteration 6325: loss 0.000905433320440352\n",
      "Iteration 6326: loss 0.0009054330876097083\n",
      "Iteration 6327: loss 0.000905433320440352\n",
      "Iteration 6328: loss 0.000905433320440352\n",
      "Iteration 6329: loss 0.0009054330876097083\n",
      "Iteration 6330: loss 0.000905433320440352\n",
      "Iteration 6331: loss 0.000905433320440352\n",
      "Iteration 6332: loss 0.0009054330876097083\n",
      "Iteration 6333: loss 0.000905433320440352\n",
      "Iteration 6334: loss 0.000905433320440352\n",
      "Iteration 6335: loss 0.0009054330876097083\n",
      "Iteration 6336: loss 0.000905433320440352\n",
      "Iteration 6337: loss 0.000905433320440352\n",
      "Iteration 6338: loss 0.0009054330876097083\n",
      "Iteration 6339: loss 0.000905433320440352\n",
      "Iteration 6340: loss 0.000905433320440352\n",
      "Iteration 6341: loss 0.0009054330876097083\n",
      "Iteration 6342: loss 0.000905433320440352\n",
      "Iteration 6343: loss 0.000905433320440352\n",
      "Iteration 6344: loss 0.0009054330876097083\n",
      "Iteration 6345: loss 0.000905433320440352\n",
      "Iteration 6346: loss 0.000905433320440352\n",
      "Iteration 6347: loss 0.0009054330876097083\n",
      "Iteration 6348: loss 0.000905433320440352\n",
      "Iteration 6349: loss 0.000905433320440352\n",
      "Iteration 6350: loss 0.0009054330876097083\n",
      "Iteration 6351: loss 0.000905433320440352\n",
      "Iteration 6352: loss 0.000905433320440352\n",
      "Iteration 6353: loss 0.0009054330876097083\n",
      "Iteration 6354: loss 0.000905433320440352\n",
      "Iteration 6355: loss 0.000905433320440352\n",
      "Iteration 6356: loss 0.0009054330876097083\n",
      "Iteration 6357: loss 0.000905433320440352\n",
      "Iteration 6358: loss 0.000905433320440352\n",
      "Iteration 6359: loss 0.0009054330876097083\n",
      "Iteration 6360: loss 0.000905433320440352\n",
      "Iteration 6361: loss 0.000905433320440352\n",
      "Iteration 6362: loss 0.0009054330876097083\n",
      "Iteration 6363: loss 0.000905433320440352\n",
      "Iteration 6364: loss 0.000905433320440352\n",
      "Iteration 6365: loss 0.0009054330876097083\n",
      "Iteration 6366: loss 0.000905433320440352\n",
      "Iteration 6367: loss 0.000905433320440352\n",
      "Iteration 6368: loss 0.0009054330876097083\n",
      "Iteration 6369: loss 0.000905433320440352\n",
      "Iteration 6370: loss 0.000905433320440352\n",
      "Iteration 6371: loss 0.0009054330876097083\n",
      "Iteration 6372: loss 0.000905433320440352\n",
      "Iteration 6373: loss 0.000905433320440352\n",
      "Iteration 6374: loss 0.0009054330876097083\n",
      "Iteration 6375: loss 0.000905433320440352\n",
      "Iteration 6376: loss 0.000905433320440352\n",
      "Iteration 6377: loss 0.0009054330876097083\n",
      "Iteration 6378: loss 0.000905433320440352\n",
      "Iteration 6379: loss 0.000905433320440352\n",
      "Iteration 6380: loss 0.0009054330876097083\n",
      "Iteration 6381: loss 0.000905433320440352\n",
      "Iteration 6382: loss 0.000905433320440352\n",
      "Iteration 6383: loss 0.0009054330876097083\n",
      "Iteration 6384: loss 0.000905433320440352\n",
      "Iteration 6385: loss 0.000905433320440352\n",
      "Iteration 6386: loss 0.0009054330876097083\n",
      "Iteration 6387: loss 0.000905433320440352\n",
      "Iteration 6388: loss 0.000905433320440352\n",
      "Iteration 6389: loss 0.0009054330876097083\n",
      "Iteration 6390: loss 0.000905433320440352\n",
      "Iteration 6391: loss 0.000905433320440352\n",
      "Iteration 6392: loss 0.0009054330876097083\n",
      "Iteration 6393: loss 0.000905433320440352\n",
      "Iteration 6394: loss 0.000905433320440352\n",
      "Iteration 6395: loss 0.0009054330876097083\n",
      "Iteration 6396: loss 0.000905433320440352\n",
      "Iteration 6397: loss 0.000905433320440352\n",
      "Iteration 6398: loss 0.0009054330876097083\n",
      "Iteration 6399: loss 0.000905433320440352\n",
      "Iteration 6400: loss 0.000905433320440352\n",
      "Iteration 6401: loss 0.0009054330876097083\n",
      "Iteration 6402: loss 0.000905433320440352\n",
      "Iteration 6403: loss 0.000905433320440352\n",
      "Iteration 6404: loss 0.0009054330876097083\n",
      "Iteration 6405: loss 0.000905433320440352\n",
      "Iteration 6406: loss 0.000905433320440352\n",
      "Iteration 6407: loss 0.0009054330876097083\n",
      "Iteration 6408: loss 0.000905433320440352\n",
      "Iteration 6409: loss 0.000905433320440352\n",
      "Iteration 6410: loss 0.0009054330876097083\n",
      "Iteration 6411: loss 0.000905433320440352\n",
      "Iteration 6412: loss 0.000905433320440352\n",
      "Iteration 6413: loss 0.0009054330876097083\n",
      "Iteration 6414: loss 0.000905433320440352\n",
      "Iteration 6415: loss 0.000905433320440352\n",
      "Iteration 6416: loss 0.0009054330876097083\n",
      "Iteration 6417: loss 0.000905433320440352\n",
      "Iteration 6418: loss 0.000905433320440352\n",
      "Iteration 6419: loss 0.0009054330876097083\n",
      "Iteration 6420: loss 0.000905433320440352\n",
      "Iteration 6421: loss 0.000905433320440352\n",
      "Iteration 6422: loss 0.0009054330876097083\n",
      "Iteration 6423: loss 0.000905433320440352\n",
      "Iteration 6424: loss 0.000905433320440352\n",
      "Iteration 6425: loss 0.0009054330876097083\n",
      "Iteration 6426: loss 0.000905433320440352\n",
      "Iteration 6427: loss 0.000905433320440352\n",
      "Iteration 6428: loss 0.0009054330876097083\n",
      "Iteration 6429: loss 0.000905433320440352\n",
      "Iteration 6430: loss 0.000905433320440352\n",
      "Iteration 6431: loss 0.0009054330876097083\n",
      "Iteration 6432: loss 0.000905433320440352\n",
      "Iteration 6433: loss 0.000905433320440352\n",
      "Iteration 6434: loss 0.0009054330876097083\n",
      "Iteration 6435: loss 0.000905433320440352\n",
      "Iteration 6436: loss 0.000905433320440352\n",
      "Iteration 6437: loss 0.0009054330876097083\n",
      "Iteration 6438: loss 0.000905433320440352\n",
      "Iteration 6439: loss 0.000905433320440352\n",
      "Iteration 6440: loss 0.0009054330876097083\n",
      "Iteration 6441: loss 0.000905433320440352\n",
      "Iteration 6442: loss 0.000905433320440352\n",
      "Iteration 6443: loss 0.0009054330876097083\n",
      "Iteration 6444: loss 0.000905433320440352\n",
      "Iteration 6445: loss 0.000905433320440352\n",
      "Iteration 6446: loss 0.0009054330876097083\n",
      "Iteration 6447: loss 0.000905433320440352\n",
      "Iteration 6448: loss 0.000905433320440352\n",
      "Iteration 6449: loss 0.0009054330876097083\n",
      "Iteration 6450: loss 0.000905433320440352\n",
      "Iteration 6451: loss 0.000905433320440352\n",
      "Iteration 6452: loss 0.0009054330876097083\n",
      "Iteration 6453: loss 0.000905433320440352\n",
      "Iteration 6454: loss 0.000905433320440352\n",
      "Iteration 6455: loss 0.0009054330876097083\n",
      "Iteration 6456: loss 0.000905433320440352\n",
      "Iteration 6457: loss 0.000905433320440352\n",
      "Iteration 6458: loss 0.0009054330876097083\n",
      "Iteration 6459: loss 0.000905433320440352\n",
      "Iteration 6460: loss 0.000905433320440352\n",
      "Iteration 6461: loss 0.0009054330876097083\n",
      "Iteration 6462: loss 0.000905433320440352\n",
      "Iteration 6463: loss 0.000905433320440352\n",
      "Iteration 6464: loss 0.0009054330876097083\n",
      "Iteration 6465: loss 0.000905433320440352\n",
      "Iteration 6466: loss 0.000905433320440352\n",
      "Iteration 6467: loss 0.0009054330876097083\n",
      "Iteration 6468: loss 0.000905433320440352\n",
      "Iteration 6469: loss 0.000905433320440352\n",
      "Iteration 6470: loss 0.0009054330876097083\n",
      "Iteration 6471: loss 0.000905433320440352\n",
      "Iteration 6472: loss 0.000905433320440352\n",
      "Iteration 6473: loss 0.0009054330876097083\n",
      "Iteration 6474: loss 0.000905433320440352\n",
      "Iteration 6475: loss 0.000905433320440352\n",
      "Iteration 6476: loss 0.0009054330876097083\n",
      "Iteration 6477: loss 0.000905433320440352\n",
      "Iteration 6478: loss 0.000905433320440352\n",
      "Iteration 6479: loss 0.0009054330876097083\n",
      "Iteration 6480: loss 0.000905433320440352\n",
      "Iteration 6481: loss 0.000905433320440352\n",
      "Iteration 6482: loss 0.0009054330876097083\n",
      "Iteration 6483: loss 0.000905433320440352\n",
      "Iteration 6484: loss 0.000905433320440352\n",
      "Iteration 6485: loss 0.0009054330876097083\n",
      "Iteration 6486: loss 0.000905433320440352\n",
      "Iteration 6487: loss 0.000905433320440352\n",
      "Iteration 6488: loss 0.0009054330876097083\n",
      "Iteration 6489: loss 0.000905433320440352\n",
      "Iteration 6490: loss 0.000905433320440352\n",
      "Iteration 6491: loss 0.0009054330876097083\n",
      "Iteration 6492: loss 0.000905433320440352\n",
      "Iteration 6493: loss 0.000905433320440352\n",
      "Iteration 6494: loss 0.0009054330876097083\n",
      "Iteration 6495: loss 0.000905433320440352\n",
      "Iteration 6496: loss 0.000905433320440352\n",
      "Iteration 6497: loss 0.0009054330876097083\n",
      "Iteration 6498: loss 0.000905433320440352\n",
      "Iteration 6499: loss 0.000905433320440352\n",
      "Iteration 6500: loss 0.0009054330876097083\n",
      "Iteration 6501: loss 0.000905433320440352\n",
      "Iteration 6502: loss 0.000905433320440352\n",
      "Iteration 6503: loss 0.0009054330876097083\n",
      "Iteration 6504: loss 0.000905433320440352\n",
      "Iteration 6505: loss 0.000905433320440352\n",
      "Iteration 6506: loss 0.0009054330876097083\n",
      "Iteration 6507: loss 0.000905433320440352\n",
      "Iteration 6508: loss 0.000905433320440352\n",
      "Iteration 6509: loss 0.0009054330876097083\n",
      "Iteration 6510: loss 0.000905433320440352\n",
      "Iteration 6511: loss 0.000905433320440352\n",
      "Iteration 6512: loss 0.0009054330876097083\n",
      "Iteration 6513: loss 0.000905433320440352\n",
      "Iteration 6514: loss 0.000905433320440352\n",
      "Iteration 6515: loss 0.0009054330876097083\n",
      "Iteration 6516: loss 0.000905433320440352\n",
      "Iteration 6517: loss 0.000905433320440352\n",
      "Iteration 6518: loss 0.0009054330876097083\n",
      "Iteration 6519: loss 0.000905433320440352\n",
      "Iteration 6520: loss 0.000905433320440352\n",
      "Iteration 6521: loss 0.0009054330876097083\n",
      "Iteration 6522: loss 0.000905433320440352\n",
      "Iteration 6523: loss 0.000905433320440352\n",
      "Iteration 6524: loss 0.0009054330876097083\n",
      "Iteration 6525: loss 0.000905433320440352\n",
      "Iteration 6526: loss 0.000905433320440352\n",
      "Iteration 6527: loss 0.0009054330876097083\n",
      "Iteration 6528: loss 0.000905433320440352\n",
      "Iteration 6529: loss 0.000905433320440352\n",
      "Iteration 6530: loss 0.0009054330876097083\n",
      "Iteration 6531: loss 0.000905433320440352\n",
      "Iteration 6532: loss 0.000905433320440352\n",
      "Iteration 6533: loss 0.0009054330876097083\n",
      "Iteration 6534: loss 0.000905433320440352\n",
      "Iteration 6535: loss 0.000905433320440352\n",
      "Iteration 6536: loss 0.0009054330876097083\n",
      "Iteration 6537: loss 0.000905433320440352\n",
      "Iteration 6538: loss 0.000905433320440352\n",
      "Iteration 6539: loss 0.0009054330876097083\n",
      "Iteration 6540: loss 0.000905433320440352\n",
      "Iteration 6541: loss 0.000905433320440352\n",
      "Iteration 6542: loss 0.0009054330876097083\n",
      "Iteration 6543: loss 0.000905433320440352\n",
      "Iteration 6544: loss 0.000905433320440352\n",
      "Iteration 6545: loss 0.0009054330876097083\n",
      "Iteration 6546: loss 0.000905433320440352\n",
      "Iteration 6547: loss 0.000905433320440352\n",
      "Iteration 6548: loss 0.0009054330876097083\n",
      "Iteration 6549: loss 0.000905433320440352\n",
      "Iteration 6550: loss 0.000905433320440352\n",
      "Iteration 6551: loss 0.0009054330876097083\n",
      "Iteration 6552: loss 0.000905433320440352\n",
      "Iteration 6553: loss 0.000905433320440352\n",
      "Iteration 6554: loss 0.0009054330876097083\n",
      "Iteration 6555: loss 0.000905433320440352\n",
      "Iteration 6556: loss 0.000905433320440352\n",
      "Iteration 6557: loss 0.0009054330876097083\n",
      "Iteration 6558: loss 0.000905433320440352\n",
      "Iteration 6559: loss 0.000905433320440352\n",
      "Iteration 6560: loss 0.0009054330876097083\n",
      "Iteration 6561: loss 0.000905433320440352\n",
      "Iteration 6562: loss 0.000905433320440352\n",
      "Iteration 6563: loss 0.0009054330876097083\n",
      "Iteration 6564: loss 0.000905433320440352\n",
      "Iteration 6565: loss 0.000905433320440352\n",
      "Iteration 6566: loss 0.0009054330876097083\n",
      "Iteration 6567: loss 0.000905433320440352\n",
      "Iteration 6568: loss 0.000905433320440352\n",
      "Iteration 6569: loss 0.0009054330876097083\n",
      "Iteration 6570: loss 0.000905433320440352\n",
      "Iteration 6571: loss 0.000905433320440352\n",
      "Iteration 6572: loss 0.0009054330876097083\n",
      "Iteration 6573: loss 0.000905433320440352\n",
      "Iteration 6574: loss 0.000905433320440352\n",
      "Iteration 6575: loss 0.0009054330876097083\n",
      "Iteration 6576: loss 0.000905433320440352\n",
      "Iteration 6577: loss 0.000905433320440352\n",
      "Iteration 6578: loss 0.0009054330876097083\n",
      "Iteration 6579: loss 0.000905433320440352\n",
      "Iteration 6580: loss 0.000905433320440352\n",
      "Iteration 6581: loss 0.0009054330876097083\n",
      "Iteration 6582: loss 0.000905433320440352\n",
      "Iteration 6583: loss 0.000905433320440352\n",
      "Iteration 6584: loss 0.0009054330876097083\n",
      "Iteration 6585: loss 0.000905433320440352\n",
      "Iteration 6586: loss 0.000905433320440352\n",
      "Iteration 6587: loss 0.0009054330876097083\n",
      "Iteration 6588: loss 0.000905433320440352\n",
      "Iteration 6589: loss 0.000905433320440352\n",
      "Iteration 6590: loss 0.0009054330876097083\n",
      "Iteration 6591: loss 0.000905433320440352\n",
      "Iteration 6592: loss 0.000905433320440352\n",
      "Iteration 6593: loss 0.0009054330876097083\n",
      "Iteration 6594: loss 0.000905433320440352\n",
      "Iteration 6595: loss 0.000905433320440352\n",
      "Iteration 6596: loss 0.0009054330876097083\n",
      "Iteration 6597: loss 0.000905433320440352\n",
      "Iteration 6598: loss 0.000905433320440352\n",
      "Iteration 6599: loss 0.0009054330876097083\n",
      "Iteration 6600: loss 0.000905433320440352\n",
      "Iteration 6601: loss 0.000905433320440352\n",
      "Iteration 6602: loss 0.0009054330876097083\n",
      "Iteration 6603: loss 0.000905433320440352\n",
      "Iteration 6604: loss 0.000905433320440352\n",
      "Iteration 6605: loss 0.0009054330876097083\n",
      "Iteration 6606: loss 0.000905433320440352\n",
      "Iteration 6607: loss 0.000905433320440352\n",
      "Iteration 6608: loss 0.0009054330876097083\n",
      "Iteration 6609: loss 0.000905433320440352\n",
      "Iteration 6610: loss 0.000905433320440352\n",
      "Iteration 6611: loss 0.0009054330876097083\n",
      "Iteration 6612: loss 0.000905433320440352\n",
      "Iteration 6613: loss 0.000905433320440352\n",
      "Iteration 6614: loss 0.0009054330876097083\n",
      "Iteration 6615: loss 0.000905433320440352\n",
      "Iteration 6616: loss 0.000905433320440352\n",
      "Iteration 6617: loss 0.0009054330876097083\n",
      "Iteration 6618: loss 0.000905433320440352\n",
      "Iteration 6619: loss 0.000905433320440352\n",
      "Iteration 6620: loss 0.0009054330876097083\n",
      "Iteration 6621: loss 0.000905433320440352\n",
      "Iteration 6622: loss 0.000905433320440352\n",
      "Iteration 6623: loss 0.0009054330876097083\n",
      "Iteration 6624: loss 0.000905433320440352\n",
      "Iteration 6625: loss 0.000905433320440352\n",
      "Iteration 6626: loss 0.0009054330876097083\n",
      "Iteration 6627: loss 0.000905433320440352\n",
      "Iteration 6628: loss 0.000905433320440352\n",
      "Iteration 6629: loss 0.0009054330876097083\n",
      "Iteration 6630: loss 0.000905433320440352\n",
      "Iteration 6631: loss 0.000905433320440352\n",
      "Iteration 6632: loss 0.0009054330876097083\n",
      "Iteration 6633: loss 0.000905433320440352\n",
      "Iteration 6634: loss 0.000905433320440352\n",
      "Iteration 6635: loss 0.0009054330876097083\n",
      "Iteration 6636: loss 0.000905433320440352\n",
      "Iteration 6637: loss 0.000905433320440352\n",
      "Iteration 6638: loss 0.0009054330876097083\n",
      "Iteration 6639: loss 0.000905433320440352\n",
      "Iteration 6640: loss 0.000905433320440352\n",
      "Iteration 6641: loss 0.0009054330876097083\n",
      "Iteration 6642: loss 0.000905433320440352\n",
      "Iteration 6643: loss 0.000905433320440352\n",
      "Iteration 6644: loss 0.0009054330876097083\n",
      "Iteration 6645: loss 0.000905433320440352\n",
      "Iteration 6646: loss 0.000905433320440352\n",
      "Iteration 6647: loss 0.0009054330876097083\n",
      "Iteration 6648: loss 0.000905433320440352\n",
      "Iteration 6649: loss 0.000905433320440352\n",
      "Iteration 6650: loss 0.0009054330876097083\n",
      "Iteration 6651: loss 0.000905433320440352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6652: loss 0.000905433320440352\n",
      "Iteration 6653: loss 0.0009054330876097083\n",
      "Iteration 6654: loss 0.000905433320440352\n",
      "Iteration 6655: loss 0.000905433320440352\n",
      "Iteration 6656: loss 0.0009054330876097083\n",
      "Iteration 6657: loss 0.000905433320440352\n",
      "Iteration 6658: loss 0.000905433320440352\n",
      "Iteration 6659: loss 0.0009054330876097083\n",
      "Iteration 6660: loss 0.000905433320440352\n",
      "Iteration 6661: loss 0.000905433320440352\n",
      "Iteration 6662: loss 0.0009054330876097083\n",
      "Iteration 6663: loss 0.000905433320440352\n",
      "Iteration 6664: loss 0.000905433320440352\n",
      "Iteration 6665: loss 0.0009054330876097083\n",
      "Iteration 6666: loss 0.000905433320440352\n",
      "Iteration 6667: loss 0.000905433320440352\n",
      "Iteration 6668: loss 0.0009054330876097083\n",
      "Iteration 6669: loss 0.000905433320440352\n",
      "Iteration 6670: loss 0.000905433320440352\n",
      "Iteration 6671: loss 0.0009054330876097083\n",
      "Iteration 6672: loss 0.000905433320440352\n",
      "Iteration 6673: loss 0.000905433320440352\n",
      "Iteration 6674: loss 0.0009054330876097083\n",
      "Iteration 6675: loss 0.000905433320440352\n",
      "Iteration 6676: loss 0.000905433320440352\n",
      "Iteration 6677: loss 0.0009054330876097083\n",
      "Iteration 6678: loss 0.000905433320440352\n",
      "Iteration 6679: loss 0.000905433320440352\n",
      "Iteration 6680: loss 0.0009054330876097083\n",
      "Iteration 6681: loss 0.000905433320440352\n",
      "Iteration 6682: loss 0.000905433320440352\n",
      "Iteration 6683: loss 0.0009054330876097083\n",
      "Iteration 6684: loss 0.000905433320440352\n",
      "Iteration 6685: loss 0.000905433320440352\n",
      "Iteration 6686: loss 0.0009054330876097083\n",
      "Iteration 6687: loss 0.000905433320440352\n",
      "Iteration 6688: loss 0.000905433320440352\n",
      "Iteration 6689: loss 0.0009054330876097083\n",
      "Iteration 6690: loss 0.000905433320440352\n",
      "Iteration 6691: loss 0.000905433320440352\n",
      "Iteration 6692: loss 0.0009054330876097083\n",
      "Iteration 6693: loss 0.000905433320440352\n",
      "Iteration 6694: loss 0.000905433320440352\n",
      "Iteration 6695: loss 0.0009054330876097083\n",
      "Iteration 6696: loss 0.000905433320440352\n",
      "Iteration 6697: loss 0.000905433320440352\n",
      "Iteration 6698: loss 0.0009054330876097083\n",
      "Iteration 6699: loss 0.000905433320440352\n",
      "Iteration 6700: loss 0.000905433320440352\n",
      "Iteration 6701: loss 0.0009054330876097083\n",
      "Iteration 6702: loss 0.000905433320440352\n",
      "Iteration 6703: loss 0.000905433320440352\n",
      "Iteration 6704: loss 0.0009054330876097083\n",
      "Iteration 6705: loss 0.000905433320440352\n",
      "Iteration 6706: loss 0.000905433320440352\n",
      "Iteration 6707: loss 0.0009054330876097083\n",
      "Iteration 6708: loss 0.000905433320440352\n",
      "Iteration 6709: loss 0.000905433320440352\n",
      "Iteration 6710: loss 0.0009054330876097083\n",
      "Iteration 6711: loss 0.000905433320440352\n",
      "Iteration 6712: loss 0.000905433320440352\n",
      "Iteration 6713: loss 0.0009054330876097083\n",
      "Iteration 6714: loss 0.000905433320440352\n",
      "Iteration 6715: loss 0.000905433320440352\n",
      "Iteration 6716: loss 0.0009054330876097083\n",
      "Iteration 6717: loss 0.000905433320440352\n",
      "Iteration 6718: loss 0.000905433320440352\n",
      "Iteration 6719: loss 0.0009054330876097083\n",
      "Iteration 6720: loss 0.000905433320440352\n",
      "Iteration 6721: loss 0.000905433320440352\n",
      "Iteration 6722: loss 0.0009054330876097083\n",
      "Iteration 6723: loss 0.000905433320440352\n",
      "Iteration 6724: loss 0.000905433320440352\n",
      "Iteration 6725: loss 0.0009054330876097083\n",
      "Iteration 6726: loss 0.000905433320440352\n",
      "Iteration 6727: loss 0.000905433320440352\n",
      "Iteration 6728: loss 0.0009054330876097083\n",
      "Iteration 6729: loss 0.000905433320440352\n",
      "Iteration 6730: loss 0.000905433320440352\n",
      "Iteration 6731: loss 0.0009054330876097083\n",
      "Iteration 6732: loss 0.000905433320440352\n",
      "Iteration 6733: loss 0.000905433320440352\n",
      "Iteration 6734: loss 0.0009054330876097083\n",
      "Iteration 6735: loss 0.000905433320440352\n",
      "Iteration 6736: loss 0.000905433320440352\n",
      "Iteration 6737: loss 0.0009054330876097083\n",
      "Iteration 6738: loss 0.000905433320440352\n",
      "Iteration 6739: loss 0.000905433320440352\n",
      "Iteration 6740: loss 0.0009054330876097083\n",
      "Iteration 6741: loss 0.000905433320440352\n",
      "Iteration 6742: loss 0.000905433320440352\n",
      "Iteration 6743: loss 0.0009054330876097083\n",
      "Iteration 6744: loss 0.000905433320440352\n",
      "Iteration 6745: loss 0.000905433320440352\n",
      "Iteration 6746: loss 0.0009054330876097083\n",
      "Iteration 6747: loss 0.000905433320440352\n",
      "Iteration 6748: loss 0.000905433320440352\n",
      "Iteration 6749: loss 0.0009054330876097083\n",
      "Iteration 6750: loss 0.000905433320440352\n",
      "Iteration 6751: loss 0.000905433320440352\n",
      "Iteration 6752: loss 0.0009054330876097083\n",
      "Iteration 6753: loss 0.000905433320440352\n",
      "Iteration 6754: loss 0.000905433320440352\n",
      "Iteration 6755: loss 0.0009054330876097083\n",
      "Iteration 6756: loss 0.000905433320440352\n",
      "Iteration 6757: loss 0.000905433320440352\n",
      "Iteration 6758: loss 0.0009054330876097083\n",
      "Iteration 6759: loss 0.000905433320440352\n",
      "Iteration 6760: loss 0.000905433320440352\n",
      "Iteration 6761: loss 0.0009054330876097083\n",
      "Iteration 6762: loss 0.000905433320440352\n",
      "Iteration 6763: loss 0.000905433320440352\n",
      "Iteration 6764: loss 0.0009054330876097083\n",
      "Iteration 6765: loss 0.000905433320440352\n",
      "Iteration 6766: loss 0.000905433320440352\n",
      "Iteration 6767: loss 0.0009054330876097083\n",
      "Iteration 6768: loss 0.000905433320440352\n",
      "Iteration 6769: loss 0.000905433320440352\n",
      "Iteration 6770: loss 0.0009054330876097083\n",
      "Iteration 6771: loss 0.000905433320440352\n",
      "Iteration 6772: loss 0.000905433320440352\n",
      "Iteration 6773: loss 0.0009054330876097083\n",
      "Iteration 6774: loss 0.000905433320440352\n",
      "Iteration 6775: loss 0.000905433320440352\n",
      "Iteration 6776: loss 0.0009054330876097083\n",
      "Iteration 6777: loss 0.000905433320440352\n",
      "Iteration 6778: loss 0.000905433320440352\n",
      "Iteration 6779: loss 0.0009054330876097083\n",
      "Iteration 6780: loss 0.000905433320440352\n",
      "Iteration 6781: loss 0.000905433320440352\n",
      "Iteration 6782: loss 0.0009054330876097083\n",
      "Iteration 6783: loss 0.000905433320440352\n",
      "Iteration 6784: loss 0.000905433320440352\n",
      "Iteration 6785: loss 0.0009054330876097083\n",
      "Iteration 6786: loss 0.000905433320440352\n",
      "Iteration 6787: loss 0.000905433320440352\n",
      "Iteration 6788: loss 0.0009054330876097083\n",
      "Iteration 6789: loss 0.000905433320440352\n",
      "Iteration 6790: loss 0.000905433320440352\n",
      "Iteration 6791: loss 0.0009054330876097083\n",
      "Iteration 6792: loss 0.000905433320440352\n",
      "Iteration 6793: loss 0.000905433320440352\n",
      "Iteration 6794: loss 0.0009054330876097083\n",
      "Iteration 6795: loss 0.000905433320440352\n",
      "Iteration 6796: loss 0.000905433320440352\n",
      "Iteration 6797: loss 0.0009054330876097083\n",
      "Iteration 6798: loss 0.000905433320440352\n",
      "Iteration 6799: loss 0.000905433320440352\n",
      "Iteration 6800: loss 0.0009054330876097083\n",
      "Iteration 6801: loss 0.000905433320440352\n",
      "Iteration 6802: loss 0.000905433320440352\n",
      "Iteration 6803: loss 0.0009054330876097083\n",
      "Iteration 6804: loss 0.000905433320440352\n",
      "Iteration 6805: loss 0.000905433320440352\n",
      "Iteration 6806: loss 0.0009054330876097083\n",
      "Iteration 6807: loss 0.000905433320440352\n",
      "Iteration 6808: loss 0.000905433320440352\n",
      "Iteration 6809: loss 0.0009054330876097083\n",
      "Iteration 6810: loss 0.000905433320440352\n",
      "Iteration 6811: loss 0.000905433320440352\n",
      "Iteration 6812: loss 0.0009054330876097083\n",
      "Iteration 6813: loss 0.000905433320440352\n",
      "Iteration 6814: loss 0.000905433320440352\n",
      "Iteration 6815: loss 0.0009054330876097083\n",
      "Iteration 6816: loss 0.000905433320440352\n",
      "Iteration 6817: loss 0.000905433320440352\n",
      "Iteration 6818: loss 0.0009054330876097083\n",
      "Iteration 6819: loss 0.000905433320440352\n",
      "Iteration 6820: loss 0.000905433320440352\n",
      "Iteration 6821: loss 0.0009054330876097083\n",
      "Iteration 6822: loss 0.000905433320440352\n",
      "Iteration 6823: loss 0.000905433320440352\n",
      "Iteration 6824: loss 0.0009054330876097083\n",
      "Iteration 6825: loss 0.000905433320440352\n",
      "Iteration 6826: loss 0.000905433320440352\n",
      "Iteration 6827: loss 0.0009054330876097083\n",
      "Iteration 6828: loss 0.000905433320440352\n",
      "Iteration 6829: loss 0.000905433320440352\n",
      "Iteration 6830: loss 0.0009054330876097083\n",
      "Iteration 6831: loss 0.000905433320440352\n",
      "Iteration 6832: loss 0.000905433320440352\n",
      "Iteration 6833: loss 0.0009054330876097083\n",
      "Iteration 6834: loss 0.000905433320440352\n",
      "Iteration 6835: loss 0.000905433320440352\n",
      "Iteration 6836: loss 0.0009054330876097083\n",
      "Iteration 6837: loss 0.000905433320440352\n",
      "Iteration 6838: loss 0.000905433320440352\n",
      "Iteration 6839: loss 0.0009054330876097083\n",
      "Iteration 6840: loss 0.000905433320440352\n",
      "Iteration 6841: loss 0.000905433320440352\n",
      "Iteration 6842: loss 0.0009054330876097083\n",
      "Iteration 6843: loss 0.000905433320440352\n",
      "Iteration 6844: loss 0.000905433320440352\n",
      "Iteration 6845: loss 0.0009054330876097083\n",
      "Iteration 6846: loss 0.000905433320440352\n",
      "Iteration 6847: loss 0.000905433320440352\n",
      "Iteration 6848: loss 0.0009054330876097083\n",
      "Iteration 6849: loss 0.000905433320440352\n",
      "Iteration 6850: loss 0.000905433320440352\n",
      "Iteration 6851: loss 0.0009054330876097083\n",
      "Iteration 6852: loss 0.000905433320440352\n",
      "Iteration 6853: loss 0.000905433320440352\n",
      "Iteration 6854: loss 0.0009054330876097083\n",
      "Iteration 6855: loss 0.000905433320440352\n",
      "Iteration 6856: loss 0.000905433320440352\n",
      "Iteration 6857: loss 0.0009054330876097083\n",
      "Iteration 6858: loss 0.000905433320440352\n",
      "Iteration 6859: loss 0.000905433320440352\n",
      "Iteration 6860: loss 0.0009054330876097083\n",
      "Iteration 6861: loss 0.000905433320440352\n",
      "Iteration 6862: loss 0.000905433320440352\n",
      "Iteration 6863: loss 0.0009054330876097083\n",
      "Iteration 6864: loss 0.000905433320440352\n",
      "Iteration 6865: loss 0.000905433320440352\n",
      "Iteration 6866: loss 0.0009054330876097083\n",
      "Iteration 6867: loss 0.000905433320440352\n",
      "Iteration 6868: loss 0.000905433320440352\n",
      "Iteration 6869: loss 0.0009054330876097083\n",
      "Iteration 6870: loss 0.000905433320440352\n",
      "Iteration 6871: loss 0.000905433320440352\n",
      "Iteration 6872: loss 0.0009054330876097083\n",
      "Iteration 6873: loss 0.000905433320440352\n",
      "Iteration 6874: loss 0.000905433320440352\n",
      "Iteration 6875: loss 0.0009054330876097083\n",
      "Iteration 6876: loss 0.000905433320440352\n",
      "Iteration 6877: loss 0.000905433320440352\n",
      "Iteration 6878: loss 0.0009054330876097083\n",
      "Iteration 6879: loss 0.000905433320440352\n",
      "Iteration 6880: loss 0.000905433320440352\n",
      "Iteration 6881: loss 0.0009054330876097083\n",
      "Iteration 6882: loss 0.000905433320440352\n",
      "Iteration 6883: loss 0.000905433320440352\n",
      "Iteration 6884: loss 0.0009054330876097083\n",
      "Iteration 6885: loss 0.000905433320440352\n",
      "Iteration 6886: loss 0.000905433320440352\n",
      "Iteration 6887: loss 0.0009054330876097083\n",
      "Iteration 6888: loss 0.000905433320440352\n",
      "Iteration 6889: loss 0.000905433320440352\n",
      "Iteration 6890: loss 0.0009054330876097083\n",
      "Iteration 6891: loss 0.000905433320440352\n",
      "Iteration 6892: loss 0.000905433320440352\n",
      "Iteration 6893: loss 0.0009054330876097083\n",
      "Iteration 6894: loss 0.000905433320440352\n",
      "Iteration 6895: loss 0.000905433320440352\n",
      "Iteration 6896: loss 0.0009054330876097083\n",
      "Iteration 6897: loss 0.000905433320440352\n",
      "Iteration 6898: loss 0.000905433320440352\n",
      "Iteration 6899: loss 0.0009054330876097083\n",
      "Iteration 6900: loss 0.000905433320440352\n",
      "Iteration 6901: loss 0.000905433320440352\n",
      "Iteration 6902: loss 0.0009054330876097083\n",
      "Iteration 6903: loss 0.000905433320440352\n",
      "Iteration 6904: loss 0.000905433320440352\n",
      "Iteration 6905: loss 0.0009054330876097083\n",
      "Iteration 6906: loss 0.000905433320440352\n",
      "Iteration 6907: loss 0.000905433320440352\n",
      "Iteration 6908: loss 0.0009054330876097083\n",
      "Iteration 6909: loss 0.000905433320440352\n",
      "Iteration 6910: loss 0.000905433320440352\n",
      "Iteration 6911: loss 0.0009054330876097083\n",
      "Iteration 6912: loss 0.000905433320440352\n",
      "Iteration 6913: loss 0.000905433320440352\n",
      "Iteration 6914: loss 0.0009054330876097083\n",
      "Iteration 6915: loss 0.000905433320440352\n",
      "Iteration 6916: loss 0.000905433320440352\n",
      "Iteration 6917: loss 0.0009054330876097083\n",
      "Iteration 6918: loss 0.000905433320440352\n",
      "Iteration 6919: loss 0.000905433320440352\n",
      "Iteration 6920: loss 0.0009054330876097083\n",
      "Iteration 6921: loss 0.000905433320440352\n",
      "Iteration 6922: loss 0.000905433320440352\n",
      "Iteration 6923: loss 0.0009054330876097083\n",
      "Iteration 6924: loss 0.000905433320440352\n",
      "Iteration 6925: loss 0.000905433320440352\n",
      "Iteration 6926: loss 0.0009054330876097083\n",
      "Iteration 6927: loss 0.000905433320440352\n",
      "Iteration 6928: loss 0.000905433320440352\n",
      "Iteration 6929: loss 0.0009054330876097083\n",
      "Iteration 6930: loss 0.000905433320440352\n",
      "Iteration 6931: loss 0.000905433320440352\n",
      "Iteration 6932: loss 0.0009054330876097083\n",
      "Iteration 6933: loss 0.000905433320440352\n",
      "Iteration 6934: loss 0.000905433320440352\n",
      "Iteration 6935: loss 0.0009054330876097083\n",
      "Iteration 6936: loss 0.000905433320440352\n",
      "Iteration 6937: loss 0.000905433320440352\n",
      "Iteration 6938: loss 0.0009054330876097083\n",
      "Iteration 6939: loss 0.000905433320440352\n",
      "Iteration 6940: loss 0.000905433320440352\n",
      "Iteration 6941: loss 0.0009054330876097083\n",
      "Iteration 6942: loss 0.000905433320440352\n",
      "Iteration 6943: loss 0.000905433320440352\n",
      "Iteration 6944: loss 0.0009054330876097083\n",
      "Iteration 6945: loss 0.000905433320440352\n",
      "Iteration 6946: loss 0.000905433320440352\n",
      "Iteration 6947: loss 0.0009054330876097083\n",
      "Iteration 6948: loss 0.000905433320440352\n",
      "Iteration 6949: loss 0.000905433320440352\n",
      "Iteration 6950: loss 0.0009054330876097083\n",
      "Iteration 6951: loss 0.000905433320440352\n",
      "Iteration 6952: loss 0.000905433320440352\n",
      "Iteration 6953: loss 0.0009054330876097083\n",
      "Iteration 6954: loss 0.000905433320440352\n",
      "Iteration 6955: loss 0.000905433320440352\n",
      "Iteration 6956: loss 0.0009054330876097083\n",
      "Iteration 6957: loss 0.000905433320440352\n",
      "Iteration 6958: loss 0.000905433320440352\n",
      "Iteration 6959: loss 0.0009054330876097083\n",
      "Iteration 6960: loss 0.000905433320440352\n",
      "Iteration 6961: loss 0.000905433320440352\n",
      "Iteration 6962: loss 0.0009054330876097083\n",
      "Iteration 6963: loss 0.000905433320440352\n",
      "Iteration 6964: loss 0.000905433320440352\n",
      "Iteration 6965: loss 0.0009054330876097083\n",
      "Iteration 6966: loss 0.000905433320440352\n",
      "Iteration 6967: loss 0.000905433320440352\n",
      "Iteration 6968: loss 0.0009054330876097083\n",
      "Iteration 6969: loss 0.000905433320440352\n",
      "Iteration 6970: loss 0.000905433320440352\n",
      "Iteration 6971: loss 0.0009054330876097083\n",
      "Iteration 6972: loss 0.000905433320440352\n",
      "Iteration 6973: loss 0.000905433320440352\n",
      "Iteration 6974: loss 0.0009054330876097083\n",
      "Iteration 6975: loss 0.000905433320440352\n",
      "Iteration 6976: loss 0.000905433320440352\n",
      "Iteration 6977: loss 0.0009054330876097083\n",
      "Iteration 6978: loss 0.000905433320440352\n",
      "Iteration 6979: loss 0.000905433320440352\n",
      "Iteration 6980: loss 0.0009054330876097083\n",
      "Iteration 6981: loss 0.000905433320440352\n",
      "Iteration 6982: loss 0.000905433320440352\n",
      "Iteration 6983: loss 0.0009054330876097083\n",
      "Iteration 6984: loss 0.000905433320440352\n",
      "Iteration 6985: loss 0.000905433320440352\n",
      "Iteration 6986: loss 0.0009054330876097083\n",
      "Iteration 6987: loss 0.000905433320440352\n",
      "Iteration 6988: loss 0.000905433320440352\n",
      "Iteration 6989: loss 0.0009054330876097083\n",
      "Iteration 6990: loss 0.000905433320440352\n",
      "Iteration 6991: loss 0.000905433320440352\n",
      "Iteration 6992: loss 0.0009054330876097083\n",
      "Iteration 6993: loss 0.000905433320440352\n",
      "Iteration 6994: loss 0.000905433320440352\n",
      "Iteration 6995: loss 0.0009054330876097083\n",
      "Iteration 6996: loss 0.000905433320440352\n",
      "Iteration 6997: loss 0.000905433320440352\n",
      "Iteration 6998: loss 0.0009054330876097083\n",
      "Iteration 6999: loss 0.000905433320440352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7000: loss 0.000905433320440352\n",
      "Iteration 7001: loss 0.0009054330876097083\n",
      "Iteration 7002: loss 0.000905433320440352\n",
      "Iteration 7003: loss 0.000905433320440352\n",
      "Iteration 7004: loss 0.0009054330876097083\n",
      "Iteration 7005: loss 0.000905433320440352\n",
      "Iteration 7006: loss 0.000905433320440352\n",
      "Iteration 7007: loss 0.0009054330876097083\n",
      "Iteration 7008: loss 0.000905433320440352\n",
      "Iteration 7009: loss 0.000905433320440352\n",
      "Iteration 7010: loss 0.0009054330876097083\n",
      "Iteration 7011: loss 0.000905433320440352\n",
      "Iteration 7012: loss 0.000905433320440352\n",
      "Iteration 7013: loss 0.0009054330876097083\n",
      "Iteration 7014: loss 0.000905433320440352\n",
      "Iteration 7015: loss 0.000905433320440352\n",
      "Iteration 7016: loss 0.0009054330876097083\n",
      "Iteration 7017: loss 0.000905433320440352\n",
      "Iteration 7018: loss 0.000905433320440352\n",
      "Iteration 7019: loss 0.0009054330876097083\n",
      "Iteration 7020: loss 0.000905433320440352\n",
      "Iteration 7021: loss 0.000905433320440352\n",
      "Iteration 7022: loss 0.0009054330876097083\n",
      "Iteration 7023: loss 0.000905433320440352\n",
      "Iteration 7024: loss 0.000905433320440352\n",
      "Iteration 7025: loss 0.0009054330876097083\n",
      "Iteration 7026: loss 0.000905433320440352\n",
      "Iteration 7027: loss 0.000905433320440352\n",
      "Iteration 7028: loss 0.0009054330876097083\n",
      "Iteration 7029: loss 0.000905433320440352\n",
      "Iteration 7030: loss 0.000905433320440352\n",
      "Iteration 7031: loss 0.0009054330876097083\n",
      "Iteration 7032: loss 0.000905433320440352\n",
      "Iteration 7033: loss 0.000905433320440352\n",
      "Iteration 7034: loss 0.0009054330876097083\n",
      "Iteration 7035: loss 0.000905433320440352\n",
      "Iteration 7036: loss 0.000905433320440352\n",
      "Iteration 7037: loss 0.0009054330876097083\n",
      "Iteration 7038: loss 0.000905433320440352\n",
      "Iteration 7039: loss 0.000905433320440352\n",
      "Iteration 7040: loss 0.0009054330876097083\n",
      "Iteration 7041: loss 0.000905433320440352\n",
      "Iteration 7042: loss 0.000905433320440352\n",
      "Iteration 7043: loss 0.0009054330876097083\n",
      "Iteration 7044: loss 0.000905433320440352\n",
      "Iteration 7045: loss 0.000905433320440352\n",
      "Iteration 7046: loss 0.0009054330876097083\n",
      "Iteration 7047: loss 0.000905433320440352\n",
      "Iteration 7048: loss 0.000905433320440352\n",
      "Iteration 7049: loss 0.0009054330876097083\n",
      "Iteration 7050: loss 0.000905433320440352\n",
      "Iteration 7051: loss 0.000905433320440352\n",
      "Iteration 7052: loss 0.0009054330876097083\n",
      "Iteration 7053: loss 0.000905433320440352\n",
      "Iteration 7054: loss 0.000905433320440352\n",
      "Iteration 7055: loss 0.0009054330876097083\n",
      "Iteration 7056: loss 0.000905433320440352\n",
      "Iteration 7057: loss 0.000905433320440352\n",
      "Iteration 7058: loss 0.0009054330876097083\n",
      "Iteration 7059: loss 0.000905433320440352\n",
      "Iteration 7060: loss 0.000905433320440352\n",
      "Iteration 7061: loss 0.0009054330876097083\n",
      "Iteration 7062: loss 0.000905433320440352\n",
      "Iteration 7063: loss 0.000905433320440352\n",
      "Iteration 7064: loss 0.0009054330876097083\n",
      "Iteration 7065: loss 0.000905433320440352\n",
      "Iteration 7066: loss 0.000905433320440352\n",
      "Iteration 7067: loss 0.0009054330876097083\n",
      "Iteration 7068: loss 0.000905433320440352\n",
      "Iteration 7069: loss 0.000905433320440352\n",
      "Iteration 7070: loss 0.0009054330876097083\n",
      "Iteration 7071: loss 0.000905433320440352\n",
      "Iteration 7072: loss 0.000905433320440352\n",
      "Iteration 7073: loss 0.0009054330876097083\n",
      "Iteration 7074: loss 0.000905433320440352\n",
      "Iteration 7075: loss 0.000905433320440352\n",
      "Iteration 7076: loss 0.0009054330876097083\n",
      "Iteration 7077: loss 0.000905433320440352\n",
      "Iteration 7078: loss 0.000905433320440352\n",
      "Iteration 7079: loss 0.0009054330876097083\n",
      "Iteration 7080: loss 0.000905433320440352\n",
      "Iteration 7081: loss 0.000905433320440352\n",
      "Iteration 7082: loss 0.0009054330876097083\n",
      "Iteration 7083: loss 0.000905433320440352\n",
      "Iteration 7084: loss 0.000905433320440352\n",
      "Iteration 7085: loss 0.0009054330876097083\n",
      "Iteration 7086: loss 0.000905433320440352\n",
      "Iteration 7087: loss 0.000905433320440352\n",
      "Iteration 7088: loss 0.0009054330876097083\n",
      "Iteration 7089: loss 0.000905433320440352\n",
      "Iteration 7090: loss 0.000905433320440352\n",
      "Iteration 7091: loss 0.0009054330876097083\n",
      "Iteration 7092: loss 0.000905433320440352\n",
      "Iteration 7093: loss 0.000905433320440352\n",
      "Iteration 7094: loss 0.0009054330876097083\n",
      "Iteration 7095: loss 0.000905433320440352\n",
      "Iteration 7096: loss 0.000905433320440352\n",
      "Iteration 7097: loss 0.0009054330876097083\n",
      "Iteration 7098: loss 0.000905433320440352\n",
      "Iteration 7099: loss 0.000905433320440352\n",
      "Iteration 7100: loss 0.0009054330876097083\n",
      "Iteration 7101: loss 0.000905433320440352\n",
      "Iteration 7102: loss 0.000905433320440352\n",
      "Iteration 7103: loss 0.0009054330876097083\n",
      "Iteration 7104: loss 0.000905433320440352\n",
      "Iteration 7105: loss 0.000905433320440352\n",
      "Iteration 7106: loss 0.0009054330876097083\n",
      "Iteration 7107: loss 0.000905433320440352\n",
      "Iteration 7108: loss 0.000905433320440352\n",
      "Iteration 7109: loss 0.0009054330876097083\n",
      "Iteration 7110: loss 0.000905433320440352\n",
      "Iteration 7111: loss 0.000905433320440352\n",
      "Iteration 7112: loss 0.0009054330876097083\n",
      "Iteration 7113: loss 0.000905433320440352\n",
      "Iteration 7114: loss 0.000905433320440352\n",
      "Iteration 7115: loss 0.0009054330876097083\n",
      "Iteration 7116: loss 0.000905433320440352\n",
      "Iteration 7117: loss 0.000905433320440352\n",
      "Iteration 7118: loss 0.0009054330876097083\n",
      "Iteration 7119: loss 0.000905433320440352\n",
      "Iteration 7120: loss 0.000905433320440352\n",
      "Iteration 7121: loss 0.0009054330876097083\n",
      "Iteration 7122: loss 0.000905433320440352\n",
      "Iteration 7123: loss 0.000905433320440352\n",
      "Iteration 7124: loss 0.0009054330876097083\n",
      "Iteration 7125: loss 0.000905433320440352\n",
      "Iteration 7126: loss 0.000905433320440352\n",
      "Iteration 7127: loss 0.0009054330876097083\n",
      "Iteration 7128: loss 0.000905433320440352\n",
      "Iteration 7129: loss 0.000905433320440352\n",
      "Iteration 7130: loss 0.0009054330876097083\n",
      "Iteration 7131: loss 0.000905433320440352\n",
      "Iteration 7132: loss 0.000905433320440352\n",
      "Iteration 7133: loss 0.0009054330876097083\n",
      "Iteration 7134: loss 0.000905433320440352\n",
      "Iteration 7135: loss 0.000905433320440352\n",
      "Iteration 7136: loss 0.0009054330876097083\n",
      "Iteration 7137: loss 0.000905433320440352\n",
      "Iteration 7138: loss 0.000905433320440352\n",
      "Iteration 7139: loss 0.0009054330876097083\n",
      "Iteration 7140: loss 0.000905433320440352\n",
      "Iteration 7141: loss 0.000905433320440352\n",
      "Iteration 7142: loss 0.0009054330876097083\n",
      "Iteration 7143: loss 0.000905433320440352\n",
      "Iteration 7144: loss 0.000905433320440352\n",
      "Iteration 7145: loss 0.0009054330876097083\n",
      "Iteration 7146: loss 0.000905433320440352\n",
      "Iteration 7147: loss 0.000905433320440352\n",
      "Iteration 7148: loss 0.0009054330876097083\n",
      "Iteration 7149: loss 0.000905433320440352\n",
      "Iteration 7150: loss 0.000905433320440352\n",
      "Iteration 7151: loss 0.0009054330876097083\n",
      "Iteration 7152: loss 0.000905433320440352\n",
      "Iteration 7153: loss 0.000905433320440352\n",
      "Iteration 7154: loss 0.0009054330876097083\n",
      "Iteration 7155: loss 0.000905433320440352\n",
      "Iteration 7156: loss 0.000905433320440352\n",
      "Iteration 7157: loss 0.0009054330876097083\n",
      "Iteration 7158: loss 0.000905433320440352\n",
      "Iteration 7159: loss 0.000905433320440352\n",
      "Iteration 7160: loss 0.0009054330876097083\n",
      "Iteration 7161: loss 0.000905433320440352\n",
      "Iteration 7162: loss 0.000905433320440352\n",
      "Iteration 7163: loss 0.0009054330876097083\n",
      "Iteration 7164: loss 0.000905433320440352\n",
      "Iteration 7165: loss 0.000905433320440352\n",
      "Iteration 7166: loss 0.0009054330876097083\n",
      "Iteration 7167: loss 0.000905433320440352\n",
      "Iteration 7168: loss 0.000905433320440352\n",
      "Iteration 7169: loss 0.0009054330876097083\n",
      "Iteration 7170: loss 0.000905433320440352\n",
      "Iteration 7171: loss 0.000905433320440352\n",
      "Iteration 7172: loss 0.0009054330876097083\n",
      "Iteration 7173: loss 0.000905433320440352\n",
      "Iteration 7174: loss 0.000905433320440352\n",
      "Iteration 7175: loss 0.0009054330876097083\n",
      "Iteration 7176: loss 0.000905433320440352\n",
      "Iteration 7177: loss 0.000905433320440352\n",
      "Iteration 7178: loss 0.0009054330876097083\n",
      "Iteration 7179: loss 0.000905433320440352\n",
      "Iteration 7180: loss 0.000905433320440352\n",
      "Iteration 7181: loss 0.0009054330876097083\n",
      "Iteration 7182: loss 0.000905433320440352\n",
      "Iteration 7183: loss 0.000905433320440352\n",
      "Iteration 7184: loss 0.0009054330876097083\n",
      "Iteration 7185: loss 0.000905433320440352\n",
      "Iteration 7186: loss 0.000905433320440352\n",
      "Iteration 7187: loss 0.0009054330876097083\n",
      "Iteration 7188: loss 0.000905433320440352\n",
      "Iteration 7189: loss 0.000905433320440352\n",
      "Iteration 7190: loss 0.0009054330876097083\n",
      "Iteration 7191: loss 0.000905433320440352\n",
      "Iteration 7192: loss 0.000905433320440352\n",
      "Iteration 7193: loss 0.0009054330876097083\n",
      "Iteration 7194: loss 0.000905433320440352\n",
      "Iteration 7195: loss 0.000905433320440352\n",
      "Iteration 7196: loss 0.0009054330876097083\n",
      "Iteration 7197: loss 0.000905433320440352\n",
      "Iteration 7198: loss 0.000905433320440352\n",
      "Iteration 7199: loss 0.0009054330876097083\n",
      "Iteration 7200: loss 0.000905433320440352\n",
      "Iteration 7201: loss 0.000905433320440352\n",
      "Iteration 7202: loss 0.0009054330876097083\n",
      "Iteration 7203: loss 0.000905433320440352\n",
      "Iteration 7204: loss 0.000905433320440352\n",
      "Iteration 7205: loss 0.0009054330876097083\n",
      "Iteration 7206: loss 0.000905433320440352\n",
      "Iteration 7207: loss 0.000905433320440352\n",
      "Iteration 7208: loss 0.0009054330876097083\n",
      "Iteration 7209: loss 0.000905433320440352\n",
      "Iteration 7210: loss 0.000905433320440352\n",
      "Iteration 7211: loss 0.0009054330876097083\n",
      "Iteration 7212: loss 0.000905433320440352\n",
      "Iteration 7213: loss 0.000905433320440352\n",
      "Iteration 7214: loss 0.0009054330876097083\n",
      "Iteration 7215: loss 0.000905433320440352\n",
      "Iteration 7216: loss 0.000905433320440352\n",
      "Iteration 7217: loss 0.0009054330876097083\n",
      "Iteration 7218: loss 0.000905433320440352\n",
      "Iteration 7219: loss 0.000905433320440352\n",
      "Iteration 7220: loss 0.0009054330876097083\n",
      "Iteration 7221: loss 0.000905433320440352\n",
      "Iteration 7222: loss 0.000905433320440352\n",
      "Iteration 7223: loss 0.0009054330876097083\n",
      "Iteration 7224: loss 0.000905433320440352\n",
      "Iteration 7225: loss 0.000905433320440352\n",
      "Iteration 7226: loss 0.0009054330876097083\n",
      "Iteration 7227: loss 0.000905433320440352\n",
      "Iteration 7228: loss 0.000905433320440352\n",
      "Iteration 7229: loss 0.0009054330876097083\n",
      "Iteration 7230: loss 0.000905433320440352\n",
      "Iteration 7231: loss 0.000905433320440352\n",
      "Iteration 7232: loss 0.0009054330876097083\n",
      "Iteration 7233: loss 0.000905433320440352\n",
      "Iteration 7234: loss 0.000905433320440352\n",
      "Iteration 7235: loss 0.0009054330876097083\n",
      "Iteration 7236: loss 0.000905433320440352\n",
      "Iteration 7237: loss 0.000905433320440352\n",
      "Iteration 7238: loss 0.0009054330876097083\n",
      "Iteration 7239: loss 0.000905433320440352\n",
      "Iteration 7240: loss 0.000905433320440352\n",
      "Iteration 7241: loss 0.0009054330876097083\n",
      "Iteration 7242: loss 0.000905433320440352\n",
      "Iteration 7243: loss 0.000905433320440352\n",
      "Iteration 7244: loss 0.0009054330876097083\n",
      "Iteration 7245: loss 0.000905433320440352\n",
      "Iteration 7246: loss 0.000905433320440352\n",
      "Iteration 7247: loss 0.0009054330876097083\n",
      "Iteration 7248: loss 0.000905433320440352\n",
      "Iteration 7249: loss 0.000905433320440352\n",
      "Iteration 7250: loss 0.0009054330876097083\n",
      "Iteration 7251: loss 0.000905433320440352\n",
      "Iteration 7252: loss 0.000905433320440352\n",
      "Iteration 7253: loss 0.0009054330876097083\n",
      "Iteration 7254: loss 0.000905433320440352\n",
      "Iteration 7255: loss 0.000905433320440352\n",
      "Iteration 7256: loss 0.0009054330876097083\n",
      "Iteration 7257: loss 0.000905433320440352\n",
      "Iteration 7258: loss 0.000905433320440352\n",
      "Iteration 7259: loss 0.0009054330876097083\n",
      "Iteration 7260: loss 0.000905433320440352\n",
      "Iteration 7261: loss 0.000905433320440352\n",
      "Iteration 7262: loss 0.0009054330876097083\n",
      "Iteration 7263: loss 0.000905433320440352\n",
      "Iteration 7264: loss 0.000905433320440352\n",
      "Iteration 7265: loss 0.0009054330876097083\n",
      "Iteration 7266: loss 0.000905433320440352\n",
      "Iteration 7267: loss 0.000905433320440352\n",
      "Iteration 7268: loss 0.0009054330876097083\n",
      "Iteration 7269: loss 0.000905433320440352\n",
      "Iteration 7270: loss 0.000905433320440352\n",
      "Iteration 7271: loss 0.0009054330876097083\n",
      "Iteration 7272: loss 0.000905433320440352\n",
      "Iteration 7273: loss 0.000905433320440352\n",
      "Iteration 7274: loss 0.0009054330876097083\n",
      "Iteration 7275: loss 0.000905433320440352\n",
      "Iteration 7276: loss 0.000905433320440352\n",
      "Iteration 7277: loss 0.0009054330876097083\n",
      "Iteration 7278: loss 0.000905433320440352\n",
      "Iteration 7279: loss 0.000905433320440352\n",
      "Iteration 7280: loss 0.0009054330876097083\n",
      "Iteration 7281: loss 0.000905433320440352\n",
      "Iteration 7282: loss 0.000905433320440352\n",
      "Iteration 7283: loss 0.0009054330876097083\n",
      "Iteration 7284: loss 0.000905433320440352\n",
      "Iteration 7285: loss 0.000905433320440352\n",
      "Iteration 7286: loss 0.0009054330876097083\n",
      "Iteration 7287: loss 0.000905433320440352\n",
      "Iteration 7288: loss 0.000905433320440352\n",
      "Iteration 7289: loss 0.0009054330876097083\n",
      "Iteration 7290: loss 0.000905433320440352\n",
      "Iteration 7291: loss 0.000905433320440352\n",
      "Iteration 7292: loss 0.0009054330876097083\n",
      "Iteration 7293: loss 0.000905433320440352\n",
      "Iteration 7294: loss 0.000905433320440352\n",
      "Iteration 7295: loss 0.0009054330876097083\n",
      "Iteration 7296: loss 0.000905433320440352\n",
      "Iteration 7297: loss 0.000905433320440352\n",
      "Iteration 7298: loss 0.0009054330876097083\n",
      "Iteration 7299: loss 0.000905433320440352\n",
      "Iteration 7300: loss 0.000905433320440352\n",
      "Iteration 7301: loss 0.0009054330876097083\n",
      "Iteration 7302: loss 0.000905433320440352\n",
      "Iteration 7303: loss 0.000905433320440352\n",
      "Iteration 7304: loss 0.0009054330876097083\n",
      "Iteration 7305: loss 0.000905433320440352\n",
      "Iteration 7306: loss 0.000905433320440352\n",
      "Iteration 7307: loss 0.0009054330876097083\n",
      "Iteration 7308: loss 0.000905433320440352\n",
      "Iteration 7309: loss 0.000905433320440352\n",
      "Iteration 7310: loss 0.0009054330876097083\n",
      "Iteration 7311: loss 0.000905433320440352\n",
      "Iteration 7312: loss 0.000905433320440352\n",
      "Iteration 7313: loss 0.0009054330876097083\n",
      "Iteration 7314: loss 0.000905433320440352\n",
      "Iteration 7315: loss 0.000905433320440352\n",
      "Iteration 7316: loss 0.0009054330876097083\n",
      "Iteration 7317: loss 0.000905433320440352\n",
      "Iteration 7318: loss 0.000905433320440352\n",
      "Iteration 7319: loss 0.0009054330876097083\n",
      "Iteration 7320: loss 0.000905433320440352\n",
      "Iteration 7321: loss 0.000905433320440352\n",
      "Iteration 7322: loss 0.0009054330876097083\n",
      "Iteration 7323: loss 0.000905433320440352\n",
      "Iteration 7324: loss 0.000905433320440352\n",
      "Iteration 7325: loss 0.0009054330876097083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7326: loss 0.000905433320440352\n",
      "Iteration 7327: loss 0.000905433320440352\n",
      "Iteration 7328: loss 0.0009054330876097083\n",
      "Iteration 7329: loss 0.000905433320440352\n",
      "Iteration 7330: loss 0.000905433320440352\n",
      "Iteration 7331: loss 0.0009054330876097083\n",
      "Iteration 7332: loss 0.000905433320440352\n",
      "Iteration 7333: loss 0.000905433320440352\n",
      "Iteration 7334: loss 0.0009054330876097083\n",
      "Iteration 7335: loss 0.000905433320440352\n",
      "Iteration 7336: loss 0.000905433320440352\n",
      "Iteration 7337: loss 0.0009054330876097083\n",
      "Iteration 7338: loss 0.000905433320440352\n",
      "Iteration 7339: loss 0.000905433320440352\n",
      "Iteration 7340: loss 0.0009054330876097083\n",
      "Iteration 7341: loss 0.000905433320440352\n",
      "Iteration 7342: loss 0.000905433320440352\n",
      "Iteration 7343: loss 0.0009054330876097083\n",
      "Iteration 7344: loss 0.000905433320440352\n",
      "Iteration 7345: loss 0.000905433320440352\n",
      "Iteration 7346: loss 0.0009054330876097083\n",
      "Iteration 7347: loss 0.000905433320440352\n",
      "Iteration 7348: loss 0.000905433320440352\n",
      "Iteration 7349: loss 0.0009054330876097083\n",
      "Iteration 7350: loss 0.000905433320440352\n",
      "Iteration 7351: loss 0.000905433320440352\n",
      "Iteration 7352: loss 0.0009054330876097083\n",
      "Iteration 7353: loss 0.000905433320440352\n",
      "Iteration 7354: loss 0.000905433320440352\n",
      "Iteration 7355: loss 0.0009054330876097083\n",
      "Iteration 7356: loss 0.000905433320440352\n",
      "Iteration 7357: loss 0.000905433320440352\n",
      "Iteration 7358: loss 0.0009054330876097083\n",
      "Iteration 7359: loss 0.000905433320440352\n",
      "Iteration 7360: loss 0.000905433320440352\n",
      "Iteration 7361: loss 0.0009054330876097083\n",
      "Iteration 7362: loss 0.000905433320440352\n",
      "Iteration 7363: loss 0.000905433320440352\n",
      "Iteration 7364: loss 0.0009054330876097083\n",
      "Iteration 7365: loss 0.000905433320440352\n",
      "Iteration 7366: loss 0.000905433320440352\n",
      "Iteration 7367: loss 0.0009054330876097083\n",
      "Iteration 7368: loss 0.000905433320440352\n",
      "Iteration 7369: loss 0.000905433320440352\n",
      "Iteration 7370: loss 0.0009054330876097083\n",
      "Iteration 7371: loss 0.000905433320440352\n",
      "Iteration 7372: loss 0.000905433320440352\n",
      "Iteration 7373: loss 0.0009054330876097083\n",
      "Iteration 7374: loss 0.000905433320440352\n",
      "Iteration 7375: loss 0.000905433320440352\n",
      "Iteration 7376: loss 0.0009054330876097083\n",
      "Iteration 7377: loss 0.000905433320440352\n",
      "Iteration 7378: loss 0.000905433320440352\n",
      "Iteration 7379: loss 0.0009054330876097083\n",
      "Iteration 7380: loss 0.000905433320440352\n",
      "Iteration 7381: loss 0.000905433320440352\n",
      "Iteration 7382: loss 0.0009054330876097083\n",
      "Iteration 7383: loss 0.000905433320440352\n",
      "Iteration 7384: loss 0.000905433320440352\n",
      "Iteration 7385: loss 0.0009054330876097083\n",
      "Iteration 7386: loss 0.000905433320440352\n",
      "Iteration 7387: loss 0.000905433320440352\n",
      "Iteration 7388: loss 0.0009054330876097083\n",
      "Iteration 7389: loss 0.000905433320440352\n",
      "Iteration 7390: loss 0.000905433320440352\n",
      "Iteration 7391: loss 0.0009054330876097083\n",
      "Iteration 7392: loss 0.000905433320440352\n",
      "Iteration 7393: loss 0.000905433320440352\n",
      "Iteration 7394: loss 0.0009054330876097083\n",
      "Iteration 7395: loss 0.000905433320440352\n",
      "Iteration 7396: loss 0.000905433320440352\n",
      "Iteration 7397: loss 0.0009054330876097083\n",
      "Iteration 7398: loss 0.000905433320440352\n",
      "Iteration 7399: loss 0.000905433320440352\n",
      "Iteration 7400: loss 0.0009054330876097083\n",
      "Iteration 7401: loss 0.000905433320440352\n",
      "Iteration 7402: loss 0.000905433320440352\n",
      "Iteration 7403: loss 0.0009054330876097083\n",
      "Iteration 7404: loss 0.000905433320440352\n",
      "Iteration 7405: loss 0.000905433320440352\n",
      "Iteration 7406: loss 0.0009054330876097083\n",
      "Iteration 7407: loss 0.000905433320440352\n",
      "Iteration 7408: loss 0.000905433320440352\n",
      "Iteration 7409: loss 0.0009054330876097083\n",
      "Iteration 7410: loss 0.000905433320440352\n",
      "Iteration 7411: loss 0.000905433320440352\n",
      "Iteration 7412: loss 0.0009054330876097083\n",
      "Iteration 7413: loss 0.000905433320440352\n",
      "Iteration 7414: loss 0.000905433320440352\n",
      "Iteration 7415: loss 0.0009054330876097083\n",
      "Iteration 7416: loss 0.000905433320440352\n",
      "Iteration 7417: loss 0.000905433320440352\n",
      "Iteration 7418: loss 0.0009054330876097083\n",
      "Iteration 7419: loss 0.000905433320440352\n",
      "Iteration 7420: loss 0.000905433320440352\n",
      "Iteration 7421: loss 0.0009054330876097083\n",
      "Iteration 7422: loss 0.000905433320440352\n",
      "Iteration 7423: loss 0.000905433320440352\n",
      "Iteration 7424: loss 0.0009054330876097083\n",
      "Iteration 7425: loss 0.000905433320440352\n",
      "Iteration 7426: loss 0.000905433320440352\n",
      "Iteration 7427: loss 0.0009054330876097083\n",
      "Iteration 7428: loss 0.000905433320440352\n",
      "Iteration 7429: loss 0.000905433320440352\n",
      "Iteration 7430: loss 0.0009054330876097083\n",
      "Iteration 7431: loss 0.000905433320440352\n",
      "Iteration 7432: loss 0.000905433320440352\n",
      "Iteration 7433: loss 0.0009054330876097083\n",
      "Iteration 7434: loss 0.000905433320440352\n",
      "Iteration 7435: loss 0.000905433320440352\n",
      "Iteration 7436: loss 0.0009054330876097083\n",
      "Iteration 7437: loss 0.000905433320440352\n",
      "Iteration 7438: loss 0.000905433320440352\n",
      "Iteration 7439: loss 0.0009054330876097083\n",
      "Iteration 7440: loss 0.000905433320440352\n",
      "Iteration 7441: loss 0.000905433320440352\n",
      "Iteration 7442: loss 0.0009054330876097083\n",
      "Iteration 7443: loss 0.000905433320440352\n",
      "Iteration 7444: loss 0.000905433320440352\n",
      "Iteration 7445: loss 0.0009054330876097083\n",
      "Iteration 7446: loss 0.000905433320440352\n",
      "Iteration 7447: loss 0.000905433320440352\n",
      "Iteration 7448: loss 0.0009054330876097083\n",
      "Iteration 7449: loss 0.000905433320440352\n",
      "Iteration 7450: loss 0.000905433320440352\n",
      "Iteration 7451: loss 0.0009054330876097083\n",
      "Iteration 7452: loss 0.000905433320440352\n",
      "Iteration 7453: loss 0.000905433320440352\n",
      "Iteration 7454: loss 0.0009054330876097083\n",
      "Iteration 7455: loss 0.000905433320440352\n",
      "Iteration 7456: loss 0.000905433320440352\n",
      "Iteration 7457: loss 0.0009054330876097083\n",
      "Iteration 7458: loss 0.000905433320440352\n",
      "Iteration 7459: loss 0.000905433320440352\n",
      "Iteration 7460: loss 0.0009054330876097083\n",
      "Iteration 7461: loss 0.000905433320440352\n",
      "Iteration 7462: loss 0.000905433320440352\n",
      "Iteration 7463: loss 0.0009054330876097083\n",
      "Iteration 7464: loss 0.000905433320440352\n",
      "Iteration 7465: loss 0.000905433320440352\n",
      "Iteration 7466: loss 0.0009054330876097083\n",
      "Iteration 7467: loss 0.000905433320440352\n",
      "Iteration 7468: loss 0.000905433320440352\n",
      "Iteration 7469: loss 0.0009054330876097083\n",
      "Iteration 7470: loss 0.000905433320440352\n",
      "Iteration 7471: loss 0.000905433320440352\n",
      "Iteration 7472: loss 0.0009054330876097083\n",
      "Iteration 7473: loss 0.000905433320440352\n",
      "Iteration 7474: loss 0.000905433320440352\n",
      "Iteration 7475: loss 0.0009054330876097083\n",
      "Iteration 7476: loss 0.000905433320440352\n",
      "Iteration 7477: loss 0.000905433320440352\n",
      "Iteration 7478: loss 0.0009054330876097083\n",
      "Iteration 7479: loss 0.000905433320440352\n",
      "Iteration 7480: loss 0.000905433320440352\n",
      "Iteration 7481: loss 0.0009054330876097083\n",
      "Iteration 7482: loss 0.000905433320440352\n",
      "Iteration 7483: loss 0.000905433320440352\n",
      "Iteration 7484: loss 0.0009054330876097083\n",
      "Iteration 7485: loss 0.000905433320440352\n",
      "Iteration 7486: loss 0.000905433320440352\n",
      "Iteration 7487: loss 0.0009054330876097083\n",
      "Iteration 7488: loss 0.000905433320440352\n",
      "Iteration 7489: loss 0.000905433320440352\n",
      "Iteration 7490: loss 0.0009054330876097083\n",
      "Iteration 7491: loss 0.000905433320440352\n",
      "Iteration 7492: loss 0.000905433320440352\n",
      "Iteration 7493: loss 0.0009054330876097083\n",
      "Iteration 7494: loss 0.000905433320440352\n",
      "Iteration 7495: loss 0.000905433320440352\n",
      "Iteration 7496: loss 0.0009054330876097083\n",
      "Iteration 7497: loss 0.000905433320440352\n",
      "Iteration 7498: loss 0.000905433320440352\n",
      "Iteration 7499: loss 0.0009054330876097083\n",
      "Iteration 7500: loss 0.000905433320440352\n",
      "Iteration 7501: loss 0.000905433320440352\n",
      "Iteration 7502: loss 0.0009054330876097083\n",
      "Iteration 7503: loss 0.000905433320440352\n",
      "Iteration 7504: loss 0.000905433320440352\n",
      "Iteration 7505: loss 0.0009054330876097083\n",
      "Iteration 7506: loss 0.000905433320440352\n",
      "Iteration 7507: loss 0.000905433320440352\n",
      "Iteration 7508: loss 0.0009054330876097083\n",
      "Iteration 7509: loss 0.000905433320440352\n",
      "Iteration 7510: loss 0.000905433320440352\n",
      "Iteration 7511: loss 0.0009054330876097083\n",
      "Iteration 7512: loss 0.000905433320440352\n",
      "Iteration 7513: loss 0.000905433320440352\n",
      "Iteration 7514: loss 0.0009054330876097083\n",
      "Iteration 7515: loss 0.000905433320440352\n",
      "Iteration 7516: loss 0.000905433320440352\n",
      "Iteration 7517: loss 0.0009054330876097083\n",
      "Iteration 7518: loss 0.000905433320440352\n",
      "Iteration 7519: loss 0.000905433320440352\n",
      "Iteration 7520: loss 0.0009054330876097083\n",
      "Iteration 7521: loss 0.000905433320440352\n",
      "Iteration 7522: loss 0.000905433320440352\n",
      "Iteration 7523: loss 0.0009054330876097083\n",
      "Iteration 7524: loss 0.000905433320440352\n",
      "Iteration 7525: loss 0.000905433320440352\n",
      "Iteration 7526: loss 0.0009054330876097083\n",
      "Iteration 7527: loss 0.000905433320440352\n",
      "Iteration 7528: loss 0.000905433320440352\n",
      "Iteration 7529: loss 0.0009054330876097083\n",
      "Iteration 7530: loss 0.000905433320440352\n",
      "Iteration 7531: loss 0.000905433320440352\n",
      "Iteration 7532: loss 0.0009054330876097083\n",
      "Iteration 7533: loss 0.000905433320440352\n",
      "Iteration 7534: loss 0.000905433320440352\n",
      "Iteration 7535: loss 0.0009054330876097083\n",
      "Iteration 7536: loss 0.000905433320440352\n",
      "Iteration 7537: loss 0.000905433320440352\n",
      "Iteration 7538: loss 0.0009054330876097083\n",
      "Iteration 7539: loss 0.000905433320440352\n",
      "Iteration 7540: loss 0.000905433320440352\n",
      "Iteration 7541: loss 0.0009054330876097083\n",
      "Iteration 7542: loss 0.000905433320440352\n",
      "Iteration 7543: loss 0.000905433320440352\n",
      "Iteration 7544: loss 0.0009054330876097083\n",
      "Iteration 7545: loss 0.000905433320440352\n",
      "Iteration 7546: loss 0.000905433320440352\n",
      "Iteration 7547: loss 0.0009054330876097083\n",
      "Iteration 7548: loss 0.000905433320440352\n",
      "Iteration 7549: loss 0.000905433320440352\n",
      "Iteration 7550: loss 0.0009054330876097083\n",
      "Iteration 7551: loss 0.000905433320440352\n",
      "Iteration 7552: loss 0.000905433320440352\n",
      "Iteration 7553: loss 0.0009054330876097083\n",
      "Iteration 7554: loss 0.000905433320440352\n",
      "Iteration 7555: loss 0.000905433320440352\n",
      "Iteration 7556: loss 0.0009054330876097083\n",
      "Iteration 7557: loss 0.000905433320440352\n",
      "Iteration 7558: loss 0.000905433320440352\n",
      "Iteration 7559: loss 0.0009054330876097083\n",
      "Iteration 7560: loss 0.000905433320440352\n",
      "Iteration 7561: loss 0.000905433320440352\n",
      "Iteration 7562: loss 0.0009054330876097083\n",
      "Iteration 7563: loss 0.000905433320440352\n",
      "Iteration 7564: loss 0.000905433320440352\n",
      "Iteration 7565: loss 0.0009054330876097083\n",
      "Iteration 7566: loss 0.000905433320440352\n",
      "Iteration 7567: loss 0.000905433320440352\n",
      "Iteration 7568: loss 0.0009054330876097083\n",
      "Iteration 7569: loss 0.000905433320440352\n",
      "Iteration 7570: loss 0.000905433320440352\n",
      "Iteration 7571: loss 0.0009054330876097083\n",
      "Iteration 7572: loss 0.000905433320440352\n",
      "Iteration 7573: loss 0.000905433320440352\n",
      "Iteration 7574: loss 0.0009054330876097083\n",
      "Iteration 7575: loss 0.000905433320440352\n",
      "Iteration 7576: loss 0.000905433320440352\n",
      "Iteration 7577: loss 0.0009054330876097083\n",
      "Iteration 7578: loss 0.000905433320440352\n",
      "Iteration 7579: loss 0.000905433320440352\n",
      "Iteration 7580: loss 0.0009054330876097083\n",
      "Iteration 7581: loss 0.000905433320440352\n",
      "Iteration 7582: loss 0.000905433320440352\n",
      "Iteration 7583: loss 0.0009054330876097083\n",
      "Iteration 7584: loss 0.000905433320440352\n",
      "Iteration 7585: loss 0.000905433320440352\n",
      "Iteration 7586: loss 0.0009054330876097083\n",
      "Iteration 7587: loss 0.000905433320440352\n",
      "Iteration 7588: loss 0.000905433320440352\n",
      "Iteration 7589: loss 0.0009054330876097083\n",
      "Iteration 7590: loss 0.000905433320440352\n",
      "Iteration 7591: loss 0.000905433320440352\n",
      "Iteration 7592: loss 0.0009054330876097083\n",
      "Iteration 7593: loss 0.000905433320440352\n",
      "Iteration 7594: loss 0.000905433320440352\n",
      "Iteration 7595: loss 0.0009054330876097083\n",
      "Iteration 7596: loss 0.000905433320440352\n",
      "Iteration 7597: loss 0.000905433320440352\n",
      "Iteration 7598: loss 0.0009054330876097083\n",
      "Iteration 7599: loss 0.000905433320440352\n",
      "Iteration 7600: loss 0.000905433320440352\n",
      "Iteration 7601: loss 0.0009054330876097083\n",
      "Iteration 7602: loss 0.000905433320440352\n",
      "Iteration 7603: loss 0.000905433320440352\n",
      "Iteration 7604: loss 0.0009054330876097083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7605: loss 0.000905433320440352\n",
      "Iteration 7606: loss 0.000905433320440352\n",
      "Iteration 7607: loss 0.0009054330876097083\n",
      "Iteration 7608: loss 0.000905433320440352\n",
      "Iteration 7609: loss 0.000905433320440352\n",
      "Iteration 7610: loss 0.0009054330876097083\n",
      "Iteration 7611: loss 0.000905433320440352\n",
      "Iteration 7612: loss 0.000905433320440352\n",
      "Iteration 7613: loss 0.0009054330876097083\n",
      "Iteration 7614: loss 0.000905433320440352\n",
      "Iteration 7615: loss 0.000905433320440352\n",
      "Iteration 7616: loss 0.0009054330876097083\n",
      "Iteration 7617: loss 0.000905433320440352\n",
      "Iteration 7618: loss 0.000905433320440352\n",
      "Iteration 7619: loss 0.0009054330876097083\n",
      "Iteration 7620: loss 0.000905433320440352\n",
      "Iteration 7621: loss 0.000905433320440352\n",
      "Iteration 7622: loss 0.0009054330876097083\n",
      "Iteration 7623: loss 0.000905433320440352\n",
      "Iteration 7624: loss 0.000905433320440352\n",
      "Iteration 7625: loss 0.0009054330876097083\n",
      "Iteration 7626: loss 0.000905433320440352\n",
      "Iteration 7627: loss 0.000905433320440352\n",
      "Iteration 7628: loss 0.0009054330876097083\n",
      "Iteration 7629: loss 0.000905433320440352\n",
      "Iteration 7630: loss 0.000905433320440352\n",
      "Iteration 7631: loss 0.0009054330876097083\n",
      "Iteration 7632: loss 0.000905433320440352\n",
      "Iteration 7633: loss 0.000905433320440352\n",
      "Iteration 7634: loss 0.0009054330876097083\n",
      "Iteration 7635: loss 0.000905433320440352\n",
      "Iteration 7636: loss 0.000905433320440352\n",
      "Iteration 7637: loss 0.0009054330876097083\n",
      "Iteration 7638: loss 0.000905433320440352\n",
      "Iteration 7639: loss 0.000905433320440352\n",
      "Iteration 7640: loss 0.0009054330876097083\n",
      "Iteration 7641: loss 0.000905433320440352\n",
      "Iteration 7642: loss 0.000905433320440352\n",
      "Iteration 7643: loss 0.0009054330876097083\n",
      "Iteration 7644: loss 0.000905433320440352\n",
      "Iteration 7645: loss 0.000905433320440352\n",
      "Iteration 7646: loss 0.0009054330876097083\n",
      "Iteration 7647: loss 0.000905433320440352\n",
      "Iteration 7648: loss 0.000905433320440352\n",
      "Iteration 7649: loss 0.0009054330876097083\n",
      "Iteration 7650: loss 0.000905433320440352\n",
      "Iteration 7651: loss 0.000905433320440352\n",
      "Iteration 7652: loss 0.0009054330876097083\n",
      "Iteration 7653: loss 0.000905433320440352\n",
      "Iteration 7654: loss 0.000905433320440352\n",
      "Iteration 7655: loss 0.0009054330876097083\n",
      "Iteration 7656: loss 0.000905433320440352\n",
      "Iteration 7657: loss 0.000905433320440352\n",
      "Iteration 7658: loss 0.0009054330876097083\n",
      "Iteration 7659: loss 0.000905433320440352\n",
      "Iteration 7660: loss 0.000905433320440352\n",
      "Iteration 7661: loss 0.0009054330876097083\n",
      "Iteration 7662: loss 0.000905433320440352\n",
      "Iteration 7663: loss 0.000905433320440352\n",
      "Iteration 7664: loss 0.0009054330876097083\n",
      "Iteration 7665: loss 0.000905433320440352\n",
      "Iteration 7666: loss 0.000905433320440352\n",
      "Iteration 7667: loss 0.0009054330876097083\n",
      "Iteration 7668: loss 0.000905433320440352\n",
      "Iteration 7669: loss 0.000905433320440352\n",
      "Iteration 7670: loss 0.0009054330876097083\n",
      "Iteration 7671: loss 0.000905433320440352\n",
      "Iteration 7672: loss 0.000905433320440352\n",
      "Iteration 7673: loss 0.0009054330876097083\n",
      "Iteration 7674: loss 0.000905433320440352\n",
      "Iteration 7675: loss 0.000905433320440352\n",
      "Iteration 7676: loss 0.0009054330876097083\n",
      "Iteration 7677: loss 0.000905433320440352\n",
      "Iteration 7678: loss 0.000905433320440352\n",
      "Iteration 7679: loss 0.0009054330876097083\n",
      "Iteration 7680: loss 0.000905433320440352\n",
      "Iteration 7681: loss 0.000905433320440352\n",
      "Iteration 7682: loss 0.0009054330876097083\n",
      "Iteration 7683: loss 0.000905433320440352\n",
      "Iteration 7684: loss 0.000905433320440352\n",
      "Iteration 7685: loss 0.0009054330876097083\n",
      "Iteration 7686: loss 0.000905433320440352\n",
      "Iteration 7687: loss 0.000905433320440352\n",
      "Iteration 7688: loss 0.0009054330876097083\n",
      "Iteration 7689: loss 0.000905433320440352\n",
      "Iteration 7690: loss 0.000905433320440352\n",
      "Iteration 7691: loss 0.0009054330876097083\n",
      "Iteration 7692: loss 0.000905433320440352\n",
      "Iteration 7693: loss 0.000905433320440352\n",
      "Iteration 7694: loss 0.0009054330876097083\n",
      "Iteration 7695: loss 0.000905433320440352\n",
      "Iteration 7696: loss 0.000905433320440352\n",
      "Iteration 7697: loss 0.0009054330876097083\n",
      "Iteration 7698: loss 0.000905433320440352\n",
      "Iteration 7699: loss 0.000905433320440352\n",
      "Iteration 7700: loss 0.0009054330876097083\n",
      "Iteration 7701: loss 0.000905433320440352\n",
      "Iteration 7702: loss 0.000905433320440352\n",
      "Iteration 7703: loss 0.0009054330876097083\n",
      "Iteration 7704: loss 0.000905433320440352\n",
      "Iteration 7705: loss 0.000905433320440352\n",
      "Iteration 7706: loss 0.0009054330876097083\n",
      "Iteration 7707: loss 0.000905433320440352\n",
      "Iteration 7708: loss 0.000905433320440352\n",
      "Iteration 7709: loss 0.0009054330876097083\n",
      "Iteration 7710: loss 0.000905433320440352\n",
      "Iteration 7711: loss 0.000905433320440352\n",
      "Iteration 7712: loss 0.0009054330876097083\n",
      "Iteration 7713: loss 0.000905433320440352\n",
      "Iteration 7714: loss 0.000905433320440352\n",
      "Iteration 7715: loss 0.0009054330876097083\n",
      "Iteration 7716: loss 0.000905433320440352\n",
      "Iteration 7717: loss 0.000905433320440352\n",
      "Iteration 7718: loss 0.0009054330876097083\n",
      "Iteration 7719: loss 0.000905433320440352\n",
      "Iteration 7720: loss 0.000905433320440352\n",
      "Iteration 7721: loss 0.0009054330876097083\n",
      "Iteration 7722: loss 0.000905433320440352\n",
      "Iteration 7723: loss 0.000905433320440352\n",
      "Iteration 7724: loss 0.0009054330876097083\n",
      "Iteration 7725: loss 0.000905433320440352\n",
      "Iteration 7726: loss 0.000905433320440352\n",
      "Iteration 7727: loss 0.0009054330876097083\n",
      "Iteration 7728: loss 0.000905433320440352\n",
      "Iteration 7729: loss 0.000905433320440352\n",
      "Iteration 7730: loss 0.0009054330876097083\n",
      "Iteration 7731: loss 0.000905433320440352\n",
      "Iteration 7732: loss 0.000905433320440352\n",
      "Iteration 7733: loss 0.0009054330876097083\n",
      "Iteration 7734: loss 0.000905433320440352\n",
      "Iteration 7735: loss 0.000905433320440352\n",
      "Iteration 7736: loss 0.0009054330876097083\n",
      "Iteration 7737: loss 0.000905433320440352\n",
      "Iteration 7738: loss 0.000905433320440352\n",
      "Iteration 7739: loss 0.0009054330876097083\n",
      "Iteration 7740: loss 0.000905433320440352\n",
      "Iteration 7741: loss 0.000905433320440352\n",
      "Iteration 7742: loss 0.0009054330876097083\n",
      "Iteration 7743: loss 0.000905433320440352\n",
      "Iteration 7744: loss 0.000905433320440352\n",
      "Iteration 7745: loss 0.0009054330876097083\n",
      "Iteration 7746: loss 0.000905433320440352\n",
      "Iteration 7747: loss 0.000905433320440352\n",
      "Iteration 7748: loss 0.0009054330876097083\n",
      "Iteration 7749: loss 0.000905433320440352\n",
      "Iteration 7750: loss 0.000905433320440352\n",
      "Iteration 7751: loss 0.0009054330876097083\n",
      "Iteration 7752: loss 0.000905433320440352\n",
      "Iteration 7753: loss 0.000905433320440352\n",
      "Iteration 7754: loss 0.0009054330876097083\n",
      "Iteration 7755: loss 0.000905433320440352\n",
      "Iteration 7756: loss 0.000905433320440352\n",
      "Iteration 7757: loss 0.0009054330876097083\n",
      "Iteration 7758: loss 0.000905433320440352\n",
      "Iteration 7759: loss 0.000905433320440352\n",
      "Iteration 7760: loss 0.0009054330876097083\n",
      "Iteration 7761: loss 0.000905433320440352\n",
      "Iteration 7762: loss 0.000905433320440352\n",
      "Iteration 7763: loss 0.0009054330876097083\n",
      "Iteration 7764: loss 0.000905433320440352\n",
      "Iteration 7765: loss 0.000905433320440352\n",
      "Iteration 7766: loss 0.0009054330876097083\n",
      "Iteration 7767: loss 0.000905433320440352\n",
      "Iteration 7768: loss 0.000905433320440352\n",
      "Iteration 7769: loss 0.0009054330876097083\n",
      "Iteration 7770: loss 0.000905433320440352\n",
      "Iteration 7771: loss 0.000905433320440352\n",
      "Iteration 7772: loss 0.0009054330876097083\n",
      "Iteration 7773: loss 0.000905433320440352\n",
      "Iteration 7774: loss 0.000905433320440352\n",
      "Iteration 7775: loss 0.0009054330876097083\n",
      "Iteration 7776: loss 0.000905433320440352\n",
      "Iteration 7777: loss 0.000905433320440352\n",
      "Iteration 7778: loss 0.0009054330876097083\n",
      "Iteration 7779: loss 0.000905433320440352\n",
      "Iteration 7780: loss 0.000905433320440352\n",
      "Iteration 7781: loss 0.0009054330876097083\n",
      "Iteration 7782: loss 0.000905433320440352\n",
      "Iteration 7783: loss 0.000905433320440352\n",
      "Iteration 7784: loss 0.0009054330876097083\n",
      "Iteration 7785: loss 0.000905433320440352\n",
      "Iteration 7786: loss 0.000905433320440352\n",
      "Iteration 7787: loss 0.0009054330876097083\n",
      "Iteration 7788: loss 0.000905433320440352\n",
      "Iteration 7789: loss 0.000905433320440352\n",
      "Iteration 7790: loss 0.0009054330876097083\n",
      "Iteration 7791: loss 0.000905433320440352\n",
      "Iteration 7792: loss 0.000905433320440352\n",
      "Iteration 7793: loss 0.0009054330876097083\n",
      "Iteration 7794: loss 0.000905433320440352\n",
      "Iteration 7795: loss 0.000905433320440352\n",
      "Iteration 7796: loss 0.0009054330876097083\n",
      "Iteration 7797: loss 0.000905433320440352\n",
      "Iteration 7798: loss 0.000905433320440352\n",
      "Iteration 7799: loss 0.0009054330876097083\n",
      "Iteration 7800: loss 0.000905433320440352\n",
      "Iteration 7801: loss 0.000905433320440352\n",
      "Iteration 7802: loss 0.0009054330876097083\n",
      "Iteration 7803: loss 0.000905433320440352\n",
      "Iteration 7804: loss 0.000905433320440352\n",
      "Iteration 7805: loss 0.0009054330876097083\n",
      "Iteration 7806: loss 0.000905433320440352\n",
      "Iteration 7807: loss 0.000905433320440352\n",
      "Iteration 7808: loss 0.0009054330876097083\n",
      "Iteration 7809: loss 0.000905433320440352\n",
      "Iteration 7810: loss 0.000905433320440352\n",
      "Iteration 7811: loss 0.0009054330876097083\n",
      "Iteration 7812: loss 0.000905433320440352\n",
      "Iteration 7813: loss 0.000905433320440352\n",
      "Iteration 7814: loss 0.0009054330876097083\n",
      "Iteration 7815: loss 0.000905433320440352\n",
      "Iteration 7816: loss 0.000905433320440352\n",
      "Iteration 7817: loss 0.0009054330876097083\n",
      "Iteration 7818: loss 0.000905433320440352\n",
      "Iteration 7819: loss 0.000905433320440352\n",
      "Iteration 7820: loss 0.0009054330876097083\n",
      "Iteration 7821: loss 0.000905433320440352\n",
      "Iteration 7822: loss 0.000905433320440352\n",
      "Iteration 7823: loss 0.0009054330876097083\n",
      "Iteration 7824: loss 0.000905433320440352\n",
      "Iteration 7825: loss 0.000905433320440352\n",
      "Iteration 7826: loss 0.0009054330876097083\n",
      "Iteration 7827: loss 0.000905433320440352\n",
      "Iteration 7828: loss 0.000905433320440352\n",
      "Iteration 7829: loss 0.0009054330876097083\n",
      "Iteration 7830: loss 0.000905433320440352\n",
      "Iteration 7831: loss 0.000905433320440352\n",
      "Iteration 7832: loss 0.0009054330876097083\n",
      "Iteration 7833: loss 0.000905433320440352\n",
      "Iteration 7834: loss 0.000905433320440352\n",
      "Iteration 7835: loss 0.0009054330876097083\n",
      "Iteration 7836: loss 0.000905433320440352\n",
      "Iteration 7837: loss 0.000905433320440352\n",
      "Iteration 7838: loss 0.0009054330876097083\n",
      "Iteration 7839: loss 0.000905433320440352\n",
      "Iteration 7840: loss 0.000905433320440352\n",
      "Iteration 7841: loss 0.0009054330876097083\n",
      "Iteration 7842: loss 0.000905433320440352\n",
      "Iteration 7843: loss 0.000905433320440352\n",
      "Iteration 7844: loss 0.0009054330876097083\n",
      "Iteration 7845: loss 0.000905433320440352\n",
      "Iteration 7846: loss 0.000905433320440352\n",
      "Iteration 7847: loss 0.0009054330876097083\n",
      "Iteration 7848: loss 0.000905433320440352\n",
      "Iteration 7849: loss 0.000905433320440352\n",
      "Iteration 7850: loss 0.0009054330876097083\n",
      "Iteration 7851: loss 0.000905433320440352\n",
      "Iteration 7852: loss 0.000905433320440352\n",
      "Iteration 7853: loss 0.0009054330876097083\n",
      "Iteration 7854: loss 0.000905433320440352\n",
      "Iteration 7855: loss 0.000905433320440352\n",
      "Iteration 7856: loss 0.0009054330876097083\n",
      "Iteration 7857: loss 0.000905433320440352\n",
      "Iteration 7858: loss 0.000905433320440352\n",
      "Iteration 7859: loss 0.0009054330876097083\n",
      "Iteration 7860: loss 0.000905433320440352\n",
      "Iteration 7861: loss 0.000905433320440352\n",
      "Iteration 7862: loss 0.0009054330876097083\n",
      "Iteration 7863: loss 0.000905433320440352\n",
      "Iteration 7864: loss 0.000905433320440352\n",
      "Iteration 7865: loss 0.0009054330876097083\n",
      "Iteration 7866: loss 0.000905433320440352\n",
      "Iteration 7867: loss 0.000905433320440352\n",
      "Iteration 7868: loss 0.0009054330876097083\n",
      "Iteration 7869: loss 0.000905433320440352\n",
      "Iteration 7870: loss 0.000905433320440352\n",
      "Iteration 7871: loss 0.0009054330876097083\n",
      "Iteration 7872: loss 0.000905433320440352\n",
      "Iteration 7873: loss 0.000905433320440352\n",
      "Iteration 7874: loss 0.0009054330876097083\n",
      "Iteration 7875: loss 0.000905433320440352\n",
      "Iteration 7876: loss 0.000905433320440352\n",
      "Iteration 7877: loss 0.0009054330876097083\n",
      "Iteration 7878: loss 0.000905433320440352\n",
      "Iteration 7879: loss 0.000905433320440352\n",
      "Iteration 7880: loss 0.0009054330876097083\n",
      "Iteration 7881: loss 0.000905433320440352\n",
      "Iteration 7882: loss 0.000905433320440352\n",
      "Iteration 7883: loss 0.0009054330876097083\n",
      "Iteration 7884: loss 0.000905433320440352\n",
      "Iteration 7885: loss 0.000905433320440352\n",
      "Iteration 7886: loss 0.0009054330876097083\n",
      "Iteration 7887: loss 0.000905433320440352\n",
      "Iteration 7888: loss 0.000905433320440352\n",
      "Iteration 7889: loss 0.0009054330876097083\n",
      "Iteration 7890: loss 0.000905433320440352\n",
      "Iteration 7891: loss 0.000905433320440352\n",
      "Iteration 7892: loss 0.0009054330876097083\n",
      "Iteration 7893: loss 0.000905433320440352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7894: loss 0.000905433320440352\n",
      "Iteration 7895: loss 0.0009054330876097083\n",
      "Iteration 7896: loss 0.000905433320440352\n",
      "Iteration 7897: loss 0.000905433320440352\n",
      "Iteration 7898: loss 0.0009054330876097083\n",
      "Iteration 7899: loss 0.000905433320440352\n",
      "Iteration 7900: loss 0.000905433320440352\n",
      "Iteration 7901: loss 0.0009054330876097083\n",
      "Iteration 7902: loss 0.000905433320440352\n",
      "Iteration 7903: loss 0.000905433320440352\n",
      "Iteration 7904: loss 0.0009054330876097083\n",
      "Iteration 7905: loss 0.000905433320440352\n",
      "Iteration 7906: loss 0.000905433320440352\n",
      "Iteration 7907: loss 0.0009054330876097083\n",
      "Iteration 7908: loss 0.000905433320440352\n",
      "Iteration 7909: loss 0.000905433320440352\n",
      "Iteration 7910: loss 0.0009054330876097083\n",
      "Iteration 7911: loss 0.000905433320440352\n",
      "Iteration 7912: loss 0.000905433320440352\n",
      "Iteration 7913: loss 0.0009054330876097083\n",
      "Iteration 7914: loss 0.000905433320440352\n",
      "Iteration 7915: loss 0.000905433320440352\n",
      "Iteration 7916: loss 0.0009054330876097083\n",
      "Iteration 7917: loss 0.000905433320440352\n",
      "Iteration 7918: loss 0.000905433320440352\n",
      "Iteration 7919: loss 0.0009054330876097083\n",
      "Iteration 7920: loss 0.000905433320440352\n",
      "Iteration 7921: loss 0.000905433320440352\n",
      "Iteration 7922: loss 0.0009054330876097083\n",
      "Iteration 7923: loss 0.000905433320440352\n",
      "Iteration 7924: loss 0.000905433320440352\n",
      "Iteration 7925: loss 0.0009054330876097083\n",
      "Iteration 7926: loss 0.000905433320440352\n",
      "Iteration 7927: loss 0.000905433320440352\n",
      "Iteration 7928: loss 0.0009054330876097083\n",
      "Iteration 7929: loss 0.000905433320440352\n",
      "Iteration 7930: loss 0.000905433320440352\n",
      "Iteration 7931: loss 0.0009054330876097083\n",
      "Iteration 7932: loss 0.000905433320440352\n",
      "Iteration 7933: loss 0.000905433320440352\n",
      "Iteration 7934: loss 0.0009054330876097083\n",
      "Iteration 7935: loss 0.000905433320440352\n",
      "Iteration 7936: loss 0.000905433320440352\n",
      "Iteration 7937: loss 0.0009054330876097083\n",
      "Iteration 7938: loss 0.000905433320440352\n",
      "Iteration 7939: loss 0.000905433320440352\n",
      "Iteration 7940: loss 0.0009054330876097083\n",
      "Iteration 7941: loss 0.000905433320440352\n",
      "Iteration 7942: loss 0.000905433320440352\n",
      "Iteration 7943: loss 0.0009054330876097083\n",
      "Iteration 7944: loss 0.000905433320440352\n",
      "Iteration 7945: loss 0.000905433320440352\n",
      "Iteration 7946: loss 0.0009054330876097083\n",
      "Iteration 7947: loss 0.000905433320440352\n",
      "Iteration 7948: loss 0.000905433320440352\n",
      "Iteration 7949: loss 0.0009054330876097083\n",
      "Iteration 7950: loss 0.000905433320440352\n",
      "Iteration 7951: loss 0.000905433320440352\n",
      "Iteration 7952: loss 0.0009054330876097083\n",
      "Iteration 7953: loss 0.000905433320440352\n",
      "Iteration 7954: loss 0.000905433320440352\n",
      "Iteration 7955: loss 0.0009054330876097083\n",
      "Iteration 7956: loss 0.000905433320440352\n",
      "Iteration 7957: loss 0.000905433320440352\n",
      "Iteration 7958: loss 0.0009054330876097083\n",
      "Iteration 7959: loss 0.000905433320440352\n",
      "Iteration 7960: loss 0.000905433320440352\n",
      "Iteration 7961: loss 0.0009054330876097083\n",
      "Iteration 7962: loss 0.000905433320440352\n",
      "Iteration 7963: loss 0.000905433320440352\n",
      "Iteration 7964: loss 0.0009054330876097083\n",
      "Iteration 7965: loss 0.000905433320440352\n",
      "Iteration 7966: loss 0.000905433320440352\n",
      "Iteration 7967: loss 0.0009054330876097083\n",
      "Iteration 7968: loss 0.000905433320440352\n",
      "Iteration 7969: loss 0.000905433320440352\n",
      "Iteration 7970: loss 0.0009054330876097083\n",
      "Iteration 7971: loss 0.000905433320440352\n",
      "Iteration 7972: loss 0.000905433320440352\n",
      "Iteration 7973: loss 0.0009054330876097083\n",
      "Iteration 7974: loss 0.000905433320440352\n",
      "Iteration 7975: loss 0.000905433320440352\n",
      "Iteration 7976: loss 0.0009054330876097083\n",
      "Iteration 7977: loss 0.000905433320440352\n",
      "Iteration 7978: loss 0.000905433320440352\n",
      "Iteration 7979: loss 0.0009054330876097083\n",
      "Iteration 7980: loss 0.000905433320440352\n",
      "Iteration 7981: loss 0.000905433320440352\n",
      "Iteration 7982: loss 0.0009054330876097083\n",
      "Iteration 7983: loss 0.000905433320440352\n",
      "Iteration 7984: loss 0.000905433320440352\n",
      "Iteration 7985: loss 0.0009054330876097083\n",
      "Iteration 7986: loss 0.000905433320440352\n",
      "Iteration 7987: loss 0.000905433320440352\n",
      "Iteration 7988: loss 0.0009054330876097083\n",
      "Iteration 7989: loss 0.000905433320440352\n",
      "Iteration 7990: loss 0.000905433320440352\n",
      "Iteration 7991: loss 0.0009054330876097083\n",
      "Iteration 7992: loss 0.000905433320440352\n",
      "Iteration 7993: loss 0.000905433320440352\n",
      "Iteration 7994: loss 0.0009054330876097083\n",
      "Iteration 7995: loss 0.000905433320440352\n",
      "Iteration 7996: loss 0.000905433320440352\n",
      "Iteration 7997: loss 0.0009054330876097083\n",
      "Iteration 7998: loss 0.000905433320440352\n",
      "Iteration 7999: loss 0.000905433320440352\n",
      "Iteration 8000: loss 0.0009054330876097083\n",
      "Iteration 8001: loss 0.000905433320440352\n",
      "Iteration 8002: loss 0.000905433320440352\n",
      "Iteration 8003: loss 0.0009054330876097083\n",
      "Iteration 8004: loss 0.000905433320440352\n",
      "Iteration 8005: loss 0.000905433320440352\n",
      "Iteration 8006: loss 0.0009054330876097083\n",
      "Iteration 8007: loss 0.000905433320440352\n",
      "Iteration 8008: loss 0.000905433320440352\n",
      "Iteration 8009: loss 0.0009054330876097083\n",
      "Iteration 8010: loss 0.000905433320440352\n",
      "Iteration 8011: loss 0.000905433320440352\n",
      "Iteration 8012: loss 0.0009054330876097083\n",
      "Iteration 8013: loss 0.000905433320440352\n",
      "Iteration 8014: loss 0.000905433320440352\n",
      "Iteration 8015: loss 0.0009054330876097083\n",
      "Iteration 8016: loss 0.000905433320440352\n",
      "Iteration 8017: loss 0.000905433320440352\n",
      "Iteration 8018: loss 0.0009054330876097083\n",
      "Iteration 8019: loss 0.000905433320440352\n",
      "Iteration 8020: loss 0.000905433320440352\n",
      "Iteration 8021: loss 0.0009054330876097083\n",
      "Iteration 8022: loss 0.000905433320440352\n",
      "Iteration 8023: loss 0.000905433320440352\n",
      "Iteration 8024: loss 0.0009054330876097083\n",
      "Iteration 8025: loss 0.000905433320440352\n",
      "Iteration 8026: loss 0.000905433320440352\n",
      "Iteration 8027: loss 0.0009054330876097083\n",
      "Iteration 8028: loss 0.000905433320440352\n",
      "Iteration 8029: loss 0.000905433320440352\n",
      "Iteration 8030: loss 0.0009054330876097083\n",
      "Iteration 8031: loss 0.000905433320440352\n",
      "Iteration 8032: loss 0.000905433320440352\n",
      "Iteration 8033: loss 0.0009054330876097083\n",
      "Iteration 8034: loss 0.000905433320440352\n",
      "Iteration 8035: loss 0.000905433320440352\n",
      "Iteration 8036: loss 0.0009054330876097083\n",
      "Iteration 8037: loss 0.000905433320440352\n",
      "Iteration 8038: loss 0.000905433320440352\n",
      "Iteration 8039: loss 0.0009054330876097083\n",
      "Iteration 8040: loss 0.000905433320440352\n",
      "Iteration 8041: loss 0.000905433320440352\n",
      "Iteration 8042: loss 0.0009054330876097083\n",
      "Iteration 8043: loss 0.000905433320440352\n",
      "Iteration 8044: loss 0.000905433320440352\n",
      "Iteration 8045: loss 0.0009054330876097083\n",
      "Iteration 8046: loss 0.000905433320440352\n",
      "Iteration 8047: loss 0.000905433320440352\n",
      "Iteration 8048: loss 0.0009054330876097083\n",
      "Iteration 8049: loss 0.000905433320440352\n",
      "Iteration 8050: loss 0.000905433320440352\n",
      "Iteration 8051: loss 0.0009054330876097083\n",
      "Iteration 8052: loss 0.000905433320440352\n",
      "Iteration 8053: loss 0.000905433320440352\n",
      "Iteration 8054: loss 0.0009054330876097083\n",
      "Iteration 8055: loss 0.000905433320440352\n",
      "Iteration 8056: loss 0.000905433320440352\n",
      "Iteration 8057: loss 0.0009054330876097083\n",
      "Iteration 8058: loss 0.000905433320440352\n",
      "Iteration 8059: loss 0.000905433320440352\n",
      "Iteration 8060: loss 0.0009054330876097083\n",
      "Iteration 8061: loss 0.000905433320440352\n",
      "Iteration 8062: loss 0.000905433320440352\n",
      "Iteration 8063: loss 0.0009054330876097083\n",
      "Iteration 8064: loss 0.000905433320440352\n",
      "Iteration 8065: loss 0.000905433320440352\n",
      "Iteration 8066: loss 0.0009054330876097083\n",
      "Iteration 8067: loss 0.000905433320440352\n",
      "Iteration 8068: loss 0.000905433320440352\n",
      "Iteration 8069: loss 0.0009054330876097083\n",
      "Iteration 8070: loss 0.000905433320440352\n",
      "Iteration 8071: loss 0.000905433320440352\n",
      "Iteration 8072: loss 0.0009054330876097083\n",
      "Iteration 8073: loss 0.000905433320440352\n",
      "Iteration 8074: loss 0.000905433320440352\n",
      "Iteration 8075: loss 0.0009054330876097083\n",
      "Iteration 8076: loss 0.000905433320440352\n",
      "Iteration 8077: loss 0.000905433320440352\n",
      "Iteration 8078: loss 0.0009054330876097083\n",
      "Iteration 8079: loss 0.000905433320440352\n",
      "Iteration 8080: loss 0.000905433320440352\n",
      "Iteration 8081: loss 0.0009054330876097083\n",
      "Iteration 8082: loss 0.000905433320440352\n",
      "Iteration 8083: loss 0.000905433320440352\n",
      "Iteration 8084: loss 0.0009054330876097083\n",
      "Iteration 8085: loss 0.000905433320440352\n",
      "Iteration 8086: loss 0.000905433320440352\n",
      "Iteration 8087: loss 0.0009054330876097083\n",
      "Iteration 8088: loss 0.000905433320440352\n",
      "Iteration 8089: loss 0.000905433320440352\n",
      "Iteration 8090: loss 0.0009054330876097083\n",
      "Iteration 8091: loss 0.000905433320440352\n",
      "Iteration 8092: loss 0.000905433320440352\n",
      "Iteration 8093: loss 0.0009054330876097083\n",
      "Iteration 8094: loss 0.000905433320440352\n",
      "Iteration 8095: loss 0.000905433320440352\n",
      "Iteration 8096: loss 0.0009054330876097083\n",
      "Iteration 8097: loss 0.000905433320440352\n",
      "Iteration 8098: loss 0.000905433320440352\n",
      "Iteration 8099: loss 0.0009054330876097083\n",
      "Iteration 8100: loss 0.000905433320440352\n",
      "Iteration 8101: loss 0.000905433320440352\n",
      "Iteration 8102: loss 0.0009054330876097083\n",
      "Iteration 8103: loss 0.000905433320440352\n",
      "Iteration 8104: loss 0.000905433320440352\n",
      "Iteration 8105: loss 0.0009054330876097083\n",
      "Iteration 8106: loss 0.000905433320440352\n",
      "Iteration 8107: loss 0.000905433320440352\n",
      "Iteration 8108: loss 0.0009054330876097083\n",
      "Iteration 8109: loss 0.000905433320440352\n",
      "Iteration 8110: loss 0.000905433320440352\n",
      "Iteration 8111: loss 0.0009054330876097083\n",
      "Iteration 8112: loss 0.000905433320440352\n",
      "Iteration 8113: loss 0.000905433320440352\n",
      "Iteration 8114: loss 0.0009054330876097083\n",
      "Iteration 8115: loss 0.000905433320440352\n",
      "Iteration 8116: loss 0.000905433320440352\n",
      "Iteration 8117: loss 0.0009054330876097083\n",
      "Iteration 8118: loss 0.000905433320440352\n",
      "Iteration 8119: loss 0.000905433320440352\n",
      "Iteration 8120: loss 0.0009054330876097083\n",
      "Iteration 8121: loss 0.000905433320440352\n",
      "Iteration 8122: loss 0.000905433320440352\n",
      "Iteration 8123: loss 0.0009054330876097083\n",
      "Iteration 8124: loss 0.000905433320440352\n",
      "Iteration 8125: loss 0.000905433320440352\n",
      "Iteration 8126: loss 0.0009054330876097083\n",
      "Iteration 8127: loss 0.000905433320440352\n",
      "Iteration 8128: loss 0.000905433320440352\n",
      "Iteration 8129: loss 0.0009054330876097083\n",
      "Iteration 8130: loss 0.000905433320440352\n",
      "Iteration 8131: loss 0.000905433320440352\n",
      "Iteration 8132: loss 0.0009054330876097083\n",
      "Iteration 8133: loss 0.000905433320440352\n",
      "Iteration 8134: loss 0.000905433320440352\n",
      "Iteration 8135: loss 0.0009054330876097083\n",
      "Iteration 8136: loss 0.000905433320440352\n",
      "Iteration 8137: loss 0.000905433320440352\n",
      "Iteration 8138: loss 0.0009054330876097083\n",
      "Iteration 8139: loss 0.000905433320440352\n",
      "Iteration 8140: loss 0.000905433320440352\n",
      "Iteration 8141: loss 0.0009054330876097083\n",
      "Iteration 8142: loss 0.000905433320440352\n",
      "Iteration 8143: loss 0.000905433320440352\n",
      "Iteration 8144: loss 0.0009054330876097083\n",
      "Iteration 8145: loss 0.000905433320440352\n",
      "Iteration 8146: loss 0.000905433320440352\n",
      "Iteration 8147: loss 0.0009054330876097083\n",
      "Iteration 8148: loss 0.000905433320440352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8149: loss 0.000905433320440352\n",
      "Iteration 8150: loss 0.0009054330876097083\n",
      "Iteration 8151: loss 0.000905433320440352\n",
      "Iteration 8152: loss 0.000905433320440352\n",
      "Iteration 8153: loss 0.0009054330876097083\n",
      "Iteration 8154: loss 0.000905433320440352\n",
      "Iteration 8155: loss 0.000905433320440352\n",
      "Iteration 8156: loss 0.0009054330876097083\n",
      "Iteration 8157: loss 0.000905433320440352\n",
      "Iteration 8158: loss 0.000905433320440352\n",
      "Iteration 8159: loss 0.0009054330876097083\n",
      "Iteration 8160: loss 0.000905433320440352\n",
      "Iteration 8161: loss 0.000905433320440352\n",
      "Iteration 8162: loss 0.0009054330876097083\n",
      "Iteration 8163: loss 0.000905433320440352\n",
      "Iteration 8164: loss 0.000905433320440352\n",
      "Iteration 8165: loss 0.0009054330876097083\n",
      "Iteration 8166: loss 0.000905433320440352\n",
      "Iteration 8167: loss 0.000905433320440352\n",
      "Iteration 8168: loss 0.0009054330876097083\n",
      "Iteration 8169: loss 0.000905433320440352\n",
      "Iteration 8170: loss 0.000905433320440352\n",
      "Iteration 8171: loss 0.0009054330876097083\n",
      "Iteration 8172: loss 0.000905433320440352\n",
      "Iteration 8173: loss 0.000905433320440352\n",
      "Iteration 8174: loss 0.0009054330876097083\n",
      "Iteration 8175: loss 0.000905433320440352\n",
      "Iteration 8176: loss 0.000905433320440352\n",
      "Iteration 8177: loss 0.0009054330876097083\n",
      "Iteration 8178: loss 0.000905433320440352\n",
      "Iteration 8179: loss 0.000905433320440352\n",
      "Iteration 8180: loss 0.0009054330876097083\n",
      "Iteration 8181: loss 0.000905433320440352\n",
      "Iteration 8182: loss 0.000905433320440352\n",
      "Iteration 8183: loss 0.0009054330876097083\n",
      "Iteration 8184: loss 0.000905433320440352\n",
      "Iteration 8185: loss 0.000905433320440352\n",
      "Iteration 8186: loss 0.0009054330876097083\n",
      "Iteration 8187: loss 0.000905433320440352\n",
      "Iteration 8188: loss 0.000905433320440352\n",
      "Iteration 8189: loss 0.0009054330876097083\n",
      "Iteration 8190: loss 0.000905433320440352\n",
      "Iteration 8191: loss 0.000905433320440352\n",
      "Iteration 8192: loss 0.0009054330876097083\n",
      "Iteration 8193: loss 0.000905433320440352\n",
      "Iteration 8194: loss 0.000905433320440352\n",
      "Iteration 8195: loss 0.0009054330876097083\n",
      "Iteration 8196: loss 0.000905433320440352\n",
      "Iteration 8197: loss 0.000905433320440352\n",
      "Iteration 8198: loss 0.0009054330876097083\n",
      "Iteration 8199: loss 0.000905433320440352\n",
      "Iteration 8200: loss 0.000905433320440352\n",
      "Iteration 8201: loss 0.0009054330876097083\n",
      "Iteration 8202: loss 0.000905433320440352\n",
      "Iteration 8203: loss 0.000905433320440352\n",
      "Iteration 8204: loss 0.0009054330876097083\n",
      "Iteration 8205: loss 0.000905433320440352\n",
      "Iteration 8206: loss 0.000905433320440352\n",
      "Iteration 8207: loss 0.0009054330876097083\n",
      "Iteration 8208: loss 0.000905433320440352\n",
      "Iteration 8209: loss 0.000905433320440352\n",
      "Iteration 8210: loss 0.0009054330876097083\n",
      "Iteration 8211: loss 0.000905433320440352\n",
      "Iteration 8212: loss 0.000905433320440352\n",
      "Iteration 8213: loss 0.0009054330876097083\n",
      "Iteration 8214: loss 0.000905433320440352\n",
      "Iteration 8215: loss 0.000905433320440352\n",
      "Iteration 8216: loss 0.0009054330876097083\n",
      "Iteration 8217: loss 0.000905433320440352\n",
      "Iteration 8218: loss 0.000905433320440352\n",
      "Iteration 8219: loss 0.0009054330876097083\n",
      "Iteration 8220: loss 0.000905433320440352\n",
      "Iteration 8221: loss 0.000905433320440352\n",
      "Iteration 8222: loss 0.0009054330876097083\n",
      "Iteration 8223: loss 0.000905433320440352\n",
      "Iteration 8224: loss 0.000905433320440352\n",
      "Iteration 8225: loss 0.0009054330876097083\n",
      "Iteration 8226: loss 0.000905433320440352\n",
      "Iteration 8227: loss 0.000905433320440352\n",
      "Iteration 8228: loss 0.0009054330876097083\n",
      "Iteration 8229: loss 0.000905433320440352\n",
      "Iteration 8230: loss 0.000905433320440352\n",
      "Iteration 8231: loss 0.0009054330876097083\n",
      "Iteration 8232: loss 0.000905433320440352\n",
      "Iteration 8233: loss 0.000905433320440352\n",
      "Iteration 8234: loss 0.0009054330876097083\n",
      "Iteration 8235: loss 0.000905433320440352\n",
      "Iteration 8236: loss 0.000905433320440352\n",
      "Iteration 8237: loss 0.0009054330876097083\n",
      "Iteration 8238: loss 0.000905433320440352\n",
      "Iteration 8239: loss 0.000905433320440352\n",
      "Iteration 8240: loss 0.0009054330876097083\n",
      "Iteration 8241: loss 0.000905433320440352\n",
      "Iteration 8242: loss 0.000905433320440352\n",
      "Iteration 8243: loss 0.0009054330876097083\n",
      "Iteration 8244: loss 0.000905433320440352\n",
      "Iteration 8245: loss 0.000905433320440352\n",
      "Iteration 8246: loss 0.0009054330876097083\n",
      "Iteration 8247: loss 0.000905433320440352\n",
      "Iteration 8248: loss 0.000905433320440352\n",
      "Iteration 8249: loss 0.0009054330876097083\n",
      "Iteration 8250: loss 0.000905433320440352\n",
      "Iteration 8251: loss 0.000905433320440352\n",
      "Iteration 8252: loss 0.0009054330876097083\n",
      "Iteration 8253: loss 0.000905433320440352\n",
      "Iteration 8254: loss 0.000905433320440352\n",
      "Iteration 8255: loss 0.0009054330876097083\n",
      "Iteration 8256: loss 0.000905433320440352\n",
      "Iteration 8257: loss 0.000905433320440352\n",
      "Iteration 8258: loss 0.0009054330876097083\n",
      "Iteration 8259: loss 0.000905433320440352\n",
      "Iteration 8260: loss 0.000905433320440352\n",
      "Iteration 8261: loss 0.0009054330876097083\n",
      "Iteration 8262: loss 0.000905433320440352\n",
      "Iteration 8263: loss 0.000905433320440352\n",
      "Iteration 8264: loss 0.0009054330876097083\n",
      "Iteration 8265: loss 0.000905433320440352\n",
      "Iteration 8266: loss 0.000905433320440352\n",
      "Iteration 8267: loss 0.0009054330876097083\n",
      "Iteration 8268: loss 0.000905433320440352\n",
      "Iteration 8269: loss 0.000905433320440352\n",
      "Iteration 8270: loss 0.0009054330876097083\n",
      "Iteration 8271: loss 0.000905433320440352\n",
      "Iteration 8272: loss 0.000905433320440352\n",
      "Iteration 8273: loss 0.0009054330876097083\n",
      "Iteration 8274: loss 0.000905433320440352\n",
      "Iteration 8275: loss 0.000905433320440352\n",
      "Iteration 8276: loss 0.0009054330876097083\n",
      "Iteration 8277: loss 0.000905433320440352\n",
      "Iteration 8278: loss 0.000905433320440352\n",
      "Iteration 8279: loss 0.0009054330876097083\n",
      "Iteration 8280: loss 0.000905433320440352\n",
      "Iteration 8281: loss 0.000905433320440352\n",
      "Iteration 8282: loss 0.0009054330876097083\n",
      "Iteration 8283: loss 0.000905433320440352\n",
      "Iteration 8284: loss 0.000905433320440352\n",
      "Iteration 8285: loss 0.0009054330876097083\n",
      "Iteration 8286: loss 0.000905433320440352\n",
      "Iteration 8287: loss 0.000905433320440352\n",
      "Iteration 8288: loss 0.0009054330876097083\n",
      "Iteration 8289: loss 0.000905433320440352\n",
      "Iteration 8290: loss 0.000905433320440352\n",
      "Iteration 8291: loss 0.0009054330876097083\n",
      "Iteration 8292: loss 0.000905433320440352\n",
      "Iteration 8293: loss 0.000905433320440352\n",
      "Iteration 8294: loss 0.0009054330876097083\n",
      "Iteration 8295: loss 0.000905433320440352\n",
      "Iteration 8296: loss 0.000905433320440352\n",
      "Iteration 8297: loss 0.0009054330876097083\n",
      "Iteration 8298: loss 0.000905433320440352\n",
      "Iteration 8299: loss 0.000905433320440352\n",
      "Iteration 8300: loss 0.0009054330876097083\n",
      "Iteration 8301: loss 0.000905433320440352\n",
      "Iteration 8302: loss 0.000905433320440352\n",
      "Iteration 8303: loss 0.0009054330876097083\n",
      "Iteration 8304: loss 0.000905433320440352\n",
      "Iteration 8305: loss 0.000905433320440352\n",
      "Iteration 8306: loss 0.0009054330876097083\n",
      "Iteration 8307: loss 0.000905433320440352\n",
      "Iteration 8308: loss 0.000905433320440352\n",
      "Iteration 8309: loss 0.0009054330876097083\n",
      "Iteration 8310: loss 0.000905433320440352\n",
      "Iteration 8311: loss 0.000905433320440352\n",
      "Iteration 8312: loss 0.0009054330876097083\n",
      "Iteration 8313: loss 0.000905433320440352\n",
      "Iteration 8314: loss 0.000905433320440352\n",
      "Iteration 8315: loss 0.0009054330876097083\n",
      "Iteration 8316: loss 0.000905433320440352\n",
      "Iteration 8317: loss 0.000905433320440352\n",
      "Iteration 8318: loss 0.0009054330876097083\n",
      "Iteration 8319: loss 0.000905433320440352\n",
      "Iteration 8320: loss 0.000905433320440352\n",
      "Iteration 8321: loss 0.0009054330876097083\n",
      "Iteration 8322: loss 0.000905433320440352\n",
      "Iteration 8323: loss 0.000905433320440352\n",
      "Iteration 8324: loss 0.0009054330876097083\n",
      "Iteration 8325: loss 0.000905433320440352\n",
      "Iteration 8326: loss 0.000905433320440352\n",
      "Iteration 8327: loss 0.0009054330876097083\n",
      "Iteration 8328: loss 0.000905433320440352\n",
      "Iteration 8329: loss 0.000905433320440352\n",
      "Iteration 8330: loss 0.0009054330876097083\n",
      "Iteration 8331: loss 0.000905433320440352\n",
      "Iteration 8332: loss 0.000905433320440352\n",
      "Iteration 8333: loss 0.0009054330876097083\n",
      "Iteration 8334: loss 0.000905433320440352\n",
      "Iteration 8335: loss 0.000905433320440352\n",
      "Iteration 8336: loss 0.0009054330876097083\n",
      "Iteration 8337: loss 0.000905433320440352\n",
      "Iteration 8338: loss 0.000905433320440352\n",
      "Iteration 8339: loss 0.0009054330876097083\n",
      "Iteration 8340: loss 0.000905433320440352\n",
      "Iteration 8341: loss 0.000905433320440352\n",
      "Iteration 8342: loss 0.0009054330876097083\n",
      "Iteration 8343: loss 0.000905433320440352\n",
      "Iteration 8344: loss 0.000905433320440352\n",
      "Iteration 8345: loss 0.0009054330876097083\n",
      "Iteration 8346: loss 0.000905433320440352\n",
      "Iteration 8347: loss 0.000905433320440352\n",
      "Iteration 8348: loss 0.0009054330876097083\n",
      "Iteration 8349: loss 0.000905433320440352\n",
      "Iteration 8350: loss 0.000905433320440352\n",
      "Iteration 8351: loss 0.0009054330876097083\n",
      "Iteration 8352: loss 0.000905433320440352\n",
      "Iteration 8353: loss 0.000905433320440352\n",
      "Iteration 8354: loss 0.0009054330876097083\n",
      "Iteration 8355: loss 0.000905433320440352\n",
      "Iteration 8356: loss 0.000905433320440352\n",
      "Iteration 8357: loss 0.0009054330876097083\n",
      "Iteration 8358: loss 0.000905433320440352\n",
      "Iteration 8359: loss 0.000905433320440352\n",
      "Iteration 8360: loss 0.0009054330876097083\n",
      "Iteration 8361: loss 0.000905433320440352\n",
      "Iteration 8362: loss 0.000905433320440352\n",
      "Iteration 8363: loss 0.0009054330876097083\n",
      "Iteration 8364: loss 0.000905433320440352\n",
      "Iteration 8365: loss 0.000905433320440352\n",
      "Iteration 8366: loss 0.0009054330876097083\n",
      "Iteration 8367: loss 0.000905433320440352\n",
      "Iteration 8368: loss 0.000905433320440352\n",
      "Iteration 8369: loss 0.0009054330876097083\n",
      "Iteration 8370: loss 0.000905433320440352\n",
      "Iteration 8371: loss 0.000905433320440352\n",
      "Iteration 8372: loss 0.0009054330876097083\n",
      "Iteration 8373: loss 0.000905433320440352\n",
      "Iteration 8374: loss 0.000905433320440352\n",
      "Iteration 8375: loss 0.0009054330876097083\n",
      "Iteration 8376: loss 0.000905433320440352\n",
      "Iteration 8377: loss 0.000905433320440352\n",
      "Iteration 8378: loss 0.0009054330876097083\n",
      "Iteration 8379: loss 0.000905433320440352\n",
      "Iteration 8380: loss 0.000905433320440352\n",
      "Iteration 8381: loss 0.0009054330876097083\n",
      "Iteration 8382: loss 0.000905433320440352\n",
      "Iteration 8383: loss 0.000905433320440352\n",
      "Iteration 8384: loss 0.0009054330876097083\n",
      "Iteration 8385: loss 0.000905433320440352\n",
      "Iteration 8386: loss 0.000905433320440352\n",
      "Iteration 8387: loss 0.0009054330876097083\n",
      "Iteration 8388: loss 0.000905433320440352\n",
      "Iteration 8389: loss 0.000905433320440352\n",
      "Iteration 8390: loss 0.0009054330876097083\n",
      "Iteration 8391: loss 0.000905433320440352\n",
      "Iteration 8392: loss 0.000905433320440352\n",
      "Iteration 8393: loss 0.0009054330876097083\n",
      "Iteration 8394: loss 0.000905433320440352\n",
      "Iteration 8395: loss 0.000905433320440352\n",
      "Iteration 8396: loss 0.0009054330876097083\n",
      "Iteration 8397: loss 0.000905433320440352\n",
      "Iteration 8398: loss 0.000905433320440352\n",
      "Iteration 8399: loss 0.0009054330876097083\n",
      "Iteration 8400: loss 0.000905433320440352\n",
      "Iteration 8401: loss 0.000905433320440352\n",
      "Iteration 8402: loss 0.0009054330876097083\n",
      "Iteration 8403: loss 0.000905433320440352\n",
      "Iteration 8404: loss 0.000905433320440352\n",
      "Iteration 8405: loss 0.0009054330876097083\n",
      "Iteration 8406: loss 0.000905433320440352\n",
      "Iteration 8407: loss 0.000905433320440352\n",
      "Iteration 8408: loss 0.0009054330876097083\n",
      "Iteration 8409: loss 0.000905433320440352\n",
      "Iteration 8410: loss 0.000905433320440352\n",
      "Iteration 8411: loss 0.0009054330876097083\n",
      "Iteration 8412: loss 0.000905433320440352\n",
      "Iteration 8413: loss 0.000905433320440352\n",
      "Iteration 8414: loss 0.0009054330876097083\n",
      "Iteration 8415: loss 0.000905433320440352\n",
      "Iteration 8416: loss 0.000905433320440352\n",
      "Iteration 8417: loss 0.0009054330876097083\n",
      "Iteration 8418: loss 0.000905433320440352\n",
      "Iteration 8419: loss 0.000905433320440352\n",
      "Iteration 8420: loss 0.0009054330876097083\n",
      "Iteration 8421: loss 0.000905433320440352\n",
      "Iteration 8422: loss 0.000905433320440352\n",
      "Iteration 8423: loss 0.0009054330876097083\n",
      "Iteration 8424: loss 0.000905433320440352\n",
      "Iteration 8425: loss 0.000905433320440352\n",
      "Iteration 8426: loss 0.0009054330876097083\n",
      "Iteration 8427: loss 0.000905433320440352\n",
      "Iteration 8428: loss 0.000905433320440352\n",
      "Iteration 8429: loss 0.0009054330876097083\n",
      "Iteration 8430: loss 0.000905433320440352\n",
      "Iteration 8431: loss 0.000905433320440352\n",
      "Iteration 8432: loss 0.0009054330876097083\n",
      "Iteration 8433: loss 0.000905433320440352\n",
      "Iteration 8434: loss 0.000905433320440352\n",
      "Iteration 8435: loss 0.0009054330876097083\n",
      "Iteration 8436: loss 0.000905433320440352\n",
      "Iteration 8437: loss 0.000905433320440352\n",
      "Iteration 8438: loss 0.0009054330876097083\n",
      "Iteration 8439: loss 0.000905433320440352\n",
      "Iteration 8440: loss 0.000905433320440352\n",
      "Iteration 8441: loss 0.0009054330876097083\n",
      "Iteration 8442: loss 0.000905433320440352\n",
      "Iteration 8443: loss 0.000905433320440352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8444: loss 0.0009054330876097083\n",
      "Iteration 8445: loss 0.000905433320440352\n",
      "Iteration 8446: loss 0.000905433320440352\n",
      "Iteration 8447: loss 0.0009054330876097083\n",
      "Iteration 8448: loss 0.000905433320440352\n",
      "Iteration 8449: loss 0.000905433320440352\n",
      "Iteration 8450: loss 0.0009054330876097083\n",
      "Iteration 8451: loss 0.000905433320440352\n",
      "Iteration 8452: loss 0.000905433320440352\n",
      "Iteration 8453: loss 0.0009054330876097083\n",
      "Iteration 8454: loss 0.000905433320440352\n",
      "Iteration 8455: loss 0.000905433320440352\n",
      "Iteration 8456: loss 0.0009054330876097083\n",
      "Iteration 8457: loss 0.000905433320440352\n",
      "Iteration 8458: loss 0.000905433320440352\n",
      "Iteration 8459: loss 0.0009054330876097083\n",
      "Iteration 8460: loss 0.000905433320440352\n",
      "Iteration 8461: loss 0.000905433320440352\n",
      "Iteration 8462: loss 0.0009054330876097083\n",
      "Iteration 8463: loss 0.000905433320440352\n",
      "Iteration 8464: loss 0.000905433320440352\n",
      "Iteration 8465: loss 0.0009054330876097083\n",
      "Iteration 8466: loss 0.000905433320440352\n",
      "Iteration 8467: loss 0.000905433320440352\n",
      "Iteration 8468: loss 0.0009054330876097083\n",
      "Iteration 8469: loss 0.000905433320440352\n",
      "Iteration 8470: loss 0.000905433320440352\n",
      "Iteration 8471: loss 0.0009054330876097083\n",
      "Iteration 8472: loss 0.000905433320440352\n",
      "Iteration 8473: loss 0.000905433320440352\n",
      "Iteration 8474: loss 0.0009054330876097083\n",
      "Iteration 8475: loss 0.000905433320440352\n",
      "Iteration 8476: loss 0.000905433320440352\n",
      "Iteration 8477: loss 0.0009054330876097083\n",
      "Iteration 8478: loss 0.000905433320440352\n",
      "Iteration 8479: loss 0.000905433320440352\n",
      "Iteration 8480: loss 0.0009054330876097083\n",
      "Iteration 8481: loss 0.000905433320440352\n",
      "Iteration 8482: loss 0.000905433320440352\n",
      "Iteration 8483: loss 0.0009054330876097083\n",
      "Iteration 8484: loss 0.000905433320440352\n",
      "Iteration 8485: loss 0.000905433320440352\n",
      "Iteration 8486: loss 0.0009054330876097083\n",
      "Iteration 8487: loss 0.000905433320440352\n",
      "Iteration 8488: loss 0.000905433320440352\n",
      "Iteration 8489: loss 0.0009054330876097083\n",
      "Iteration 8490: loss 0.000905433320440352\n",
      "Iteration 8491: loss 0.000905433320440352\n",
      "Iteration 8492: loss 0.0009054330876097083\n",
      "Iteration 8493: loss 0.000905433320440352\n",
      "Iteration 8494: loss 0.000905433320440352\n",
      "Iteration 8495: loss 0.0009054330876097083\n",
      "Iteration 8496: loss 0.000905433320440352\n",
      "Iteration 8497: loss 0.000905433320440352\n",
      "Iteration 8498: loss 0.0009054330876097083\n",
      "Iteration 8499: loss 0.000905433320440352\n",
      "Iteration 8500: loss 0.000905433320440352\n",
      "Iteration 8501: loss 0.0009054330876097083\n",
      "Iteration 8502: loss 0.000905433320440352\n",
      "Iteration 8503: loss 0.000905433320440352\n",
      "Iteration 8504: loss 0.0009054330876097083\n",
      "Iteration 8505: loss 0.000905433320440352\n",
      "Iteration 8506: loss 0.000905433320440352\n",
      "Iteration 8507: loss 0.0009054330876097083\n",
      "Iteration 8508: loss 0.000905433320440352\n",
      "Iteration 8509: loss 0.000905433320440352\n",
      "Iteration 8510: loss 0.0009054330876097083\n",
      "Iteration 8511: loss 0.000905433320440352\n",
      "Iteration 8512: loss 0.000905433320440352\n",
      "Iteration 8513: loss 0.0009054330876097083\n",
      "Iteration 8514: loss 0.000905433320440352\n",
      "Iteration 8515: loss 0.000905433320440352\n",
      "Iteration 8516: loss 0.0009054330876097083\n",
      "Iteration 8517: loss 0.000905433320440352\n",
      "Iteration 8518: loss 0.000905433320440352\n",
      "Iteration 8519: loss 0.0009054330876097083\n",
      "Iteration 8520: loss 0.000905433320440352\n",
      "Iteration 8521: loss 0.000905433320440352\n",
      "Iteration 8522: loss 0.0009054330876097083\n",
      "Iteration 8523: loss 0.000905433320440352\n",
      "Iteration 8524: loss 0.000905433320440352\n",
      "Iteration 8525: loss 0.0009054330876097083\n",
      "Iteration 8526: loss 0.000905433320440352\n",
      "Iteration 8527: loss 0.000905433320440352\n",
      "Iteration 8528: loss 0.0009054330876097083\n",
      "Iteration 8529: loss 0.000905433320440352\n",
      "Iteration 8530: loss 0.000905433320440352\n",
      "Iteration 8531: loss 0.0009054330876097083\n",
      "Iteration 8532: loss 0.000905433320440352\n",
      "Iteration 8533: loss 0.000905433320440352\n",
      "Iteration 8534: loss 0.0009054330876097083\n",
      "Iteration 8535: loss 0.000905433320440352\n",
      "Iteration 8536: loss 0.000905433320440352\n",
      "Iteration 8537: loss 0.0009054330876097083\n",
      "Iteration 8538: loss 0.000905433320440352\n",
      "Iteration 8539: loss 0.000905433320440352\n",
      "Iteration 8540: loss 0.0009054330876097083\n",
      "Iteration 8541: loss 0.000905433320440352\n",
      "Iteration 8542: loss 0.000905433320440352\n",
      "Iteration 8543: loss 0.0009054330876097083\n",
      "Iteration 8544: loss 0.000905433320440352\n",
      "Iteration 8545: loss 0.000905433320440352\n",
      "Iteration 8546: loss 0.0009054330876097083\n",
      "Iteration 8547: loss 0.000905433320440352\n",
      "Iteration 8548: loss 0.000905433320440352\n",
      "Iteration 8549: loss 0.0009054330876097083\n",
      "Iteration 8550: loss 0.000905433320440352\n",
      "Iteration 8551: loss 0.000905433320440352\n",
      "Iteration 8552: loss 0.0009054330876097083\n",
      "Iteration 8553: loss 0.000905433320440352\n",
      "Iteration 8554: loss 0.000905433320440352\n",
      "Iteration 8555: loss 0.0009054330876097083\n",
      "Iteration 8556: loss 0.000905433320440352\n",
      "Iteration 8557: loss 0.000905433320440352\n",
      "Iteration 8558: loss 0.0009054330876097083\n",
      "Iteration 8559: loss 0.000905433320440352\n",
      "Iteration 8560: loss 0.000905433320440352\n",
      "Iteration 8561: loss 0.0009054330876097083\n",
      "Iteration 8562: loss 0.000905433320440352\n",
      "Iteration 8563: loss 0.000905433320440352\n",
      "Iteration 8564: loss 0.0009054330876097083\n",
      "Iteration 8565: loss 0.000905433320440352\n",
      "Iteration 8566: loss 0.000905433320440352\n",
      "Iteration 8567: loss 0.0009054330876097083\n",
      "Iteration 8568: loss 0.000905433320440352\n",
      "Iteration 8569: loss 0.000905433320440352\n",
      "Iteration 8570: loss 0.0009054330876097083\n",
      "Iteration 8571: loss 0.000905433320440352\n",
      "Iteration 8572: loss 0.000905433320440352\n",
      "Iteration 8573: loss 0.0009054330876097083\n",
      "Iteration 8574: loss 0.000905433320440352\n",
      "Iteration 8575: loss 0.000905433320440352\n",
      "Iteration 8576: loss 0.0009054330876097083\n",
      "Iteration 8577: loss 0.000905433320440352\n",
      "Iteration 8578: loss 0.000905433320440352\n",
      "Iteration 8579: loss 0.0009054330876097083\n",
      "Iteration 8580: loss 0.000905433320440352\n",
      "Iteration 8581: loss 0.000905433320440352\n",
      "Iteration 8582: loss 0.0009054330876097083\n",
      "Iteration 8583: loss 0.000905433320440352\n",
      "Iteration 8584: loss 0.000905433320440352\n",
      "Iteration 8585: loss 0.0009054330876097083\n",
      "Iteration 8586: loss 0.000905433320440352\n",
      "Iteration 8587: loss 0.000905433320440352\n",
      "Iteration 8588: loss 0.0009054330876097083\n",
      "Iteration 8589: loss 0.000905433320440352\n",
      "Iteration 8590: loss 0.000905433320440352\n",
      "Iteration 8591: loss 0.0009054330876097083\n",
      "Iteration 8592: loss 0.000905433320440352\n",
      "Iteration 8593: loss 0.000905433320440352\n",
      "Iteration 8594: loss 0.0009054330876097083\n",
      "Iteration 8595: loss 0.000905433320440352\n",
      "Iteration 8596: loss 0.000905433320440352\n",
      "Iteration 8597: loss 0.0009054330876097083\n",
      "Iteration 8598: loss 0.000905433320440352\n",
      "Iteration 8599: loss 0.000905433320440352\n",
      "Iteration 8600: loss 0.0009054330876097083\n",
      "Iteration 8601: loss 0.000905433320440352\n",
      "Iteration 8602: loss 0.000905433320440352\n",
      "Iteration 8603: loss 0.0009054330876097083\n",
      "Iteration 8604: loss 0.000905433320440352\n",
      "Iteration 8605: loss 0.000905433320440352\n",
      "Iteration 8606: loss 0.0009054330876097083\n",
      "Iteration 8607: loss 0.000905433320440352\n",
      "Iteration 8608: loss 0.000905433320440352\n",
      "Iteration 8609: loss 0.0009054330876097083\n",
      "Iteration 8610: loss 0.000905433320440352\n",
      "Iteration 8611: loss 0.000905433320440352\n",
      "Iteration 8612: loss 0.0009054330876097083\n",
      "Iteration 8613: loss 0.000905433320440352\n",
      "Iteration 8614: loss 0.000905433320440352\n",
      "Iteration 8615: loss 0.0009054330876097083\n",
      "Iteration 8616: loss 0.000905433320440352\n",
      "Iteration 8617: loss 0.000905433320440352\n",
      "Iteration 8618: loss 0.0009054330876097083\n",
      "Iteration 8619: loss 0.000905433320440352\n",
      "Iteration 8620: loss 0.000905433320440352\n",
      "Iteration 8621: loss 0.0009054330876097083\n",
      "Iteration 8622: loss 0.000905433320440352\n",
      "Iteration 8623: loss 0.000905433320440352\n",
      "Iteration 8624: loss 0.0009054330876097083\n",
      "Iteration 8625: loss 0.000905433320440352\n",
      "Iteration 8626: loss 0.000905433320440352\n",
      "Iteration 8627: loss 0.0009054330876097083\n",
      "Iteration 8628: loss 0.000905433320440352\n",
      "Iteration 8629: loss 0.000905433320440352\n",
      "Iteration 8630: loss 0.0009054330876097083\n",
      "Iteration 8631: loss 0.000905433320440352\n",
      "Iteration 8632: loss 0.000905433320440352\n",
      "Iteration 8633: loss 0.0009054330876097083\n",
      "Iteration 8634: loss 0.000905433320440352\n",
      "Iteration 8635: loss 0.000905433320440352\n",
      "Iteration 8636: loss 0.0009054330876097083\n",
      "Iteration 8637: loss 0.000905433320440352\n",
      "Iteration 8638: loss 0.000905433320440352\n",
      "Iteration 8639: loss 0.0009054330876097083\n",
      "Iteration 8640: loss 0.000905433320440352\n",
      "Iteration 8641: loss 0.000905433320440352\n",
      "Iteration 8642: loss 0.0009054330876097083\n",
      "Iteration 8643: loss 0.000905433320440352\n",
      "Iteration 8644: loss 0.000905433320440352\n",
      "Iteration 8645: loss 0.0009054330876097083\n",
      "Iteration 8646: loss 0.000905433320440352\n",
      "Iteration 8647: loss 0.000905433320440352\n",
      "Iteration 8648: loss 0.0009054330876097083\n",
      "Iteration 8649: loss 0.000905433320440352\n",
      "Iteration 8650: loss 0.000905433320440352\n",
      "Iteration 8651: loss 0.0009054330876097083\n",
      "Iteration 8652: loss 0.000905433320440352\n",
      "Iteration 8653: loss 0.000905433320440352\n",
      "Iteration 8654: loss 0.0009054330876097083\n",
      "Iteration 8655: loss 0.000905433320440352\n",
      "Iteration 8656: loss 0.000905433320440352\n",
      "Iteration 8657: loss 0.0009054330876097083\n",
      "Iteration 8658: loss 0.000905433320440352\n",
      "Iteration 8659: loss 0.000905433320440352\n",
      "Iteration 8660: loss 0.0009054330876097083\n",
      "Iteration 8661: loss 0.000905433320440352\n",
      "Iteration 8662: loss 0.000905433320440352\n",
      "Iteration 8663: loss 0.0009054330876097083\n",
      "Iteration 8664: loss 0.000905433320440352\n",
      "Iteration 8665: loss 0.000905433320440352\n",
      "Iteration 8666: loss 0.0009054330876097083\n",
      "Iteration 8667: loss 0.000905433320440352\n",
      "Iteration 8668: loss 0.000905433320440352\n",
      "Iteration 8669: loss 0.0009054330876097083\n",
      "Iteration 8670: loss 0.000905433320440352\n",
      "Iteration 8671: loss 0.000905433320440352\n",
      "Iteration 8672: loss 0.0009054330876097083\n",
      "Iteration 8673: loss 0.000905433320440352\n",
      "Iteration 8674: loss 0.000905433320440352\n",
      "Iteration 8675: loss 0.0009054330876097083\n",
      "Iteration 8676: loss 0.000905433320440352\n",
      "Iteration 8677: loss 0.000905433320440352\n",
      "Iteration 8678: loss 0.0009054330876097083\n",
      "Iteration 8679: loss 0.000905433320440352\n",
      "Iteration 8680: loss 0.000905433320440352\n",
      "Iteration 8681: loss 0.0009054330876097083\n",
      "Iteration 8682: loss 0.000905433320440352\n",
      "Iteration 8683: loss 0.000905433320440352\n",
      "Iteration 8684: loss 0.0009054330876097083\n",
      "Iteration 8685: loss 0.000905433320440352\n",
      "Iteration 8686: loss 0.000905433320440352\n",
      "Iteration 8687: loss 0.0009054330876097083\n",
      "Iteration 8688: loss 0.000905433320440352\n",
      "Iteration 8689: loss 0.000905433320440352\n",
      "Iteration 8690: loss 0.0009054330876097083\n",
      "Iteration 8691: loss 0.000905433320440352\n",
      "Iteration 8692: loss 0.000905433320440352\n",
      "Iteration 8693: loss 0.0009054330876097083\n",
      "Iteration 8694: loss 0.000905433320440352\n",
      "Iteration 8695: loss 0.000905433320440352\n",
      "Iteration 8696: loss 0.0009054330876097083\n",
      "Iteration 8697: loss 0.000905433320440352\n",
      "Iteration 8698: loss 0.000905433320440352\n",
      "Iteration 8699: loss 0.0009054330876097083\n",
      "Iteration 8700: loss 0.000905433320440352\n",
      "Iteration 8701: loss 0.000905433320440352\n",
      "Iteration 8702: loss 0.0009054330876097083\n",
      "Iteration 8703: loss 0.000905433320440352\n",
      "Iteration 8704: loss 0.000905433320440352\n",
      "Iteration 8705: loss 0.0009054330876097083\n",
      "Iteration 8706: loss 0.000905433320440352\n",
      "Iteration 8707: loss 0.000905433320440352\n",
      "Iteration 8708: loss 0.0009054330876097083\n",
      "Iteration 8709: loss 0.000905433320440352\n",
      "Iteration 8710: loss 0.000905433320440352\n",
      "Iteration 8711: loss 0.0009054330876097083\n",
      "Iteration 8712: loss 0.000905433320440352\n",
      "Iteration 8713: loss 0.000905433320440352\n",
      "Iteration 8714: loss 0.0009054330876097083\n",
      "Iteration 8715: loss 0.000905433320440352\n",
      "Iteration 8716: loss 0.000905433320440352\n",
      "Iteration 8717: loss 0.0009054330876097083\n",
      "Iteration 8718: loss 0.000905433320440352\n",
      "Iteration 8719: loss 0.000905433320440352\n",
      "Iteration 8720: loss 0.0009054330876097083\n",
      "Iteration 8721: loss 0.000905433320440352\n",
      "Iteration 8722: loss 0.000905433320440352\n",
      "Iteration 8723: loss 0.0009054330876097083\n",
      "Iteration 8724: loss 0.000905433320440352\n",
      "Iteration 8725: loss 0.000905433320440352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8726: loss 0.0009054330876097083\n",
      "Iteration 8727: loss 0.000905433320440352\n",
      "Iteration 8728: loss 0.000905433320440352\n",
      "Iteration 8729: loss 0.0009054330876097083\n",
      "Iteration 8730: loss 0.000905433320440352\n",
      "Iteration 8731: loss 0.000905433320440352\n",
      "Iteration 8732: loss 0.0009054330876097083\n",
      "Iteration 8733: loss 0.000905433320440352\n",
      "Iteration 8734: loss 0.000905433320440352\n",
      "Iteration 8735: loss 0.0009054330876097083\n",
      "Iteration 8736: loss 0.000905433320440352\n",
      "Iteration 8737: loss 0.000905433320440352\n",
      "Iteration 8738: loss 0.0009054330876097083\n",
      "Iteration 8739: loss 0.000905433320440352\n",
      "Iteration 8740: loss 0.000905433320440352\n",
      "Iteration 8741: loss 0.0009054330876097083\n",
      "Iteration 8742: loss 0.000905433320440352\n",
      "Iteration 8743: loss 0.000905433320440352\n",
      "Iteration 8744: loss 0.0009054330876097083\n",
      "Iteration 8745: loss 0.000905433320440352\n",
      "Iteration 8746: loss 0.000905433320440352\n",
      "Iteration 8747: loss 0.0009054330876097083\n",
      "Iteration 8748: loss 0.000905433320440352\n",
      "Iteration 8749: loss 0.000905433320440352\n",
      "Iteration 8750: loss 0.0009054330876097083\n",
      "Iteration 8751: loss 0.000905433320440352\n",
      "Iteration 8752: loss 0.000905433320440352\n",
      "Iteration 8753: loss 0.0009054330876097083\n",
      "Iteration 8754: loss 0.000905433320440352\n",
      "Iteration 8755: loss 0.000905433320440352\n",
      "Iteration 8756: loss 0.0009054330876097083\n",
      "Iteration 8757: loss 0.000905433320440352\n",
      "Iteration 8758: loss 0.000905433320440352\n",
      "Iteration 8759: loss 0.0009054330876097083\n",
      "Iteration 8760: loss 0.000905433320440352\n",
      "Iteration 8761: loss 0.000905433320440352\n",
      "Iteration 8762: loss 0.0009054330876097083\n",
      "Iteration 8763: loss 0.000905433320440352\n",
      "Iteration 8764: loss 0.000905433320440352\n",
      "Iteration 8765: loss 0.0009054330876097083\n",
      "Iteration 8766: loss 0.000905433320440352\n",
      "Iteration 8767: loss 0.000905433320440352\n",
      "Iteration 8768: loss 0.0009054330876097083\n",
      "Iteration 8769: loss 0.000905433320440352\n",
      "Iteration 8770: loss 0.000905433320440352\n",
      "Iteration 8771: loss 0.0009054330876097083\n",
      "Iteration 8772: loss 0.000905433320440352\n",
      "Iteration 8773: loss 0.000905433320440352\n",
      "Iteration 8774: loss 0.0009054330876097083\n",
      "Iteration 8775: loss 0.000905433320440352\n",
      "Iteration 8776: loss 0.000905433320440352\n",
      "Iteration 8777: loss 0.0009054330876097083\n",
      "Iteration 8778: loss 0.000905433320440352\n",
      "Iteration 8779: loss 0.000905433320440352\n",
      "Iteration 8780: loss 0.0009054330876097083\n",
      "Iteration 8781: loss 0.000905433320440352\n",
      "Iteration 8782: loss 0.000905433320440352\n",
      "Iteration 8783: loss 0.0009054330876097083\n",
      "Iteration 8784: loss 0.000905433320440352\n",
      "Iteration 8785: loss 0.000905433320440352\n",
      "Iteration 8786: loss 0.0009054330876097083\n",
      "Iteration 8787: loss 0.000905433320440352\n",
      "Iteration 8788: loss 0.000905433320440352\n",
      "Iteration 8789: loss 0.0009054330876097083\n",
      "Iteration 8790: loss 0.000905433320440352\n",
      "Iteration 8791: loss 0.000905433320440352\n",
      "Iteration 8792: loss 0.0009054330876097083\n",
      "Iteration 8793: loss 0.000905433320440352\n",
      "Iteration 8794: loss 0.000905433320440352\n",
      "Iteration 8795: loss 0.0009054330876097083\n",
      "Iteration 8796: loss 0.000905433320440352\n",
      "Iteration 8797: loss 0.000905433320440352\n",
      "Iteration 8798: loss 0.0009054330876097083\n",
      "Iteration 8799: loss 0.000905433320440352\n",
      "Iteration 8800: loss 0.000905433320440352\n",
      "Iteration 8801: loss 0.0009054330876097083\n",
      "Iteration 8802: loss 0.000905433320440352\n",
      "Iteration 8803: loss 0.000905433320440352\n",
      "Iteration 8804: loss 0.0009054330876097083\n",
      "Iteration 8805: loss 0.000905433320440352\n",
      "Iteration 8806: loss 0.000905433320440352\n",
      "Iteration 8807: loss 0.0009054330876097083\n",
      "Iteration 8808: loss 0.000905433320440352\n",
      "Iteration 8809: loss 0.000905433320440352\n",
      "Iteration 8810: loss 0.0009054330876097083\n",
      "Iteration 8811: loss 0.000905433320440352\n",
      "Iteration 8812: loss 0.000905433320440352\n",
      "Iteration 8813: loss 0.0009054330876097083\n",
      "Iteration 8814: loss 0.000905433320440352\n",
      "Iteration 8815: loss 0.000905433320440352\n",
      "Iteration 8816: loss 0.0009054330876097083\n",
      "Iteration 8817: loss 0.000905433320440352\n",
      "Iteration 8818: loss 0.000905433320440352\n",
      "Iteration 8819: loss 0.0009054330876097083\n",
      "Iteration 8820: loss 0.000905433320440352\n",
      "Iteration 8821: loss 0.000905433320440352\n",
      "Iteration 8822: loss 0.0009054330876097083\n",
      "Iteration 8823: loss 0.000905433320440352\n",
      "Iteration 8824: loss 0.000905433320440352\n",
      "Iteration 8825: loss 0.0009054330876097083\n",
      "Iteration 8826: loss 0.000905433320440352\n",
      "Iteration 8827: loss 0.000905433320440352\n",
      "Iteration 8828: loss 0.0009054330876097083\n",
      "Iteration 8829: loss 0.000905433320440352\n",
      "Iteration 8830: loss 0.000905433320440352\n",
      "Iteration 8831: loss 0.0009054330876097083\n",
      "Iteration 8832: loss 0.000905433320440352\n",
      "Iteration 8833: loss 0.000905433320440352\n",
      "Iteration 8834: loss 0.0009054330876097083\n",
      "Iteration 8835: loss 0.000905433320440352\n",
      "Iteration 8836: loss 0.000905433320440352\n",
      "Iteration 8837: loss 0.0009054330876097083\n",
      "Iteration 8838: loss 0.000905433320440352\n",
      "Iteration 8839: loss 0.000905433320440352\n",
      "Iteration 8840: loss 0.0009054330876097083\n",
      "Iteration 8841: loss 0.000905433320440352\n",
      "Iteration 8842: loss 0.000905433320440352\n",
      "Iteration 8843: loss 0.0009054330876097083\n",
      "Iteration 8844: loss 0.000905433320440352\n",
      "Iteration 8845: loss 0.000905433320440352\n",
      "Iteration 8846: loss 0.0009054330876097083\n",
      "Iteration 8847: loss 0.000905433320440352\n",
      "Iteration 8848: loss 0.000905433320440352\n",
      "Iteration 8849: loss 0.0009054330876097083\n",
      "Iteration 8850: loss 0.000905433320440352\n",
      "Iteration 8851: loss 0.000905433320440352\n",
      "Iteration 8852: loss 0.0009054330876097083\n",
      "Iteration 8853: loss 0.000905433320440352\n",
      "Iteration 8854: loss 0.000905433320440352\n",
      "Iteration 8855: loss 0.0009054330876097083\n",
      "Iteration 8856: loss 0.000905433320440352\n",
      "Iteration 8857: loss 0.000905433320440352\n",
      "Iteration 8858: loss 0.0009054330876097083\n",
      "Iteration 8859: loss 0.000905433320440352\n",
      "Iteration 8860: loss 0.000905433320440352\n",
      "Iteration 8861: loss 0.0009054330876097083\n",
      "Iteration 8862: loss 0.000905433320440352\n",
      "Iteration 8863: loss 0.000905433320440352\n",
      "Iteration 8864: loss 0.0009054330876097083\n",
      "Iteration 8865: loss 0.000905433320440352\n",
      "Iteration 8866: loss 0.000905433320440352\n",
      "Iteration 8867: loss 0.0009054330876097083\n",
      "Iteration 8868: loss 0.000905433320440352\n",
      "Iteration 8869: loss 0.000905433320440352\n",
      "Iteration 8870: loss 0.0009054330876097083\n",
      "Iteration 8871: loss 0.000905433320440352\n",
      "Iteration 8872: loss 0.000905433320440352\n",
      "Iteration 8873: loss 0.0009054330876097083\n",
      "Iteration 8874: loss 0.000905433320440352\n",
      "Iteration 8875: loss 0.000905433320440352\n",
      "Iteration 8876: loss 0.0009054330876097083\n",
      "Iteration 8877: loss 0.000905433320440352\n",
      "Iteration 8878: loss 0.000905433320440352\n",
      "Iteration 8879: loss 0.0009054330876097083\n",
      "Iteration 8880: loss 0.000905433320440352\n",
      "Iteration 8881: loss 0.000905433320440352\n",
      "Iteration 8882: loss 0.0009054330876097083\n",
      "Iteration 8883: loss 0.000905433320440352\n",
      "Iteration 8884: loss 0.000905433320440352\n",
      "Iteration 8885: loss 0.0009054330876097083\n",
      "Iteration 8886: loss 0.000905433320440352\n",
      "Iteration 8887: loss 0.000905433320440352\n",
      "Iteration 8888: loss 0.0009054330876097083\n",
      "Iteration 8889: loss 0.000905433320440352\n",
      "Iteration 8890: loss 0.000905433320440352\n",
      "Iteration 8891: loss 0.0009054330876097083\n",
      "Iteration 8892: loss 0.000905433320440352\n",
      "Iteration 8893: loss 0.000905433320440352\n",
      "Iteration 8894: loss 0.0009054330876097083\n",
      "Iteration 8895: loss 0.000905433320440352\n",
      "Iteration 8896: loss 0.000905433320440352\n",
      "Iteration 8897: loss 0.0009054330876097083\n",
      "Iteration 8898: loss 0.000905433320440352\n",
      "Iteration 8899: loss 0.000905433320440352\n",
      "Iteration 8900: loss 0.0009054330876097083\n",
      "Iteration 8901: loss 0.000905433320440352\n",
      "Iteration 8902: loss 0.000905433320440352\n",
      "Iteration 8903: loss 0.0009054330876097083\n",
      "Iteration 8904: loss 0.000905433320440352\n",
      "Iteration 8905: loss 0.000905433320440352\n",
      "Iteration 8906: loss 0.0009054330876097083\n",
      "Iteration 8907: loss 0.000905433320440352\n",
      "Iteration 8908: loss 0.000905433320440352\n",
      "Iteration 8909: loss 0.0009054330876097083\n",
      "Iteration 8910: loss 0.000905433320440352\n",
      "Iteration 8911: loss 0.000905433320440352\n",
      "Iteration 8912: loss 0.0009054330876097083\n",
      "Iteration 8913: loss 0.000905433320440352\n",
      "Iteration 8914: loss 0.000905433320440352\n",
      "Iteration 8915: loss 0.0009054330876097083\n",
      "Iteration 8916: loss 0.000905433320440352\n",
      "Iteration 8917: loss 0.000905433320440352\n",
      "Iteration 8918: loss 0.0009054330876097083\n",
      "Iteration 8919: loss 0.000905433320440352\n",
      "Iteration 8920: loss 0.000905433320440352\n",
      "Iteration 8921: loss 0.0009054330876097083\n",
      "Iteration 8922: loss 0.000905433320440352\n",
      "Iteration 8923: loss 0.000905433320440352\n",
      "Iteration 8924: loss 0.0009054330876097083\n",
      "Iteration 8925: loss 0.000905433320440352\n",
      "Iteration 8926: loss 0.000905433320440352\n",
      "Iteration 8927: loss 0.0009054330876097083\n",
      "Iteration 8928: loss 0.000905433320440352\n",
      "Iteration 8929: loss 0.000905433320440352\n",
      "Iteration 8930: loss 0.0009054330876097083\n",
      "Iteration 8931: loss 0.000905433320440352\n",
      "Iteration 8932: loss 0.000905433320440352\n",
      "Iteration 8933: loss 0.0009054330876097083\n",
      "Iteration 8934: loss 0.000905433320440352\n",
      "Iteration 8935: loss 0.000905433320440352\n",
      "Iteration 8936: loss 0.0009054330876097083\n",
      "Iteration 8937: loss 0.000905433320440352\n",
      "Iteration 8938: loss 0.000905433320440352\n",
      "Iteration 8939: loss 0.0009054330876097083\n",
      "Iteration 8940: loss 0.000905433320440352\n",
      "Iteration 8941: loss 0.000905433320440352\n",
      "Iteration 8942: loss 0.0009054330876097083\n",
      "Iteration 8943: loss 0.000905433320440352\n",
      "Iteration 8944: loss 0.000905433320440352\n",
      "Iteration 8945: loss 0.0009054330876097083\n",
      "Iteration 8946: loss 0.000905433320440352\n",
      "Iteration 8947: loss 0.000905433320440352\n",
      "Iteration 8948: loss 0.0009054330876097083\n",
      "Iteration 8949: loss 0.000905433320440352\n",
      "Iteration 8950: loss 0.000905433320440352\n",
      "Iteration 8951: loss 0.0009054330876097083\n",
      "Iteration 8952: loss 0.000905433320440352\n",
      "Iteration 8953: loss 0.000905433320440352\n",
      "Iteration 8954: loss 0.0009054330876097083\n",
      "Iteration 8955: loss 0.000905433320440352\n",
      "Iteration 8956: loss 0.000905433320440352\n",
      "Iteration 8957: loss 0.0009054330876097083\n",
      "Iteration 8958: loss 0.000905433320440352\n",
      "Iteration 8959: loss 0.000905433320440352\n",
      "Iteration 8960: loss 0.0009054330876097083\n",
      "Iteration 8961: loss 0.000905433320440352\n",
      "Iteration 8962: loss 0.000905433320440352\n",
      "Iteration 8963: loss 0.0009054330876097083\n",
      "Iteration 8964: loss 0.000905433320440352\n",
      "Iteration 8965: loss 0.000905433320440352\n",
      "Iteration 8966: loss 0.0009054330876097083\n",
      "Iteration 8967: loss 0.000905433320440352\n",
      "Iteration 8968: loss 0.000905433320440352\n",
      "Iteration 8969: loss 0.0009054330876097083\n",
      "Iteration 8970: loss 0.000905433320440352\n",
      "Iteration 8971: loss 0.000905433320440352\n",
      "Iteration 8972: loss 0.0009054330876097083\n",
      "Iteration 8973: loss 0.000905433320440352\n",
      "Iteration 8974: loss 0.000905433320440352\n",
      "Iteration 8975: loss 0.0009054330876097083\n",
      "Iteration 8976: loss 0.000905433320440352\n",
      "Iteration 8977: loss 0.000905433320440352\n",
      "Iteration 8978: loss 0.0009054330876097083\n",
      "Iteration 8979: loss 0.000905433320440352\n",
      "Iteration 8980: loss 0.000905433320440352\n",
      "Iteration 8981: loss 0.0009054330876097083\n",
      "Iteration 8982: loss 0.000905433320440352\n",
      "Iteration 8983: loss 0.000905433320440352\n",
      "Iteration 8984: loss 0.0009054330876097083\n",
      "Iteration 8985: loss 0.000905433320440352\n",
      "Iteration 8986: loss 0.000905433320440352\n",
      "Iteration 8987: loss 0.0009054330876097083\n",
      "Iteration 8988: loss 0.000905433320440352\n",
      "Iteration 8989: loss 0.000905433320440352\n",
      "Iteration 8990: loss 0.0009054330876097083\n",
      "Iteration 8991: loss 0.000905433320440352\n",
      "Iteration 8992: loss 0.000905433320440352\n",
      "Iteration 8993: loss 0.0009054330876097083\n",
      "Iteration 8994: loss 0.000905433320440352\n",
      "Iteration 8995: loss 0.000905433320440352\n",
      "Iteration 8996: loss 0.0009054330876097083\n",
      "Iteration 8997: loss 0.000905433320440352\n",
      "Iteration 8998: loss 0.000905433320440352\n",
      "Iteration 8999: loss 0.0009054330876097083\n",
      "Iteration 9000: loss 0.000905433320440352\n",
      "Iteration 9001: loss 0.000905433320440352\n",
      "Iteration 9002: loss 0.0009054330876097083\n",
      "Iteration 9003: loss 0.000905433320440352\n",
      "Iteration 9004: loss 0.000905433320440352\n",
      "Iteration 9005: loss 0.0009054330876097083\n",
      "Iteration 9006: loss 0.000905433320440352\n",
      "Iteration 9007: loss 0.000905433320440352\n",
      "Iteration 9008: loss 0.0009054330876097083\n",
      "Iteration 9009: loss 0.000905433320440352\n",
      "Iteration 9010: loss 0.000905433320440352\n",
      "Iteration 9011: loss 0.0009054330876097083\n",
      "Iteration 9012: loss 0.000905433320440352\n",
      "Iteration 9013: loss 0.000905433320440352\n",
      "Iteration 9014: loss 0.0009054330876097083\n",
      "Iteration 9015: loss 0.000905433320440352\n",
      "Iteration 9016: loss 0.000905433320440352\n",
      "Iteration 9017: loss 0.0009054330876097083\n",
      "Iteration 9018: loss 0.000905433320440352\n",
      "Iteration 9019: loss 0.000905433320440352\n",
      "Iteration 9020: loss 0.0009054330876097083\n",
      "Iteration 9021: loss 0.000905433320440352\n",
      "Iteration 9022: loss 0.000905433320440352\n",
      "Iteration 9023: loss 0.0009054330876097083\n",
      "Iteration 9024: loss 0.000905433320440352\n",
      "Iteration 9025: loss 0.000905433320440352\n",
      "Iteration 9026: loss 0.0009054330876097083\n",
      "Iteration 9027: loss 0.000905433320440352\n",
      "Iteration 9028: loss 0.000905433320440352\n",
      "Iteration 9029: loss 0.0009054330876097083\n",
      "Iteration 9030: loss 0.000905433320440352\n",
      "Iteration 9031: loss 0.000905433320440352\n",
      "Iteration 9032: loss 0.0009054330876097083\n",
      "Iteration 9033: loss 0.000905433320440352\n",
      "Iteration 9034: loss 0.000905433320440352\n",
      "Iteration 9035: loss 0.0009054330876097083\n",
      "Iteration 9036: loss 0.000905433320440352\n",
      "Iteration 9037: loss 0.000905433320440352\n",
      "Iteration 9038: loss 0.0009054330876097083\n",
      "Iteration 9039: loss 0.000905433320440352\n",
      "Iteration 9040: loss 0.000905433320440352\n",
      "Iteration 9041: loss 0.0009054330876097083\n",
      "Iteration 9042: loss 0.000905433320440352\n",
      "Iteration 9043: loss 0.000905433320440352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9044: loss 0.0009054330876097083\n",
      "Iteration 9045: loss 0.000905433320440352\n",
      "Iteration 9046: loss 0.000905433320440352\n",
      "Iteration 9047: loss 0.0009054330876097083\n",
      "Iteration 9048: loss 0.000905433320440352\n",
      "Iteration 9049: loss 0.000905433320440352\n",
      "Iteration 9050: loss 0.0009054330876097083\n",
      "Iteration 9051: loss 0.000905433320440352\n",
      "Iteration 9052: loss 0.000905433320440352\n",
      "Iteration 9053: loss 0.0009054330876097083\n",
      "Iteration 9054: loss 0.000905433320440352\n",
      "Iteration 9055: loss 0.000905433320440352\n",
      "Iteration 9056: loss 0.0009054330876097083\n",
      "Iteration 9057: loss 0.000905433320440352\n",
      "Iteration 9058: loss 0.000905433320440352\n",
      "Iteration 9059: loss 0.0009054330876097083\n",
      "Iteration 9060: loss 0.000905433320440352\n",
      "Iteration 9061: loss 0.000905433320440352\n",
      "Iteration 9062: loss 0.0009054330876097083\n",
      "Iteration 9063: loss 0.000905433320440352\n",
      "Iteration 9064: loss 0.000905433320440352\n",
      "Iteration 9065: loss 0.0009054330876097083\n",
      "Iteration 9066: loss 0.000905433320440352\n",
      "Iteration 9067: loss 0.000905433320440352\n",
      "Iteration 9068: loss 0.0009054330876097083\n",
      "Iteration 9069: loss 0.000905433320440352\n",
      "Iteration 9070: loss 0.000905433320440352\n",
      "Iteration 9071: loss 0.0009054330876097083\n",
      "Iteration 9072: loss 0.000905433320440352\n",
      "Iteration 9073: loss 0.000905433320440352\n",
      "Iteration 9074: loss 0.0009054330876097083\n",
      "Iteration 9075: loss 0.000905433320440352\n",
      "Iteration 9076: loss 0.000905433320440352\n",
      "Iteration 9077: loss 0.0009054330876097083\n",
      "Iteration 9078: loss 0.000905433320440352\n",
      "Iteration 9079: loss 0.000905433320440352\n",
      "Iteration 9080: loss 0.0009054330876097083\n",
      "Iteration 9081: loss 0.000905433320440352\n",
      "Iteration 9082: loss 0.000905433320440352\n",
      "Iteration 9083: loss 0.0009054330876097083\n",
      "Iteration 9084: loss 0.000905433320440352\n",
      "Iteration 9085: loss 0.000905433320440352\n",
      "Iteration 9086: loss 0.0009054330876097083\n",
      "Iteration 9087: loss 0.000905433320440352\n",
      "Iteration 9088: loss 0.000905433320440352\n",
      "Iteration 9089: loss 0.0009054330876097083\n",
      "Iteration 9090: loss 0.000905433320440352\n",
      "Iteration 9091: loss 0.000905433320440352\n",
      "Iteration 9092: loss 0.0009054330876097083\n",
      "Iteration 9093: loss 0.000905433320440352\n",
      "Iteration 9094: loss 0.000905433320440352\n",
      "Iteration 9095: loss 0.0009054330876097083\n",
      "Iteration 9096: loss 0.000905433320440352\n",
      "Iteration 9097: loss 0.000905433320440352\n",
      "Iteration 9098: loss 0.0009054330876097083\n",
      "Iteration 9099: loss 0.000905433320440352\n",
      "Iteration 9100: loss 0.000905433320440352\n",
      "Iteration 9101: loss 0.0009054330876097083\n",
      "Iteration 9102: loss 0.000905433320440352\n",
      "Iteration 9103: loss 0.000905433320440352\n",
      "Iteration 9104: loss 0.0009054330876097083\n",
      "Iteration 9105: loss 0.000905433320440352\n",
      "Iteration 9106: loss 0.000905433320440352\n",
      "Iteration 9107: loss 0.0009054330876097083\n",
      "Iteration 9108: loss 0.000905433320440352\n",
      "Iteration 9109: loss 0.000905433320440352\n",
      "Iteration 9110: loss 0.0009054330876097083\n",
      "Iteration 9111: loss 0.000905433320440352\n",
      "Iteration 9112: loss 0.000905433320440352\n",
      "Iteration 9113: loss 0.0009054330876097083\n",
      "Iteration 9114: loss 0.000905433320440352\n",
      "Iteration 9115: loss 0.000905433320440352\n",
      "Iteration 9116: loss 0.0009054330876097083\n",
      "Iteration 9117: loss 0.000905433320440352\n",
      "Iteration 9118: loss 0.000905433320440352\n",
      "Iteration 9119: loss 0.0009054330876097083\n",
      "Iteration 9120: loss 0.000905433320440352\n",
      "Iteration 9121: loss 0.000905433320440352\n",
      "Iteration 9122: loss 0.0009054330876097083\n",
      "Iteration 9123: loss 0.000905433320440352\n",
      "Iteration 9124: loss 0.000905433320440352\n",
      "Iteration 9125: loss 0.0009054330876097083\n",
      "Iteration 9126: loss 0.000905433320440352\n",
      "Iteration 9127: loss 0.000905433320440352\n",
      "Iteration 9128: loss 0.0009054330876097083\n",
      "Iteration 9129: loss 0.000905433320440352\n",
      "Iteration 9130: loss 0.000905433320440352\n",
      "Iteration 9131: loss 0.0009054330876097083\n",
      "Iteration 9132: loss 0.000905433320440352\n",
      "Iteration 9133: loss 0.000905433320440352\n",
      "Iteration 9134: loss 0.0009054330876097083\n",
      "Iteration 9135: loss 0.000905433320440352\n",
      "Iteration 9136: loss 0.000905433320440352\n",
      "Iteration 9137: loss 0.0009054330876097083\n",
      "Iteration 9138: loss 0.000905433320440352\n",
      "Iteration 9139: loss 0.000905433320440352\n",
      "Iteration 9140: loss 0.0009054330876097083\n",
      "Iteration 9141: loss 0.000905433320440352\n",
      "Iteration 9142: loss 0.000905433320440352\n",
      "Iteration 9143: loss 0.0009054330876097083\n",
      "Iteration 9144: loss 0.000905433320440352\n",
      "Iteration 9145: loss 0.000905433320440352\n",
      "Iteration 9146: loss 0.0009054330876097083\n",
      "Iteration 9147: loss 0.000905433320440352\n",
      "Iteration 9148: loss 0.000905433320440352\n",
      "Iteration 9149: loss 0.0009054330876097083\n",
      "Iteration 9150: loss 0.000905433320440352\n",
      "Iteration 9151: loss 0.000905433320440352\n",
      "Iteration 9152: loss 0.0009054330876097083\n",
      "Iteration 9153: loss 0.000905433320440352\n",
      "Iteration 9154: loss 0.000905433320440352\n",
      "Iteration 9155: loss 0.0009054330876097083\n",
      "Iteration 9156: loss 0.000905433320440352\n",
      "Iteration 9157: loss 0.000905433320440352\n",
      "Iteration 9158: loss 0.0009054330876097083\n",
      "Iteration 9159: loss 0.000905433320440352\n",
      "Iteration 9160: loss 0.000905433320440352\n",
      "Iteration 9161: loss 0.0009054330876097083\n",
      "Iteration 9162: loss 0.000905433320440352\n",
      "Iteration 9163: loss 0.000905433320440352\n",
      "Iteration 9164: loss 0.0009054330876097083\n",
      "Iteration 9165: loss 0.000905433320440352\n",
      "Iteration 9166: loss 0.000905433320440352\n",
      "Iteration 9167: loss 0.0009054330876097083\n",
      "Iteration 9168: loss 0.000905433320440352\n",
      "Iteration 9169: loss 0.000905433320440352\n",
      "Iteration 9170: loss 0.0009054330876097083\n",
      "Iteration 9171: loss 0.000905433320440352\n",
      "Iteration 9172: loss 0.000905433320440352\n",
      "Iteration 9173: loss 0.0009054330876097083\n",
      "Iteration 9174: loss 0.000905433320440352\n",
      "Iteration 9175: loss 0.000905433320440352\n",
      "Iteration 9176: loss 0.0009054330876097083\n",
      "Iteration 9177: loss 0.000905433320440352\n",
      "Iteration 9178: loss 0.000905433320440352\n",
      "Iteration 9179: loss 0.0009054330876097083\n",
      "Iteration 9180: loss 0.000905433320440352\n",
      "Iteration 9181: loss 0.000905433320440352\n",
      "Iteration 9182: loss 0.0009054330876097083\n",
      "Iteration 9183: loss 0.000905433320440352\n",
      "Iteration 9184: loss 0.000905433320440352\n",
      "Iteration 9185: loss 0.0009054330876097083\n",
      "Iteration 9186: loss 0.000905433320440352\n",
      "Iteration 9187: loss 0.000905433320440352\n",
      "Iteration 9188: loss 0.0009054330876097083\n",
      "Iteration 9189: loss 0.000905433320440352\n",
      "Iteration 9190: loss 0.000905433320440352\n",
      "Iteration 9191: loss 0.0009054330876097083\n",
      "Iteration 9192: loss 0.000905433320440352\n",
      "Iteration 9193: loss 0.000905433320440352\n",
      "Iteration 9194: loss 0.0009054330876097083\n",
      "Iteration 9195: loss 0.000905433320440352\n",
      "Iteration 9196: loss 0.000905433320440352\n",
      "Iteration 9197: loss 0.0009054330876097083\n",
      "Iteration 9198: loss 0.000905433320440352\n",
      "Iteration 9199: loss 0.000905433320440352\n",
      "Iteration 9200: loss 0.0009054330876097083\n",
      "Iteration 9201: loss 0.000905433320440352\n",
      "Iteration 9202: loss 0.000905433320440352\n",
      "Iteration 9203: loss 0.0009054330876097083\n",
      "Iteration 9204: loss 0.000905433320440352\n",
      "Iteration 9205: loss 0.000905433320440352\n",
      "Iteration 9206: loss 0.0009054330876097083\n",
      "Iteration 9207: loss 0.000905433320440352\n",
      "Iteration 9208: loss 0.000905433320440352\n",
      "Iteration 9209: loss 0.0009054330876097083\n",
      "Iteration 9210: loss 0.000905433320440352\n",
      "Iteration 9211: loss 0.000905433320440352\n",
      "Iteration 9212: loss 0.0009054330876097083\n",
      "Iteration 9213: loss 0.000905433320440352\n",
      "Iteration 9214: loss 0.000905433320440352\n",
      "Iteration 9215: loss 0.0009054330876097083\n",
      "Iteration 9216: loss 0.000905433320440352\n",
      "Iteration 9217: loss 0.000905433320440352\n",
      "Iteration 9218: loss 0.0009054330876097083\n",
      "Iteration 9219: loss 0.000905433320440352\n",
      "Iteration 9220: loss 0.000905433320440352\n",
      "Iteration 9221: loss 0.0009054330876097083\n",
      "Iteration 9222: loss 0.000905433320440352\n",
      "Iteration 9223: loss 0.000905433320440352\n",
      "Iteration 9224: loss 0.0009054330876097083\n",
      "Iteration 9225: loss 0.000905433320440352\n",
      "Iteration 9226: loss 0.000905433320440352\n",
      "Iteration 9227: loss 0.0009054330876097083\n",
      "Iteration 9228: loss 0.000905433320440352\n",
      "Iteration 9229: loss 0.000905433320440352\n",
      "Iteration 9230: loss 0.0009054330876097083\n",
      "Iteration 9231: loss 0.000905433320440352\n",
      "Iteration 9232: loss 0.000905433320440352\n",
      "Iteration 9233: loss 0.0009054330876097083\n",
      "Iteration 9234: loss 0.000905433320440352\n",
      "Iteration 9235: loss 0.000905433320440352\n",
      "Iteration 9236: loss 0.0009054330876097083\n",
      "Iteration 9237: loss 0.000905433320440352\n",
      "Iteration 9238: loss 0.000905433320440352\n",
      "Iteration 9239: loss 0.0009054330876097083\n",
      "Iteration 9240: loss 0.000905433320440352\n",
      "Iteration 9241: loss 0.000905433320440352\n",
      "Iteration 9242: loss 0.0009054330876097083\n",
      "Iteration 9243: loss 0.000905433320440352\n",
      "Iteration 9244: loss 0.000905433320440352\n",
      "Iteration 9245: loss 0.0009054330876097083\n",
      "Iteration 9246: loss 0.000905433320440352\n",
      "Iteration 9247: loss 0.000905433320440352\n",
      "Iteration 9248: loss 0.0009054330876097083\n",
      "Iteration 9249: loss 0.000905433320440352\n",
      "Iteration 9250: loss 0.000905433320440352\n",
      "Iteration 9251: loss 0.0009054330876097083\n",
      "Iteration 9252: loss 0.000905433320440352\n",
      "Iteration 9253: loss 0.000905433320440352\n",
      "Iteration 9254: loss 0.0009054330876097083\n",
      "Iteration 9255: loss 0.000905433320440352\n",
      "Iteration 9256: loss 0.000905433320440352\n",
      "Iteration 9257: loss 0.0009054330876097083\n",
      "Iteration 9258: loss 0.000905433320440352\n",
      "Iteration 9259: loss 0.000905433320440352\n",
      "Iteration 9260: loss 0.0009054330876097083\n",
      "Iteration 9261: loss 0.000905433320440352\n",
      "Iteration 9262: loss 0.000905433320440352\n",
      "Iteration 9263: loss 0.0009054330876097083\n",
      "Iteration 9264: loss 0.000905433320440352\n",
      "Iteration 9265: loss 0.000905433320440352\n",
      "Iteration 9266: loss 0.0009054330876097083\n",
      "Iteration 9267: loss 0.000905433320440352\n",
      "Iteration 9268: loss 0.000905433320440352\n",
      "Iteration 9269: loss 0.0009054330876097083\n",
      "Iteration 9270: loss 0.000905433320440352\n",
      "Iteration 9271: loss 0.000905433320440352\n",
      "Iteration 9272: loss 0.0009054330876097083\n",
      "Iteration 9273: loss 0.000905433320440352\n",
      "Iteration 9274: loss 0.000905433320440352\n",
      "Iteration 9275: loss 0.0009054330876097083\n",
      "Iteration 9276: loss 0.000905433320440352\n",
      "Iteration 9277: loss 0.000905433320440352\n",
      "Iteration 9278: loss 0.0009054330876097083\n",
      "Iteration 9279: loss 0.000905433320440352\n",
      "Iteration 9280: loss 0.000905433320440352\n",
      "Iteration 9281: loss 0.0009054330876097083\n",
      "Iteration 9282: loss 0.000905433320440352\n",
      "Iteration 9283: loss 0.000905433320440352\n",
      "Iteration 9284: loss 0.0009054330876097083\n",
      "Iteration 9285: loss 0.000905433320440352\n",
      "Iteration 9286: loss 0.000905433320440352\n",
      "Iteration 9287: loss 0.0009054330876097083\n",
      "Iteration 9288: loss 0.000905433320440352\n",
      "Iteration 9289: loss 0.000905433320440352\n",
      "Iteration 9290: loss 0.0009054330876097083\n",
      "Iteration 9291: loss 0.000905433320440352\n",
      "Iteration 9292: loss 0.000905433320440352\n",
      "Iteration 9293: loss 0.0009054330876097083\n",
      "Iteration 9294: loss 0.000905433320440352\n",
      "Iteration 9295: loss 0.000905433320440352\n",
      "Iteration 9296: loss 0.0009054330876097083\n",
      "Iteration 9297: loss 0.000905433320440352\n",
      "Iteration 9298: loss 0.000905433320440352\n",
      "Iteration 9299: loss 0.0009054330876097083\n",
      "Iteration 9300: loss 0.000905433320440352\n",
      "Iteration 9301: loss 0.000905433320440352\n",
      "Iteration 9302: loss 0.0009054330876097083\n",
      "Iteration 9303: loss 0.000905433320440352\n",
      "Iteration 9304: loss 0.000905433320440352\n",
      "Iteration 9305: loss 0.0009054330876097083\n",
      "Iteration 9306: loss 0.000905433320440352\n",
      "Iteration 9307: loss 0.000905433320440352\n",
      "Iteration 9308: loss 0.0009054330876097083\n",
      "Iteration 9309: loss 0.000905433320440352\n",
      "Iteration 9310: loss 0.000905433320440352\n",
      "Iteration 9311: loss 0.0009054330876097083\n",
      "Iteration 9312: loss 0.000905433320440352\n",
      "Iteration 9313: loss 0.000905433320440352\n",
      "Iteration 9314: loss 0.0009054330876097083\n",
      "Iteration 9315: loss 0.000905433320440352\n",
      "Iteration 9316: loss 0.000905433320440352\n",
      "Iteration 9317: loss 0.0009054330876097083\n",
      "Iteration 9318: loss 0.000905433320440352\n",
      "Iteration 9319: loss 0.000905433320440352\n",
      "Iteration 9320: loss 0.0009054330876097083\n",
      "Iteration 9321: loss 0.000905433320440352\n",
      "Iteration 9322: loss 0.000905433320440352\n",
      "Iteration 9323: loss 0.0009054330876097083\n",
      "Iteration 9324: loss 0.000905433320440352\n",
      "Iteration 9325: loss 0.000905433320440352\n",
      "Iteration 9326: loss 0.0009054330876097083\n",
      "Iteration 9327: loss 0.000905433320440352\n",
      "Iteration 9328: loss 0.000905433320440352\n",
      "Iteration 9329: loss 0.0009054330876097083\n",
      "Iteration 9330: loss 0.000905433320440352\n",
      "Iteration 9331: loss 0.000905433320440352\n",
      "Iteration 9332: loss 0.0009054330876097083\n",
      "Iteration 9333: loss 0.000905433320440352\n",
      "Iteration 9334: loss 0.000905433320440352\n",
      "Iteration 9335: loss 0.0009054330876097083\n",
      "Iteration 9336: loss 0.000905433320440352\n",
      "Iteration 9337: loss 0.000905433320440352\n",
      "Iteration 9338: loss 0.0009054330876097083\n",
      "Iteration 9339: loss 0.000905433320440352\n",
      "Iteration 9340: loss 0.000905433320440352\n",
      "Iteration 9341: loss 0.0009054330876097083\n",
      "Iteration 9342: loss 0.000905433320440352\n",
      "Iteration 9343: loss 0.000905433320440352\n",
      "Iteration 9344: loss 0.0009054330876097083\n",
      "Iteration 9345: loss 0.000905433320440352\n",
      "Iteration 9346: loss 0.000905433320440352\n",
      "Iteration 9347: loss 0.0009054330876097083\n",
      "Iteration 9348: loss 0.000905433320440352\n",
      "Iteration 9349: loss 0.000905433320440352\n",
      "Iteration 9350: loss 0.0009054330876097083\n",
      "Iteration 9351: loss 0.000905433320440352\n",
      "Iteration 9352: loss 0.000905433320440352\n",
      "Iteration 9353: loss 0.0009054330876097083\n",
      "Iteration 9354: loss 0.000905433320440352\n",
      "Iteration 9355: loss 0.000905433320440352\n",
      "Iteration 9356: loss 0.0009054330876097083\n",
      "Iteration 9357: loss 0.000905433320440352\n",
      "Iteration 9358: loss 0.000905433320440352\n",
      "Iteration 9359: loss 0.0009054330876097083\n",
      "Iteration 9360: loss 0.000905433320440352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9361: loss 0.000905433320440352\n",
      "Iteration 9362: loss 0.0009054330876097083\n",
      "Iteration 9363: loss 0.000905433320440352\n",
      "Iteration 9364: loss 0.000905433320440352\n",
      "Iteration 9365: loss 0.0009054330876097083\n",
      "Iteration 9366: loss 0.000905433320440352\n",
      "Iteration 9367: loss 0.000905433320440352\n",
      "Iteration 9368: loss 0.0009054330876097083\n",
      "Iteration 9369: loss 0.000905433320440352\n",
      "Iteration 9370: loss 0.000905433320440352\n",
      "Iteration 9371: loss 0.0009054330876097083\n",
      "Iteration 9372: loss 0.000905433320440352\n",
      "Iteration 9373: loss 0.000905433320440352\n",
      "Iteration 9374: loss 0.0009054330876097083\n",
      "Iteration 9375: loss 0.000905433320440352\n",
      "Iteration 9376: loss 0.000905433320440352\n",
      "Iteration 9377: loss 0.0009054330876097083\n",
      "Iteration 9378: loss 0.000905433320440352\n",
      "Iteration 9379: loss 0.000905433320440352\n",
      "Iteration 9380: loss 0.0009054330876097083\n",
      "Iteration 9381: loss 0.000905433320440352\n",
      "Iteration 9382: loss 0.000905433320440352\n",
      "Iteration 9383: loss 0.0009054330876097083\n",
      "Iteration 9384: loss 0.000905433320440352\n",
      "Iteration 9385: loss 0.000905433320440352\n",
      "Iteration 9386: loss 0.0009054330876097083\n",
      "Iteration 9387: loss 0.000905433320440352\n",
      "Iteration 9388: loss 0.000905433320440352\n",
      "Iteration 9389: loss 0.0009054330876097083\n",
      "Iteration 9390: loss 0.000905433320440352\n",
      "Iteration 9391: loss 0.000905433320440352\n",
      "Iteration 9392: loss 0.0009054330876097083\n",
      "Iteration 9393: loss 0.000905433320440352\n",
      "Iteration 9394: loss 0.000905433320440352\n",
      "Iteration 9395: loss 0.0009054330876097083\n",
      "Iteration 9396: loss 0.000905433320440352\n",
      "Iteration 9397: loss 0.000905433320440352\n",
      "Iteration 9398: loss 0.0009054330876097083\n",
      "Iteration 9399: loss 0.000905433320440352\n",
      "Iteration 9400: loss 0.000905433320440352\n",
      "Iteration 9401: loss 0.0009054330876097083\n",
      "Iteration 9402: loss 0.000905433320440352\n",
      "Iteration 9403: loss 0.000905433320440352\n",
      "Iteration 9404: loss 0.0009054330876097083\n",
      "Iteration 9405: loss 0.000905433320440352\n",
      "Iteration 9406: loss 0.000905433320440352\n",
      "Iteration 9407: loss 0.0009054330876097083\n",
      "Iteration 9408: loss 0.000905433320440352\n",
      "Iteration 9409: loss 0.000905433320440352\n",
      "Iteration 9410: loss 0.0009054330876097083\n",
      "Iteration 9411: loss 0.000905433320440352\n",
      "Iteration 9412: loss 0.000905433320440352\n",
      "Iteration 9413: loss 0.0009054330876097083\n",
      "Iteration 9414: loss 0.000905433320440352\n",
      "Iteration 9415: loss 0.000905433320440352\n",
      "Iteration 9416: loss 0.0009054330876097083\n",
      "Iteration 9417: loss 0.000905433320440352\n",
      "Iteration 9418: loss 0.000905433320440352\n",
      "Iteration 9419: loss 0.0009054330876097083\n",
      "Iteration 9420: loss 0.000905433320440352\n",
      "Iteration 9421: loss 0.000905433320440352\n",
      "Iteration 9422: loss 0.0009054330876097083\n",
      "Iteration 9423: loss 0.000905433320440352\n",
      "Iteration 9424: loss 0.000905433320440352\n",
      "Iteration 9425: loss 0.0009054330876097083\n",
      "Iteration 9426: loss 0.000905433320440352\n",
      "Iteration 9427: loss 0.000905433320440352\n",
      "Iteration 9428: loss 0.0009054330876097083\n",
      "Iteration 9429: loss 0.000905433320440352\n",
      "Iteration 9430: loss 0.000905433320440352\n",
      "Iteration 9431: loss 0.0009054330876097083\n",
      "Iteration 9432: loss 0.000905433320440352\n",
      "Iteration 9433: loss 0.000905433320440352\n",
      "Iteration 9434: loss 0.0009054330876097083\n",
      "Iteration 9435: loss 0.000905433320440352\n",
      "Iteration 9436: loss 0.000905433320440352\n",
      "Iteration 9437: loss 0.0009054330876097083\n",
      "Iteration 9438: loss 0.000905433320440352\n",
      "Iteration 9439: loss 0.000905433320440352\n",
      "Iteration 9440: loss 0.0009054330876097083\n",
      "Iteration 9441: loss 0.000905433320440352\n",
      "Iteration 9442: loss 0.000905433320440352\n",
      "Iteration 9443: loss 0.0009054330876097083\n",
      "Iteration 9444: loss 0.000905433320440352\n",
      "Iteration 9445: loss 0.000905433320440352\n",
      "Iteration 9446: loss 0.0009054330876097083\n",
      "Iteration 9447: loss 0.000905433320440352\n",
      "Iteration 9448: loss 0.000905433320440352\n",
      "Iteration 9449: loss 0.0009054330876097083\n",
      "Iteration 9450: loss 0.000905433320440352\n",
      "Iteration 9451: loss 0.000905433320440352\n",
      "Iteration 9452: loss 0.0009054330876097083\n",
      "Iteration 9453: loss 0.000905433320440352\n",
      "Iteration 9454: loss 0.000905433320440352\n",
      "Iteration 9455: loss 0.0009054330876097083\n",
      "Iteration 9456: loss 0.000905433320440352\n",
      "Iteration 9457: loss 0.000905433320440352\n",
      "Iteration 9458: loss 0.0009054330876097083\n",
      "Iteration 9459: loss 0.000905433320440352\n",
      "Iteration 9460: loss 0.000905433320440352\n",
      "Iteration 9461: loss 0.0009054330876097083\n",
      "Iteration 9462: loss 0.000905433320440352\n",
      "Iteration 9463: loss 0.000905433320440352\n",
      "Iteration 9464: loss 0.0009054330876097083\n",
      "Iteration 9465: loss 0.000905433320440352\n",
      "Iteration 9466: loss 0.000905433320440352\n",
      "Iteration 9467: loss 0.0009054330876097083\n",
      "Iteration 9468: loss 0.000905433320440352\n",
      "Iteration 9469: loss 0.000905433320440352\n",
      "Iteration 9470: loss 0.0009054330876097083\n",
      "Iteration 9471: loss 0.000905433320440352\n",
      "Iteration 9472: loss 0.000905433320440352\n",
      "Iteration 9473: loss 0.0009054330876097083\n",
      "Iteration 9474: loss 0.000905433320440352\n",
      "Iteration 9475: loss 0.000905433320440352\n",
      "Iteration 9476: loss 0.0009054330876097083\n",
      "Iteration 9477: loss 0.000905433320440352\n",
      "Iteration 9478: loss 0.000905433320440352\n",
      "Iteration 9479: loss 0.0009054330876097083\n",
      "Iteration 9480: loss 0.000905433320440352\n",
      "Iteration 9481: loss 0.000905433320440352\n",
      "Iteration 9482: loss 0.0009054330876097083\n",
      "Iteration 9483: loss 0.000905433320440352\n",
      "Iteration 9484: loss 0.000905433320440352\n",
      "Iteration 9485: loss 0.0009054330876097083\n",
      "Iteration 9486: loss 0.000905433320440352\n",
      "Iteration 9487: loss 0.000905433320440352\n",
      "Iteration 9488: loss 0.0009054330876097083\n",
      "Iteration 9489: loss 0.000905433320440352\n",
      "Iteration 9490: loss 0.000905433320440352\n",
      "Iteration 9491: loss 0.0009054330876097083\n",
      "Iteration 9492: loss 0.000905433320440352\n",
      "Iteration 9493: loss 0.000905433320440352\n",
      "Iteration 9494: loss 0.0009054330876097083\n",
      "Iteration 9495: loss 0.000905433320440352\n",
      "Iteration 9496: loss 0.000905433320440352\n",
      "Iteration 9497: loss 0.0009054330876097083\n",
      "Iteration 9498: loss 0.000905433320440352\n",
      "Iteration 9499: loss 0.000905433320440352\n",
      "Iteration 9500: loss 0.0009054330876097083\n",
      "Iteration 9501: loss 0.000905433320440352\n",
      "Iteration 9502: loss 0.000905433320440352\n",
      "Iteration 9503: loss 0.0009054330876097083\n",
      "Iteration 9504: loss 0.000905433320440352\n",
      "Iteration 9505: loss 0.000905433320440352\n",
      "Iteration 9506: loss 0.0009054330876097083\n",
      "Iteration 9507: loss 0.000905433320440352\n",
      "Iteration 9508: loss 0.000905433320440352\n",
      "Iteration 9509: loss 0.0009054330876097083\n",
      "Iteration 9510: loss 0.000905433320440352\n",
      "Iteration 9511: loss 0.000905433320440352\n",
      "Iteration 9512: loss 0.0009054330876097083\n",
      "Iteration 9513: loss 0.000905433320440352\n",
      "Iteration 9514: loss 0.000905433320440352\n",
      "Iteration 9515: loss 0.0009054330876097083\n",
      "Iteration 9516: loss 0.000905433320440352\n",
      "Iteration 9517: loss 0.000905433320440352\n",
      "Iteration 9518: loss 0.0009054330876097083\n",
      "Iteration 9519: loss 0.000905433320440352\n",
      "Iteration 9520: loss 0.000905433320440352\n",
      "Iteration 9521: loss 0.0009054330876097083\n",
      "Iteration 9522: loss 0.000905433320440352\n",
      "Iteration 9523: loss 0.000905433320440352\n",
      "Iteration 9524: loss 0.0009054330876097083\n",
      "Iteration 9525: loss 0.000905433320440352\n",
      "Iteration 9526: loss 0.000905433320440352\n",
      "Iteration 9527: loss 0.0009054330876097083\n",
      "Iteration 9528: loss 0.000905433320440352\n",
      "Iteration 9529: loss 0.000905433320440352\n",
      "Iteration 9530: loss 0.0009054330876097083\n",
      "Iteration 9531: loss 0.000905433320440352\n",
      "Iteration 9532: loss 0.000905433320440352\n",
      "Iteration 9533: loss 0.0009054330876097083\n",
      "Iteration 9534: loss 0.000905433320440352\n",
      "Iteration 9535: loss 0.000905433320440352\n",
      "Iteration 9536: loss 0.0009054330876097083\n",
      "Iteration 9537: loss 0.000905433320440352\n",
      "Iteration 9538: loss 0.000905433320440352\n",
      "Iteration 9539: loss 0.0009054330876097083\n",
      "Iteration 9540: loss 0.000905433320440352\n",
      "Iteration 9541: loss 0.000905433320440352\n",
      "Iteration 9542: loss 0.0009054330876097083\n",
      "Iteration 9543: loss 0.000905433320440352\n",
      "Iteration 9544: loss 0.000905433320440352\n",
      "Iteration 9545: loss 0.0009054330876097083\n",
      "Iteration 9546: loss 0.000905433320440352\n",
      "Iteration 9547: loss 0.000905433320440352\n",
      "Iteration 9548: loss 0.0009054330876097083\n",
      "Iteration 9549: loss 0.000905433320440352\n",
      "Iteration 9550: loss 0.000905433320440352\n",
      "Iteration 9551: loss 0.0009054330876097083\n",
      "Iteration 9552: loss 0.000905433320440352\n",
      "Iteration 9553: loss 0.000905433320440352\n",
      "Iteration 9554: loss 0.0009054330876097083\n",
      "Iteration 9555: loss 0.000905433320440352\n",
      "Iteration 9556: loss 0.000905433320440352\n",
      "Iteration 9557: loss 0.0009054330876097083\n",
      "Iteration 9558: loss 0.000905433320440352\n",
      "Iteration 9559: loss 0.000905433320440352\n",
      "Iteration 9560: loss 0.0009054330876097083\n",
      "Iteration 9561: loss 0.000905433320440352\n",
      "Iteration 9562: loss 0.000905433320440352\n",
      "Iteration 9563: loss 0.0009054330876097083\n",
      "Iteration 9564: loss 0.000905433320440352\n",
      "Iteration 9565: loss 0.000905433320440352\n",
      "Iteration 9566: loss 0.0009054330876097083\n",
      "Iteration 9567: loss 0.000905433320440352\n",
      "Iteration 9568: loss 0.000905433320440352\n",
      "Iteration 9569: loss 0.0009054330876097083\n",
      "Iteration 9570: loss 0.000905433320440352\n",
      "Iteration 9571: loss 0.000905433320440352\n",
      "Iteration 9572: loss 0.0009054330876097083\n",
      "Iteration 9573: loss 0.000905433320440352\n",
      "Iteration 9574: loss 0.000905433320440352\n",
      "Iteration 9575: loss 0.0009054330876097083\n",
      "Iteration 9576: loss 0.000905433320440352\n",
      "Iteration 9577: loss 0.000905433320440352\n",
      "Iteration 9578: loss 0.0009054330876097083\n",
      "Iteration 9579: loss 0.000905433320440352\n",
      "Iteration 9580: loss 0.000905433320440352\n",
      "Iteration 9581: loss 0.0009054330876097083\n",
      "Iteration 9582: loss 0.000905433320440352\n",
      "Iteration 9583: loss 0.000905433320440352\n",
      "Iteration 9584: loss 0.0009054330876097083\n",
      "Iteration 9585: loss 0.000905433320440352\n",
      "Iteration 9586: loss 0.000905433320440352\n",
      "Iteration 9587: loss 0.0009054330876097083\n",
      "Iteration 9588: loss 0.000905433320440352\n",
      "Iteration 9589: loss 0.000905433320440352\n",
      "Iteration 9590: loss 0.0009054330876097083\n",
      "Iteration 9591: loss 0.000905433320440352\n",
      "Iteration 9592: loss 0.000905433320440352\n",
      "Iteration 9593: loss 0.0009054330876097083\n",
      "Iteration 9594: loss 0.000905433320440352\n",
      "Iteration 9595: loss 0.000905433320440352\n",
      "Iteration 9596: loss 0.0009054330876097083\n",
      "Iteration 9597: loss 0.000905433320440352\n",
      "Iteration 9598: loss 0.000905433320440352\n",
      "Iteration 9599: loss 0.0009054330876097083\n",
      "Iteration 9600: loss 0.000905433320440352\n",
      "Iteration 9601: loss 0.000905433320440352\n",
      "Iteration 9602: loss 0.0009054330876097083\n",
      "Iteration 9603: loss 0.000905433320440352\n",
      "Iteration 9604: loss 0.000905433320440352\n",
      "Iteration 9605: loss 0.0009054330876097083\n",
      "Iteration 9606: loss 0.000905433320440352\n",
      "Iteration 9607: loss 0.000905433320440352\n",
      "Iteration 9608: loss 0.0009054330876097083\n",
      "Iteration 9609: loss 0.000905433320440352\n",
      "Iteration 9610: loss 0.000905433320440352\n",
      "Iteration 9611: loss 0.0009054330876097083\n",
      "Iteration 9612: loss 0.000905433320440352\n",
      "Iteration 9613: loss 0.000905433320440352\n",
      "Iteration 9614: loss 0.0009054330876097083\n",
      "Iteration 9615: loss 0.000905433320440352\n",
      "Iteration 9616: loss 0.000905433320440352\n",
      "Iteration 9617: loss 0.0009054330876097083\n",
      "Iteration 9618: loss 0.000905433320440352\n",
      "Iteration 9619: loss 0.000905433320440352\n",
      "Iteration 9620: loss 0.0009054330876097083\n",
      "Iteration 9621: loss 0.000905433320440352\n",
      "Iteration 9622: loss 0.000905433320440352\n",
      "Iteration 9623: loss 0.0009054330876097083\n",
      "Iteration 9624: loss 0.000905433320440352\n",
      "Iteration 9625: loss 0.000905433320440352\n",
      "Iteration 9626: loss 0.0009054330876097083\n",
      "Iteration 9627: loss 0.000905433320440352\n",
      "Iteration 9628: loss 0.000905433320440352\n",
      "Iteration 9629: loss 0.0009054330876097083\n",
      "Iteration 9630: loss 0.000905433320440352\n",
      "Iteration 9631: loss 0.000905433320440352\n",
      "Iteration 9632: loss 0.0009054330876097083\n",
      "Iteration 9633: loss 0.000905433320440352\n",
      "Iteration 9634: loss 0.000905433320440352\n",
      "Iteration 9635: loss 0.0009054330876097083\n",
      "Iteration 9636: loss 0.000905433320440352\n",
      "Iteration 9637: loss 0.000905433320440352\n",
      "Iteration 9638: loss 0.0009054330876097083\n",
      "Iteration 9639: loss 0.000905433320440352\n",
      "Iteration 9640: loss 0.000905433320440352\n",
      "Iteration 9641: loss 0.0009054330876097083\n",
      "Iteration 9642: loss 0.000905433320440352\n",
      "Iteration 9643: loss 0.000905433320440352\n",
      "Iteration 9644: loss 0.0009054330876097083\n",
      "Iteration 9645: loss 0.000905433320440352\n",
      "Iteration 9646: loss 0.000905433320440352\n",
      "Iteration 9647: loss 0.0009054330876097083\n",
      "Iteration 9648: loss 0.000905433320440352\n",
      "Iteration 9649: loss 0.000905433320440352\n",
      "Iteration 9650: loss 0.0009054330876097083\n",
      "Iteration 9651: loss 0.000905433320440352\n",
      "Iteration 9652: loss 0.000905433320440352\n",
      "Iteration 9653: loss 0.0009054330876097083\n",
      "Iteration 9654: loss 0.000905433320440352\n",
      "Iteration 9655: loss 0.000905433320440352\n",
      "Iteration 9656: loss 0.0009054330876097083\n",
      "Iteration 9657: loss 0.000905433320440352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9658: loss 0.000905433320440352\n",
      "Iteration 9659: loss 0.0009054330876097083\n",
      "Iteration 9660: loss 0.000905433320440352\n",
      "Iteration 9661: loss 0.000905433320440352\n",
      "Iteration 9662: loss 0.0009054330876097083\n",
      "Iteration 9663: loss 0.000905433320440352\n",
      "Iteration 9664: loss 0.000905433320440352\n",
      "Iteration 9665: loss 0.0009054330876097083\n",
      "Iteration 9666: loss 0.000905433320440352\n",
      "Iteration 9667: loss 0.000905433320440352\n",
      "Iteration 9668: loss 0.0009054330876097083\n",
      "Iteration 9669: loss 0.000905433320440352\n",
      "Iteration 9670: loss 0.000905433320440352\n",
      "Iteration 9671: loss 0.0009054330876097083\n",
      "Iteration 9672: loss 0.000905433320440352\n",
      "Iteration 9673: loss 0.000905433320440352\n",
      "Iteration 9674: loss 0.0009054330876097083\n",
      "Iteration 9675: loss 0.000905433320440352\n",
      "Iteration 9676: loss 0.000905433320440352\n",
      "Iteration 9677: loss 0.0009054330876097083\n",
      "Iteration 9678: loss 0.000905433320440352\n",
      "Iteration 9679: loss 0.000905433320440352\n",
      "Iteration 9680: loss 0.0009054330876097083\n",
      "Iteration 9681: loss 0.000905433320440352\n",
      "Iteration 9682: loss 0.000905433320440352\n",
      "Iteration 9683: loss 0.0009054330876097083\n",
      "Iteration 9684: loss 0.000905433320440352\n",
      "Iteration 9685: loss 0.000905433320440352\n",
      "Iteration 9686: loss 0.0009054330876097083\n",
      "Iteration 9687: loss 0.000905433320440352\n",
      "Iteration 9688: loss 0.000905433320440352\n",
      "Iteration 9689: loss 0.0009054330876097083\n",
      "Iteration 9690: loss 0.000905433320440352\n",
      "Iteration 9691: loss 0.000905433320440352\n",
      "Iteration 9692: loss 0.0009054330876097083\n",
      "Iteration 9693: loss 0.000905433320440352\n",
      "Iteration 9694: loss 0.000905433320440352\n",
      "Iteration 9695: loss 0.0009054330876097083\n",
      "Iteration 9696: loss 0.000905433320440352\n",
      "Iteration 9697: loss 0.000905433320440352\n",
      "Iteration 9698: loss 0.0009054330876097083\n",
      "Iteration 9699: loss 0.000905433320440352\n",
      "Iteration 9700: loss 0.000905433320440352\n",
      "Iteration 9701: loss 0.0009054330876097083\n",
      "Iteration 9702: loss 0.000905433320440352\n",
      "Iteration 9703: loss 0.000905433320440352\n",
      "Iteration 9704: loss 0.0009054330876097083\n",
      "Iteration 9705: loss 0.000905433320440352\n",
      "Iteration 9706: loss 0.000905433320440352\n",
      "Iteration 9707: loss 0.0009054330876097083\n",
      "Iteration 9708: loss 0.000905433320440352\n",
      "Iteration 9709: loss 0.000905433320440352\n",
      "Iteration 9710: loss 0.0009054330876097083\n",
      "Iteration 9711: loss 0.000905433320440352\n",
      "Iteration 9712: loss 0.000905433320440352\n",
      "Iteration 9713: loss 0.0009054330876097083\n",
      "Iteration 9714: loss 0.000905433320440352\n",
      "Iteration 9715: loss 0.000905433320440352\n",
      "Iteration 9716: loss 0.0009054330876097083\n",
      "Iteration 9717: loss 0.000905433320440352\n",
      "Iteration 9718: loss 0.000905433320440352\n",
      "Iteration 9719: loss 0.0009054330876097083\n",
      "Iteration 9720: loss 0.000905433320440352\n",
      "Iteration 9721: loss 0.000905433320440352\n",
      "Iteration 9722: loss 0.0009054330876097083\n",
      "Iteration 9723: loss 0.000905433320440352\n",
      "Iteration 9724: loss 0.000905433320440352\n",
      "Iteration 9725: loss 0.0009054330876097083\n",
      "Iteration 9726: loss 0.000905433320440352\n",
      "Iteration 9727: loss 0.000905433320440352\n",
      "Iteration 9728: loss 0.0009054330876097083\n",
      "Iteration 9729: loss 0.000905433320440352\n",
      "Iteration 9730: loss 0.000905433320440352\n",
      "Iteration 9731: loss 0.0009054330876097083\n",
      "Iteration 9732: loss 0.000905433320440352\n",
      "Iteration 9733: loss 0.000905433320440352\n",
      "Iteration 9734: loss 0.0009054330876097083\n",
      "Iteration 9735: loss 0.000905433320440352\n",
      "Iteration 9736: loss 0.000905433320440352\n",
      "Iteration 9737: loss 0.0009054330876097083\n",
      "Iteration 9738: loss 0.000905433320440352\n",
      "Iteration 9739: loss 0.000905433320440352\n",
      "Iteration 9740: loss 0.0009054330876097083\n",
      "Iteration 9741: loss 0.000905433320440352\n",
      "Iteration 9742: loss 0.000905433320440352\n",
      "Iteration 9743: loss 0.0009054330876097083\n",
      "Iteration 9744: loss 0.000905433320440352\n",
      "Iteration 9745: loss 0.000905433320440352\n",
      "Iteration 9746: loss 0.0009054330876097083\n",
      "Iteration 9747: loss 0.000905433320440352\n",
      "Iteration 9748: loss 0.000905433320440352\n",
      "Iteration 9749: loss 0.0009054330876097083\n",
      "Iteration 9750: loss 0.000905433320440352\n",
      "Iteration 9751: loss 0.000905433320440352\n",
      "Iteration 9752: loss 0.0009054330876097083\n",
      "Iteration 9753: loss 0.000905433320440352\n",
      "Iteration 9754: loss 0.000905433320440352\n",
      "Iteration 9755: loss 0.0009054330876097083\n",
      "Iteration 9756: loss 0.000905433320440352\n",
      "Iteration 9757: loss 0.000905433320440352\n",
      "Iteration 9758: loss 0.0009054330876097083\n",
      "Iteration 9759: loss 0.000905433320440352\n",
      "Iteration 9760: loss 0.000905433320440352\n",
      "Iteration 9761: loss 0.0009054330876097083\n",
      "Iteration 9762: loss 0.000905433320440352\n",
      "Iteration 9763: loss 0.000905433320440352\n",
      "Iteration 9764: loss 0.0009054330876097083\n",
      "Iteration 9765: loss 0.000905433320440352\n",
      "Iteration 9766: loss 0.000905433320440352\n",
      "Iteration 9767: loss 0.0009054330876097083\n",
      "Iteration 9768: loss 0.000905433320440352\n",
      "Iteration 9769: loss 0.000905433320440352\n",
      "Iteration 9770: loss 0.0009054330876097083\n",
      "Iteration 9771: loss 0.000905433320440352\n",
      "Iteration 9772: loss 0.000905433320440352\n",
      "Iteration 9773: loss 0.0009054330876097083\n",
      "Iteration 9774: loss 0.000905433320440352\n",
      "Iteration 9775: loss 0.000905433320440352\n",
      "Iteration 9776: loss 0.0009054330876097083\n",
      "Iteration 9777: loss 0.000905433320440352\n",
      "Iteration 9778: loss 0.000905433320440352\n",
      "Iteration 9779: loss 0.0009054330876097083\n",
      "Iteration 9780: loss 0.000905433320440352\n",
      "Iteration 9781: loss 0.000905433320440352\n",
      "Iteration 9782: loss 0.0009054330876097083\n",
      "Iteration 9783: loss 0.000905433320440352\n",
      "Iteration 9784: loss 0.000905433320440352\n",
      "Iteration 9785: loss 0.0009054330876097083\n",
      "Iteration 9786: loss 0.000905433320440352\n",
      "Iteration 9787: loss 0.000905433320440352\n",
      "Iteration 9788: loss 0.0009054330876097083\n",
      "Iteration 9789: loss 0.000905433320440352\n",
      "Iteration 9790: loss 0.000905433320440352\n",
      "Iteration 9791: loss 0.0009054330876097083\n",
      "Iteration 9792: loss 0.000905433320440352\n",
      "Iteration 9793: loss 0.000905433320440352\n",
      "Iteration 9794: loss 0.0009054330876097083\n",
      "Iteration 9795: loss 0.000905433320440352\n",
      "Iteration 9796: loss 0.000905433320440352\n",
      "Iteration 9797: loss 0.0009054330876097083\n",
      "Iteration 9798: loss 0.000905433320440352\n",
      "Iteration 9799: loss 0.000905433320440352\n",
      "Iteration 9800: loss 0.0009054330876097083\n",
      "Iteration 9801: loss 0.000905433320440352\n",
      "Iteration 9802: loss 0.000905433320440352\n",
      "Iteration 9803: loss 0.0009054330876097083\n",
      "Iteration 9804: loss 0.000905433320440352\n",
      "Iteration 9805: loss 0.000905433320440352\n",
      "Iteration 9806: loss 0.0009054330876097083\n",
      "Iteration 9807: loss 0.000905433320440352\n",
      "Iteration 9808: loss 0.000905433320440352\n",
      "Iteration 9809: loss 0.0009054330876097083\n",
      "Iteration 9810: loss 0.000905433320440352\n",
      "Iteration 9811: loss 0.000905433320440352\n",
      "Iteration 9812: loss 0.0009054330876097083\n",
      "Iteration 9813: loss 0.000905433320440352\n",
      "Iteration 9814: loss 0.000905433320440352\n",
      "Iteration 9815: loss 0.0009054330876097083\n",
      "Iteration 9816: loss 0.000905433320440352\n",
      "Iteration 9817: loss 0.000905433320440352\n",
      "Iteration 9818: loss 0.0009054330876097083\n",
      "Iteration 9819: loss 0.000905433320440352\n",
      "Iteration 9820: loss 0.000905433320440352\n",
      "Iteration 9821: loss 0.0009054330876097083\n",
      "Iteration 9822: loss 0.000905433320440352\n",
      "Iteration 9823: loss 0.000905433320440352\n",
      "Iteration 9824: loss 0.0009054330876097083\n",
      "Iteration 9825: loss 0.000905433320440352\n",
      "Iteration 9826: loss 0.000905433320440352\n",
      "Iteration 9827: loss 0.0009054330876097083\n",
      "Iteration 9828: loss 0.000905433320440352\n",
      "Iteration 9829: loss 0.000905433320440352\n",
      "Iteration 9830: loss 0.0009054330876097083\n",
      "Iteration 9831: loss 0.000905433320440352\n",
      "Iteration 9832: loss 0.000905433320440352\n",
      "Iteration 9833: loss 0.0009054330876097083\n",
      "Iteration 9834: loss 0.000905433320440352\n",
      "Iteration 9835: loss 0.000905433320440352\n",
      "Iteration 9836: loss 0.0009054330876097083\n",
      "Iteration 9837: loss 0.000905433320440352\n",
      "Iteration 9838: loss 0.000905433320440352\n",
      "Iteration 9839: loss 0.0009054330876097083\n",
      "Iteration 9840: loss 0.000905433320440352\n",
      "Iteration 9841: loss 0.000905433320440352\n",
      "Iteration 9842: loss 0.0009054330876097083\n",
      "Iteration 9843: loss 0.000905433320440352\n",
      "Iteration 9844: loss 0.000905433320440352\n",
      "Iteration 9845: loss 0.0009054330876097083\n",
      "Iteration 9846: loss 0.000905433320440352\n",
      "Iteration 9847: loss 0.000905433320440352\n",
      "Iteration 9848: loss 0.0009054330876097083\n",
      "Iteration 9849: loss 0.000905433320440352\n",
      "Iteration 9850: loss 0.000905433320440352\n",
      "Iteration 9851: loss 0.0009054330876097083\n",
      "Iteration 9852: loss 0.000905433320440352\n",
      "Iteration 9853: loss 0.000905433320440352\n",
      "Iteration 9854: loss 0.0009054330876097083\n",
      "Iteration 9855: loss 0.000905433320440352\n",
      "Iteration 9856: loss 0.000905433320440352\n",
      "Iteration 9857: loss 0.0009054330876097083\n",
      "Iteration 9858: loss 0.000905433320440352\n",
      "Iteration 9859: loss 0.000905433320440352\n",
      "Iteration 9860: loss 0.0009054330876097083\n",
      "Iteration 9861: loss 0.000905433320440352\n",
      "Iteration 9862: loss 0.000905433320440352\n",
      "Iteration 9863: loss 0.0009054330876097083\n",
      "Iteration 9864: loss 0.000905433320440352\n",
      "Iteration 9865: loss 0.000905433320440352\n",
      "Iteration 9866: loss 0.0009054330876097083\n",
      "Iteration 9867: loss 0.000905433320440352\n",
      "Iteration 9868: loss 0.000905433320440352\n",
      "Iteration 9869: loss 0.0009054330876097083\n",
      "Iteration 9870: loss 0.000905433320440352\n",
      "Iteration 9871: loss 0.000905433320440352\n",
      "Iteration 9872: loss 0.0009054330876097083\n",
      "Iteration 9873: loss 0.000905433320440352\n",
      "Iteration 9874: loss 0.000905433320440352\n",
      "Iteration 9875: loss 0.0009054330876097083\n",
      "Iteration 9876: loss 0.000905433320440352\n",
      "Iteration 9877: loss 0.000905433320440352\n",
      "Iteration 9878: loss 0.0009054330876097083\n",
      "Iteration 9879: loss 0.000905433320440352\n",
      "Iteration 9880: loss 0.000905433320440352\n",
      "Iteration 9881: loss 0.0009054330876097083\n",
      "Iteration 9882: loss 0.000905433320440352\n",
      "Iteration 9883: loss 0.000905433320440352\n",
      "Iteration 9884: loss 0.0009054330876097083\n",
      "Iteration 9885: loss 0.000905433320440352\n",
      "Iteration 9886: loss 0.000905433320440352\n",
      "Iteration 9887: loss 0.0009054330876097083\n",
      "Iteration 9888: loss 0.000905433320440352\n",
      "Iteration 9889: loss 0.000905433320440352\n",
      "Iteration 9890: loss 0.0009054330876097083\n",
      "Iteration 9891: loss 0.000905433320440352\n",
      "Iteration 9892: loss 0.000905433320440352\n",
      "Iteration 9893: loss 0.0009054330876097083\n",
      "Iteration 9894: loss 0.000905433320440352\n",
      "Iteration 9895: loss 0.000905433320440352\n",
      "Iteration 9896: loss 0.0009054330876097083\n",
      "Iteration 9897: loss 0.000905433320440352\n",
      "Iteration 9898: loss 0.000905433320440352\n",
      "Iteration 9899: loss 0.0009054330876097083\n",
      "Iteration 9900: loss 0.000905433320440352\n",
      "Iteration 9901: loss 0.000905433320440352\n",
      "Iteration 9902: loss 0.0009054330876097083\n",
      "Iteration 9903: loss 0.000905433320440352\n",
      "Iteration 9904: loss 0.000905433320440352\n",
      "Iteration 9905: loss 0.0009054330876097083\n",
      "Iteration 9906: loss 0.000905433320440352\n",
      "Iteration 9907: loss 0.000905433320440352\n",
      "Iteration 9908: loss 0.0009054330876097083\n",
      "Iteration 9909: loss 0.000905433320440352\n",
      "Iteration 9910: loss 0.000905433320440352\n",
      "Iteration 9911: loss 0.0009054330876097083\n",
      "Iteration 9912: loss 0.000905433320440352\n",
      "Iteration 9913: loss 0.000905433320440352\n",
      "Iteration 9914: loss 0.0009054330876097083\n",
      "Iteration 9915: loss 0.000905433320440352\n",
      "Iteration 9916: loss 0.000905433320440352\n",
      "Iteration 9917: loss 0.0009054330876097083\n",
      "Iteration 9918: loss 0.000905433320440352\n",
      "Iteration 9919: loss 0.000905433320440352\n",
      "Iteration 9920: loss 0.0009054330876097083\n",
      "Iteration 9921: loss 0.000905433320440352\n",
      "Iteration 9922: loss 0.000905433320440352\n",
      "Iteration 9923: loss 0.0009054330876097083\n",
      "Iteration 9924: loss 0.000905433320440352\n",
      "Iteration 9925: loss 0.000905433320440352\n",
      "Iteration 9926: loss 0.0009054330876097083\n",
      "Iteration 9927: loss 0.000905433320440352\n",
      "Iteration 9928: loss 0.000905433320440352\n",
      "Iteration 9929: loss 0.0009054330876097083\n",
      "Iteration 9930: loss 0.000905433320440352\n",
      "Iteration 9931: loss 0.000905433320440352\n",
      "Iteration 9932: loss 0.0009054330876097083\n",
      "Iteration 9933: loss 0.000905433320440352\n",
      "Iteration 9934: loss 0.000905433320440352\n",
      "Iteration 9935: loss 0.0009054330876097083\n",
      "Iteration 9936: loss 0.000905433320440352\n",
      "Iteration 9937: loss 0.000905433320440352\n",
      "Iteration 9938: loss 0.0009054330876097083\n",
      "Iteration 9939: loss 0.000905433320440352\n",
      "Iteration 9940: loss 0.000905433320440352\n",
      "Iteration 9941: loss 0.0009054330876097083\n",
      "Iteration 9942: loss 0.000905433320440352\n",
      "Iteration 9943: loss 0.000905433320440352\n",
      "Iteration 9944: loss 0.0009054330876097083\n",
      "Iteration 9945: loss 0.000905433320440352\n",
      "Iteration 9946: loss 0.000905433320440352\n",
      "Iteration 9947: loss 0.0009054330876097083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9948: loss 0.000905433320440352\n",
      "Iteration 9949: loss 0.000905433320440352\n",
      "Iteration 9950: loss 0.0009054330876097083\n",
      "Iteration 9951: loss 0.000905433320440352\n",
      "Iteration 9952: loss 0.000905433320440352\n",
      "Iteration 9953: loss 0.0009054330876097083\n",
      "Iteration 9954: loss 0.000905433320440352\n",
      "Iteration 9955: loss 0.000905433320440352\n",
      "Iteration 9956: loss 0.0009054330876097083\n",
      "Iteration 9957: loss 0.000905433320440352\n",
      "Iteration 9958: loss 0.000905433320440352\n",
      "Iteration 9959: loss 0.0009054330876097083\n",
      "Iteration 9960: loss 0.000905433320440352\n",
      "Iteration 9961: loss 0.000905433320440352\n",
      "Iteration 9962: loss 0.0009054330876097083\n",
      "Iteration 9963: loss 0.000905433320440352\n",
      "Iteration 9964: loss 0.000905433320440352\n",
      "Iteration 9965: loss 0.0009054330876097083\n",
      "Iteration 9966: loss 0.000905433320440352\n",
      "Iteration 9967: loss 0.000905433320440352\n",
      "Iteration 9968: loss 0.0009054330876097083\n",
      "Iteration 9969: loss 0.000905433320440352\n",
      "Iteration 9970: loss 0.000905433320440352\n",
      "Iteration 9971: loss 0.0009054330876097083\n",
      "Iteration 9972: loss 0.000905433320440352\n",
      "Iteration 9973: loss 0.000905433320440352\n",
      "Iteration 9974: loss 0.0009054330876097083\n",
      "Iteration 9975: loss 0.000905433320440352\n",
      "Iteration 9976: loss 0.000905433320440352\n",
      "Iteration 9977: loss 0.0009054330876097083\n",
      "Iteration 9978: loss 0.000905433320440352\n",
      "Iteration 9979: loss 0.000905433320440352\n",
      "Iteration 9980: loss 0.0009054330876097083\n",
      "Iteration 9981: loss 0.000905433320440352\n",
      "Iteration 9982: loss 0.000905433320440352\n",
      "Iteration 9983: loss 0.0009054330876097083\n",
      "Iteration 9984: loss 0.000905433320440352\n",
      "Iteration 9985: loss 0.000905433320440352\n",
      "Iteration 9986: loss 0.0009054330876097083\n",
      "Iteration 9987: loss 0.000905433320440352\n",
      "Iteration 9988: loss 0.000905433320440352\n",
      "Iteration 9989: loss 0.0009054330876097083\n",
      "Iteration 9990: loss 0.000905433320440352\n",
      "Iteration 9991: loss 0.000905433320440352\n",
      "Iteration 9992: loss 0.0009054330876097083\n",
      "Iteration 9993: loss 0.000905433320440352\n",
      "Iteration 9994: loss 0.000905433320440352\n",
      "Iteration 9995: loss 0.0009054330876097083\n",
      "Iteration 9996: loss 0.000905433320440352\n",
      "Iteration 9997: loss 0.000905433320440352\n",
      "Iteration 9998: loss 0.0009054330876097083\n",
      "Iteration 9999: loss 0.000905433320440352\n",
      "Iteration 10000: loss 0.000905433320440352\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(initialize_vars)\n",
    "    \n",
    "    for iteration in range(10001):\n",
    "        _, loss_on_animal_data, summaries = session.run([train_step, loss, merge_summaries], \n",
    "                                                         feed_dict={x: animals, label: animal_labels})\n",
    "    \n",
    "        print(\"Iteration {}: loss {}\".format(iteration, loss_on_animal_data))    \n",
    "        logging_meta['train_writer'].add_summary(summaries, iteration)\n",
    "    logging_meta['saver'].save(session, logging_meta['model_path'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
