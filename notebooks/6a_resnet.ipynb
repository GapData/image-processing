{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Net - Revisited with Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from utils.data import init_dir\n",
    "from utils.nn_visualization import variable_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Net Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable variables:  ['layer_1/w:0', 'layer_2/w:0']\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('dummy_net_inputs'):\n",
    "        data = tf.placeholder(tf.float32, shape=[None, 3], name='data')\n",
    "        labels = tf.placeholder(tf.float32, shape=[None, 1], name='labels')\n",
    "    \n",
    "    with tf.variable_scope('layer_1'):\n",
    "        w = tf.get_variable('w', initializer=tf.constant_initializer(0), shape=[3,2])\n",
    "        prediction = tf.matmul(data, w)\n",
    "    \n",
    "    with tf.variable_scope('layer_2'):\n",
    "        w = tf.get_variable('w', initializer=tf.constant_initializer(0.1), shape=[2,1])\n",
    "        prediction = tf.matmul(prediction, w)\n",
    "    \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.losses.mean_squared_error(labels, prediction)\n",
    "        tf.summary.scalar('loss_summary', loss)\n",
    "    \n",
    "    with tf.name_scope('training'):\n",
    "        all_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        \n",
    "        all_trainable_vars_names = list(map(lambda var: var.name, all_trainable_vars))\n",
    "        print(\"Trainable variables: \", all_trainable_vars_names)\n",
    "        \n",
    "        gradients = tf.gradients(loss, all_trainable_vars)\n",
    "        variable_summaries('summary_gradient_w1', gradients[0])\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.005)\n",
    "        train_step = optimizer.apply_gradients(zip(gradients, all_trainable_vars))\n",
    "\n",
    "    initialize_vars = tf.global_variables_initializer()\n",
    "    merge_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Model Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import init_model_logging\n",
    "base_dir = '/tensorboard_summaries/dummy_net/'\n",
    "exp_name = 'experiment_revisited'\n",
    "\n",
    "logging_meta = init_model_logging(base_dir, exp_name, graph=graph, remove_existing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss 25.0, pred 0.0\n",
      "Iteration 1: loss 24.860197067260742, pred 0.013999998569488525\n",
      "Iteration 2: loss 24.711469650268555, pred 0.028936689719557762\n",
      "Iteration 3: loss 24.5343017578125, pred 0.04678851738572121\n",
      "Iteration 4: loss 24.307300567626953, pred 0.06975671648979187\n",
      "Iteration 5: loss 24.0048885345459, pred 0.10052148252725601\n",
      "Iteration 6: loss 23.59521484375, pred 0.14250966906547546\n",
      "Iteration 7: loss 23.038270950317383, pred 0.2001798152923584\n",
      "Iteration 8: loss 22.284957885742188, pred 0.2793055474758148\n",
      "Iteration 9: loss 21.277956008911133, pred 0.3871963322162628\n",
      "Iteration 10: loss 19.956560134887695, pred 0.5327236652374268\n",
      "Iteration 11: loss 18.26775360107422, pred 0.7259204387664795\n",
      "Iteration 12: loss 16.186023712158203, pred 0.976814329624176\n",
      "Iteration 13: loss 13.740848541259766, pred 1.2931349277496338\n",
      "Iteration 14: loss 11.043739318847656, pred 1.6767877340316772\n",
      "Iteration 15: loss 8.295835494995117, pred 2.119750738143921\n",
      "Iteration 16: loss 5.753831386566162, pred 2.601285457611084\n",
      "Iteration 17: loss 3.65116024017334, pred 3.0891990661621094\n",
      "Iteration 18: loss 2.1119914054870605, pred 3.5467307567596436\n",
      "Iteration 19: loss 1.0941250324249268, pred 3.953995704650879\n",
      "Iteration 20: loss 0.5335483551025391, pred 4.269556045532227\n",
      "Iteration 21: loss 0.24391113221645355, pred 4.506126403808594\n",
      "Iteration 22: loss 0.10614311695098877, pred 4.674203872680664\n",
      "Iteration 23: loss 0.044581372290849686, pred 4.7888569831848145\n",
      "Iteration 24: loss 0.01827285811305046, pred 4.864822864532471\n",
      "Iteration 25: loss 0.007368789985775948, pred 4.914158344268799\n",
      "Iteration 26: loss 0.002940140198916197, pred 4.94577693939209\n",
      "Iteration 27: loss 0.0011651564855128527, pred 4.965865612030029\n",
      "Iteration 28: loss 0.0004597566439770162, pred 4.97855806350708\n",
      "Iteration 29: loss 0.0001809064269764349, pred 4.9865498542785645\n",
      "Iteration 30: loss 7.106496923370287e-05, pred 4.991569995880127\n",
      "Iteration 31: loss 2.7883623261004686e-05, pred 4.994719505310059\n",
      "Iteration 32: loss 1.0929054042208008e-05, pred 4.996694087982178\n",
      "Iteration 33: loss 4.286667717678938e-06, pred 4.997929573059082\n",
      "Iteration 34: loss 1.6772573872003704e-06, pred 4.99870491027832\n",
      "Iteration 35: loss 6.571099220309407e-07, pred 4.999189376831055\n",
      "Iteration 36: loss 2.5740882847458124e-07, pred 4.999492645263672\n",
      "Iteration 37: loss 1.0115604709426407e-07, pred 4.9996819496154785\n",
      "Iteration 38: loss 3.953778104914818e-08, pred 4.999801158905029\n",
      "Iteration 39: loss 1.548892214486841e-08, pred 4.999875545501709\n",
      "Iteration 40: loss 6.1154423747211695e-09, pred 4.999921798706055\n",
      "Iteration 41: loss 2.412207322777249e-09, pred 4.999950885772705\n",
      "Iteration 42: loss 9.313225746154785e-10, pred 4.999969482421875\n",
      "Iteration 43: loss 3.637978807091713e-10, pred 4.999980926513672\n",
      "Iteration 44: loss 1.4210854715202004e-10, pred 4.999988079071045\n",
      "Iteration 45: loss 5.1159076974727213e-11, pred 4.999992847442627\n",
      "Iteration 46: loss 1.8417267710901797e-11, pred 4.999995708465576\n",
      "Iteration 47: loss 8.185452315956354e-12, pred 4.999997138977051\n",
      "Iteration 48: loss 3.637978807091713e-12, pred 4.999998092651367\n",
      "Iteration 49: loss 9.094947017729282e-13, pred 4.999999046325684\n",
      "Iteration 50: loss 9.094947017729282e-13, pred 4.999999046325684\n",
      "Iteration 51: loss 9.094947017729282e-13, pred 4.999999046325684\n",
      "Iteration 52: loss 9.094947017729282e-13, pred 4.999999046325684\n",
      "Iteration 53: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 54: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 55: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 56: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 57: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 58: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 59: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 60: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 61: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 62: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 63: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 64: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 65: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 66: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 67: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 68: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 69: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 70: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 71: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 72: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 73: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 74: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 75: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 76: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 77: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 78: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 79: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 80: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 81: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 82: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 83: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 84: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 85: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 86: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 87: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 88: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 89: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 90: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 91: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 92: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 93: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 94: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 95: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 96: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 97: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 98: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 99: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 100: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 101: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 102: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 103: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 104: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 105: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 106: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 107: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 108: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 109: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 110: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 111: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 112: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 113: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 114: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 115: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 116: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 117: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 118: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 119: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 120: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 121: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 122: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 123: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 124: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 125: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 126: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 127: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 128: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 129: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 130: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 131: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 132: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 133: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 134: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 135: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 136: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 137: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 138: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 139: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 140: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 141: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 142: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 143: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 144: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 145: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 146: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 147: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 148: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 149: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 150: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 151: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 152: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 153: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 154: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 155: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 156: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 157: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 158: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 159: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 160: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 161: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 162: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 163: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 164: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 165: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 166: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 167: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 168: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 169: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 170: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 171: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 172: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 173: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 174: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 175: loss 2.2737367544323206e-13, pred 4.999999523162842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 176: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 177: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 178: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 179: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 180: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 181: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 182: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 183: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 184: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 185: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 186: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 187: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 188: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 189: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 190: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 191: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 192: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 193: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 194: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 195: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 196: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 197: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 198: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 199: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 200: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 201: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 202: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 203: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 204: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 205: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 206: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 207: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 208: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 209: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 210: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 211: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 212: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 213: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 214: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 215: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 216: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 217: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 218: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 219: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 220: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 221: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 222: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 223: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 224: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 225: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 226: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 227: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 228: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 229: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 230: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 231: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 232: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 233: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 234: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 235: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 236: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 237: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 238: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 239: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 240: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 241: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 242: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 243: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 244: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 245: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 246: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 247: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 248: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 249: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 250: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 251: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 252: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 253: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 254: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 255: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 256: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 257: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 258: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 259: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 260: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 261: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 262: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 263: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 264: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 265: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 266: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 267: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 268: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 269: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 270: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 271: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 272: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 273: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 274: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 275: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 276: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 277: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 278: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 279: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 280: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 281: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 282: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 283: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 284: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 285: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 286: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 287: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 288: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 289: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 290: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 291: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 292: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 293: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 294: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 295: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 296: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 297: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 298: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 299: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 300: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 301: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 302: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 303: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 304: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 305: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 306: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 307: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 308: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 309: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 310: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 311: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 312: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 313: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 314: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 315: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 316: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 317: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 318: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 319: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 320: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 321: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 322: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 323: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 324: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 325: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 326: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 327: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 328: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 329: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 330: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 331: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 332: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 333: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 334: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 335: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 336: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 337: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 338: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 339: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 340: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 341: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 342: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 343: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 344: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 345: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 346: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 347: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 348: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 349: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 350: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 351: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 352: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 353: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 354: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 355: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 356: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 357: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 358: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 359: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 360: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 361: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 362: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 363: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 364: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 365: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 366: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 367: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 368: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 369: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 370: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 371: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 372: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 373: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 374: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 375: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 376: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 377: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 378: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 379: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 380: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 381: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 382: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 383: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 384: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 385: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 386: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 387: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 388: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 389: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 390: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 391: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 392: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 393: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 394: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 395: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 396: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 397: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 398: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 399: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 400: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 401: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 402: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 403: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 404: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 405: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 406: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 407: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 408: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 409: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 410: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 411: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 412: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 413: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 414: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 415: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 416: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 417: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 418: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 419: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 420: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 421: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 422: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 423: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 424: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 425: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 426: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 427: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 428: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 429: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 430: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 431: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 432: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 433: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 434: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 435: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 436: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 437: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 438: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 439: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 440: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 441: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 442: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 443: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 444: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 445: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 446: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 447: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 448: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 449: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 450: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 451: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 452: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 453: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 454: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 455: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 456: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 457: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 458: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 459: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 460: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 461: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 462: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 463: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 464: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 465: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 466: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 467: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 468: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 469: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 470: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 471: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 472: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 473: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 474: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 475: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 476: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 477: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 478: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 479: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 480: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 481: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 482: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 483: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 484: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 485: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 486: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 487: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 488: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 489: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 490: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 491: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 492: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 493: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 494: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 495: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 496: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 497: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 498: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 499: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 500: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 501: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 502: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 503: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 504: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 505: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 506: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 507: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 508: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 509: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 510: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 511: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 512: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 513: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 514: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 515: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 516: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 517: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 518: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 519: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 520: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 521: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 522: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 523: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 524: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 525: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 526: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 527: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 528: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 529: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 530: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 531: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 532: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 533: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 534: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 535: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 536: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 537: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 538: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 539: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 540: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 541: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 542: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 543: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 544: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 545: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 546: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 547: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 548: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 549: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 550: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 551: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 552: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 553: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 554: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 555: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 556: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 557: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 558: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 559: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 560: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 561: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 562: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 563: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 564: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 565: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 566: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 567: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 568: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 569: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 570: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 571: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 572: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 573: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 574: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 575: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 576: loss 2.2737367544323206e-13, pred 4.999999523162842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 577: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 578: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 579: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 580: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 581: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 582: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 583: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 584: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 585: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 586: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 587: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 588: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 589: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 590: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 591: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 592: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 593: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 594: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 595: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 596: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 597: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 598: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 599: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 600: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 601: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 602: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 603: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 604: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 605: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 606: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 607: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 608: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 609: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 610: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 611: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 612: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 613: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 614: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 615: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 616: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 617: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 618: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 619: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 620: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 621: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 622: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 623: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 624: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 625: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 626: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 627: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 628: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 629: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 630: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 631: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 632: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 633: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 634: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 635: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 636: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 637: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 638: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 639: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 640: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 641: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 642: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 643: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 644: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 645: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 646: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 647: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 648: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 649: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 650: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 651: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 652: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 653: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 654: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 655: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 656: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 657: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 658: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 659: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 660: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 661: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 662: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 663: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 664: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 665: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 666: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 667: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 668: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 669: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 670: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 671: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 672: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 673: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 674: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 675: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 676: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 677: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 678: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 679: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 680: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 681: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 682: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 683: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 684: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 685: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 686: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 687: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 688: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 689: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 690: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 691: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 692: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 693: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 694: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 695: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 696: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 697: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 698: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 699: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 700: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 701: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 702: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 703: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 704: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 705: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 706: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 707: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 708: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 709: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 710: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 711: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 712: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 713: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 714: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 715: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 716: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 717: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 718: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 719: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 720: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 721: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 722: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 723: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 724: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 725: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 726: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 727: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 728: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 729: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 730: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 731: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 732: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 733: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 734: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 735: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 736: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 737: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 738: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 739: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 740: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 741: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 742: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 743: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 744: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 745: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 746: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 747: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 748: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 749: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 750: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 751: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 752: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 753: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 754: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 755: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 756: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 757: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 758: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 759: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 760: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 761: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 762: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 763: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 764: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 765: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 766: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 767: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 768: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 769: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 770: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 771: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 772: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 773: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 774: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 775: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 776: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 777: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 778: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 779: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 780: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 781: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 782: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 783: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 784: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 785: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 786: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 787: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 788: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 789: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 790: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 791: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 792: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 793: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 794: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 795: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 796: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 797: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 798: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 799: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 800: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 801: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 802: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 803: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 804: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 805: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 806: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 807: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 808: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 809: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 810: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 811: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 812: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 813: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 814: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 815: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 816: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 817: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 818: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 819: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 820: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 821: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 822: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 823: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 824: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 825: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 826: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 827: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 828: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 829: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 830: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 831: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 832: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 833: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 834: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 835: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 836: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 837: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 838: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 839: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 840: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 841: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 842: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 843: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 844: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 845: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 846: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 847: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 848: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 849: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 850: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 851: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 852: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 853: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 854: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 855: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 856: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 857: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 858: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 859: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 860: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 861: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 862: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 863: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 864: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 865: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 866: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 867: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 868: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 869: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 870: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 871: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 872: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 873: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 874: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 875: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 876: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 877: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 878: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 879: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 880: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 881: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 882: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 883: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 884: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 885: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 886: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 887: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 888: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 889: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 890: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 891: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 892: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 893: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 894: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 895: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 896: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 897: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 898: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 899: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 900: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 901: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 902: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 903: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 904: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 905: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 906: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 907: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 908: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 909: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 910: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 911: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 912: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 913: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 914: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 915: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 916: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 917: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 918: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 919: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 920: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 921: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 922: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 923: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 924: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 925: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 926: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 927: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 928: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 929: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 930: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 931: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 932: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 933: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 934: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 935: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 936: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 937: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 938: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 939: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 940: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 941: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 942: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 943: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 944: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 945: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 946: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 947: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 948: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 949: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 950: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 951: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 952: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 953: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 954: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 955: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 956: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 957: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 958: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 959: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 960: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 961: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 962: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 963: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 964: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 965: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 966: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 967: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 968: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 969: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 970: loss 2.2737367544323206e-13, pred 4.999999523162842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 971: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 972: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 973: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 974: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 975: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 976: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 977: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 978: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 979: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 980: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 981: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 982: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 983: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 984: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 985: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 986: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 987: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 988: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 989: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 990: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 991: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 992: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 993: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 994: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 995: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 996: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 997: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 998: loss 2.2737367544323206e-13, pred 4.999999523162842\n",
      "Iteration 999: loss 2.2737367544323206e-13, pred 4.999999523162842\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0b20097f2731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mlogging_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_writer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration {}: loss {}, pred {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_prediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mlogging_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'saver'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_path' is not defined"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "_data = np.array([[1,2,3]])\n",
    "_labels = np.array([[5]])\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    session.run(initialize_vars)\n",
    "    \n",
    "    for iteration in range(1000):\n",
    "        feed_dict = {\n",
    "            data: _data, \n",
    "            labels: _labels\n",
    "        }\n",
    "        \n",
    "        _, _prediction, _loss, _summary = session.run([train_step, prediction, loss, merge_summaries],\n",
    "                            feed_dict=feed_dict)\n",
    "        \n",
    "        logging_meta['train_writer'].add_summary(_summary, iteration)\n",
    "        print(\"Iteration {}: loss {}, pred {}\".format(iteration, _loss, _prediction[0][0]))\n",
    "    logging_meta['saver'].save(session, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
